title
Densely Connected Attention Propagation for Reading Comprehension
abstract
We propose DECAPROP ( Densely Connected Attention Propagation ) , a new densely connected neural architecture for reading comprehension ( RC ) .
There are two distinct characteristics of our model .
Firstly , our model densely connects all pairwise layers of the network , modeling relationships between passage and query across all hierarchical levels .
Secondly , the dense connectors in our network are learned via attention instead of standard residual skip - connectors .
To this end , we propose novel Bidirectional Attention Connectors ( BAC ) for efficiently forging connections throughout the network .
We conduct extensive experiments on four challenging RC benchmarks .
Our proposed approach achieves state - of - the - art results on all four , outperforming existing baselines by up to 2.6 % ?
14.2 % in absolute F1 score .
Introduction
The dominant neural architectures for reading comprehension ( RC ) typically follow a standard ' encode - interact - point ' design .
Following the embedding layer , a compositional encoder typically encodes Q ( query ) and P ( passage ) individually .
Subsequently , an ( bidirectional ) attention layer is then used to model interactions between P/Q .
Finally , these attended representations are then reasoned over to find ( point to ) the best answer span .
While , there might be slight variants of this architecture , this over all architectural design remains consistent across many RC models .
Intuitively , the design of RC models often possess some depth , i.e. , every stage of the network easily comprises several layers .
For example , the R - NET architecture adopts three BiRNN layers as the encoder and two additional BiRNN layers at the interaction layer .
BiDAF uses two BiLSTM layers at the pointer layer , etc .
As such , RC models are often relatively deep , at the very least within the context of NLP .
Unfortunately , the depth of a model is not without implications .
It is well - established fact that increasing the depth may impair gradient flow and feature propagation , making networks harder to train .
This problem is prevalent in computer vision , where mitigation strategies that rely on shortcut connections such as Residual networks , GoogLeNet and DenseNets were incepted .
Naturally , many of the existing RC models already have some built - in designs to workaround this issue by shortening the signal path in the network .
Examples include attention flow , residual connections or simply the usage of highway encoders .
As such , we hypothesize that explicitly improving information flow can lead to further and considerable improvements in RC models .
A second observation is that the flow of P/Q representations across the network are often well - aligned and ' synchronous ' , i.e. , P is often only matched with Q at the same hierarchical stage ( e.g. , only after they have passed through a fixed number of encoder layers ) .
To this end , we hypothesize that increasing the number of interaction interfaces , i.e. , matching in an asynchronous , cross-hierarchical fashion , can also lead to an improvement in performance .
Based on the above mentioned intuitions , this paper proposes a new architecture with two distinct characteristics .
Firstly , our network is densely connected , connecting every layer of P with every layer of Q .
This not only facilitates information flow but also increases the number of interaction interfaces between P/Q .
Secondly , our network is densely connected by attention , making it vastly different from any residual mitigation strategy in the literature .
To the best of our knowledge , this is the first work that explicitly considers attention as a form of skip - connector .
Notably , models such as BiDAF incorporates a form of attention propagation ( flow ) .
However , this is inherently unsuitable for forging dense connections throughout the network since this would incur a massive increase in the representation size in subsequent layers .
To this end , we propose efficient Bidirectional Attention Connectors ( BAC ) as a base building block to connect two sequences at arbitrary layers .
The key idea is to compress the attention outputs so that they can be small enough to propagate , yet enabling a connection between two sequences .
The propagated features are collectively passed into prediction layers , which effectively connect shallow layers to deeper layers .
Therefore , this enables multiple bidirectional attention calls to be executed without much concern , allowing us to efficiently connect multiple layers together .
Overall , we propose DECAPROP ( Densely Connected Attention Propagation ) , a novel architecture for reading comprehension .
DECAPROP achieves a significant gain of 2.6 % ?
14.2 % absolute improvement in F1 score over the existing state - of - the - art on four challenging RC datasets , namely News QA , Quasar - T , Search QA and Narrative QA .
Bidirectional Attention Connectors ( BAC )
This section introduces the Bidirectional Attention Connectors ( BAC ) module which is central to our over all architecture .
The BAC module can bethought of as a connector component that connects two sequences / layers .
The key goals of this module are to ( 1 ) connect any two layers of P/Q in the network , returning a residual feature that can be propagated 1 to deeper layers , ( 2 ) model cross-hierarchical interactions between P/Q and ( 3 ) minimize any costs incurred to other network components such that this component maybe executed multiple times across all layers .
Let P ?
R p d and Q ?
R q d be inputs to the BAC module .
The initial steps in this module remains identical to standard bi-attention in which an affinity matrix is constructed between P/Q .
In our bi-attention module , the affinity matrix is computed via :
where F ( . ) is a standard dense layer with ReLU activations and d is the dimensionality of the vectors .
Note that this is the scaled dot -product attention from .
Next , we learn an alignment between P/Q as follows :
where A , B are the aligned representations of the query / passsage respectively .
In many standard neural QA models , it is common to pass an augmented 2 matching vector of this attentional representation to subsequent layers .
For this purpose , functions such as
been used .
However , simple / naive augmentation would not suffice in our use case .
Even without augmentation , every call of bi-attention returns a new d dimensional vector for each element in the sequence .
If the network has l layers , then connecting 3 all pairwise layers would require l 2 connectors and therefore an output dimension of l 2 d .
This is not only computationally undesirable but also require a large network at the end to reduce this vector .
With augmentation , this problem is aggravated .
Hence , standard birectional attention is not suitable here .
To overcome this limitation , we utilize a parameterized function G (. ) to compress the bi-attention vectors down to scalar .
where g pi ?
R 3 is the output ( for each element in P ) of the BAC module .
This is done in an identical fashion for a i and q i to form g q i for each element in Q .
Intuitively g * i where * = {p , q} are the learned scalar attention that is propagated to upper layers .
Since there are only three scalars , they will not cause any problems even when executed for multiple times .
As such , the connection remains relatively lightweight .
This compression layer can be considered as a defining trait of the BAC , differentiating it from standard bi-attention .
Naturally , there are many potential candidates for the function G ( . ) .
One natural choice is the standard dense layer ( or multiple dense layers ) .
However , dense layers are limited as they do not compute dyadic pairwise interactions between features which inhibit its expressiveness .
On the other hand , factorization - based models are known to not only be expressive and efficient , but also able to model low - rank structure well .
To this end , we adopt factorization machines ( FM ) as G ( . ) .
The FM layer is defined as :
is a scalar .
Intuitively , this layer tries to learn pairwise interactions between every x i and x j using factorized ( vector ) parameters v.
In the context of our BAC module , the FM layer is trying to learn a low - rank structure from the ' match ' vector ( e.g.
.
Finally , we note that the BAC module takes inspiration from the main body of our CAFE model for entailment classification .
However , this work demonstrates the usage and potential of the BAC as a residual connector .
Densely Connected Attention Propagation ( DECAPROP )
In this section , we describe our proposed model in detail .
depicts a high - level overview of our proposed architecture .
Start Pointer
End Pointer
Contextualized Input Encoder
The inputs to our model are two sequences P and Q which represent passage and query respectively .
Given Q , the task of the RC model is to select a sequence of tokens in P as the answer .
Following many RC models , we enhance the input representations with ( 1 ) character embeddings ( passed into a BiRNN encoder ) , ( 2 ) a binary match feature which denotes if a word in the query appears in the passage ( and vice versa ) and ( 3 ) a normalized frequency score denoting how many times a word appears in the passage .
The Char BiRNN of h c dimensions , along with two other binary features , is concatenated with the word embeddings w i ?
R dw , to form the final representation of d w + h c + 2 dimensions .
Densely Connected Attention Encoder ( DECAENC )
The DECAENC accepts the inputs P and Q from the input encoder .
DECAENC is a multi - layered encoder with k layers .
For each layer , we pass P/Q into a bidirectional RNN layer of h dimensions .
Next , we apply our attention connector ( BAC ) to HP /H Q ?
R where H represents the hidden state outputs from the BiRNN encoder where the RNN cell can either be a GRU or LSTM encoder .
Let d be the input dimensions of P and Q , then this encoder goes through a process of d ? h ?
h+3 ?
h in which the BiRNN at layer l + 1 consumes the propagated features from layer l .
Intuitively , this layer models P/Q whenever they are at the same network hierarchical level .
At this point , we include ' asynchronous ' ( cross hierarchy ) connections between P and Q .
Let P i , Q i denote the representations of P , Q at layer i .
We apply the Bidirectional Attention Connectors ( BAC ) as follows :
This densely connects all representations of P and Q across multiple layers .
Z ij * ?
R 3 represents the generated features for each ij combination of P/Q .
In total , we obtain 3n 2 compressed attention features for each word .
Intuitively , these features capture fine - grained relationships between P / Q at different stages of the network flow .
The output of the encoder is the concatenation of all the BiRNN hidden states H 1 , H 2 H k and Z * which is a matrix of ( nh + 3 n 2 ) dimensions .
Densely Connected Core Architecture ( DECACORE )
This section introduces the core architecture of our proposed model .
This component corresponds to the interaction segment of standard RC model architecture .
Gated Attention
The outputs of the densely connected encoder are then passed into a standard gated attention layer .
This corresponds to the ' interact ' component in many other popular RC models that models Q/P interactions with attention .
While there are typically many choices of implementing this layer , we adopt the standard gated bi-attention layer following .
where ?
is the sigmoid function and F ( . ) are dense layers with ReLU activations .
The output P is the query - dependent passage representation .
Gated Self - Attention Next , we employ a self - attention layer , applying Equation yet again on P , matching P against itself to form B , the output representation of the core layer .
The key idea is that self - attention models each word in the query - dependent passsage representation against all other words , enabling each word to benefit from a wider global view of the context .
Dense Core
At this point , we note that there are two intermediate representations of P , i.e. , one after the gated bi-attention layer and one after the gated self - attention layer .
We denote them as U 1 , U 2 respectively .
Unlike the Densely Connected Attention Encoder , we no longer have two representations at each hierarchical level since they have already been ' fused ' .
Hence , we apply a one - sided BAC to all permutations of [ U 1 , U 2 ] and Q i , ?i = 1 , 2 k.
Note that the one - sided BAC only outputs values for the left sequence , ignoring the right sequence .
where R kj ?
R 3 represents the connection output and F C is the one - sided BAC function .
All values of R kj , ?j = 1 , 2 , ?k = 1 , 2 n are concatenated to form a matrix R of ( 2 n 6 ) , which is then concatenated with U 2 to form M ? R p ( d+12 n ) .
This final representation is then passed to the answer prediction layer .
Answer Pointer and Prediction Layer
Next , we pass M through a stacked BiRNN model with two layers and obtain two representations , H 1 and H 2 respectively .
The start and end pointers are then learned via :
where w 1 , w 2 ?
Rd are parameters of this layer .
To train the model , following prior work , we minimize the sum of negative log probabilities of the start and end indices :
where N is the number of samples , y 1 i , y 2 i are the true start and end indices .
pk is the k - th value of the vector p.
The test span is chosen by finding the maximum value of p 1 k , p 2 l where k ?
l.
Experiments
This section describes our experiment setup and empirical results .
Datasets and Competitor Baselines
We conduct experiments on four challenging QA datasets which are described as follows :
NewsQA
This challenging RC dataset comprises 100 k QA pairs .
Passages are relatively long at about 600 words on average .
This dataset has also been extensively used in benchmarking RC models .
On this dataset , the key competitors are BiDAF , Match - LSTM , FastQA / Fast QA - Ext , R2-BiLSTM , AMANDA .
Quasar -T
This dataset comprises 43 k factoid - based QA pairs and is constructed using ClueWeb09 as its backbone corpus .
The key competitors on this dataset are BiDAF and the Reinforced Ranker - Reader ( R 3 ) .
Several variations of the ranker - reader model ( e.g. , SR , SR 2 ) , which use the Match - LSTM underneath , are also compared against .
SearchQA
This dataset aims to emulate the search and retrieval process in question answering applications .
The challenge involves reasoning over multiple documents .
In this dataset , we concatenate all documents into a single passage context and perform RC over the documents .
The competitor baselines on this dataset are Attention Sum Reader ( ASR ) , Focused Hierarchical RNNs ( FH - RNN ) , AMANDA , BiDAF , AQA and the Reinforced Ranker - Reader ( R 3 ) .
Narrative QA ] is a recent QA dataset that involves comprehension over stories .
We use the summaries setting 4 which is closer to a standard QA or reading comprehension setting .
We compare with the baselines in the original paper , namely Seq2Seq , Attention Sum Reader and BiDAF .
We also compare with the recent BiAttention + MRU model .
As compared to the popular SQuAD dataset , these datasets are either ( 1 ) more challenging 5 , involves more multi-sentence reasoning or is concerned with searching across multiple documents in an ' open domain ' setting ( Searc hQA / Quasar - T ) .
Hence , these datasets accurately reflect real world applications to a greater extent .
However , we regard the concatenated documents as a single context for performing reading comprehension .
The evaluation metrics are the EM ( exact match ) and F1 score .
Note that for all datasets , we compare all models solely on the RC task .
Therefore , for fair comparison we do not compare with algorithms that use a second - pass answer re-ranker .
Finally , to ensure that our model is not a failing case of SQuAD , and as requested by reviewers , we also include development set scores of our model on SQuAD .
Experimental Setup
Our model is implemented in Tensorflow .
The sequence lengths are capped at 800/700/1500/1100 for News QA , Search QA , Quasar - T and Narrative QA respectively .
We use Adadelta with ? = 0.5 for News QA , Adam with ? = 0.001 for Search QA , Quasar - T and Narrative QA .
The choice of the RNN encoder is tuned between GRU and LSTM cells and the hidden size is tuned amongst { 32 , 50 , 64 , 75 } .
We use the CUDNN implementation of the RNN encoder .
Batch size is tuned amongst { 16 , 32 , 64 } .
Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } and applied to all RNN and fully - connected layers .
We apply variational dropout in - between RNN layers .
We initialize the word embeddings with 300D Glo Ve embeddings and are fixed during training .
The size of the character embeddings is set to 8 and the character RNN is set to the same as the word - level RNN encoders .
The maximum characters per word is set to 16 .
The number of layers in DECAENC is set to 3 and the number of factors in the factorization kernel is set to 64 .
We use a learning rate decay factor of 2 and patience of 3 epochs whenever the EM ( or ROUGE - L ) score on the development set does not increase .
Results
Overall , our results are optimistic and promising , with results indicating that DECAPROP achieves state - of - the - art performance 6 on all four datasets . 66.2 75.9 DCN + CoVE 71.3 79.9 R- NET 72.3 80.6 R - NET
( Our re-implementation ) 71.9 79.6 DECAPROP ( This paper ) 72.9 81.4 QANet 73.6 82.7 News QA reports the results on News QA .
On this dataset , DECAPROP outperforms the existing state - of - the - art , i.e. , the recent AMANDA model by ( + 4.7 % EM / + 2.6 % F1 ) .
Notably , AMANDA is a strong neural baseline that also incorporates gated self - attention layers , along with question - aware pointer layers .
Moreover , our proposed model also outperforms well - established baselines such as Match - LSTM ( + 18 % EM / + 16.3 % F1 ) and BiDAF ( + 16 % EM / + 14 % F1 ) .
reports the results on Quasar - T .
Our model achieves state - of - the - art performance on this dataset , outperforming the state - of - the - art R 3 ( Reinforced Ranker Reader ) by a considerable margin of + 4.4 % EM / + 6 % F1 .
Performance gain over standard baselines such as BiDAF and GA are even larger ( > 15 % F1 ) .
Quasar - T
Search QA and report the results 7 on Search QA .
On the original setting , our model outperforms AMANDA by + 15.4 % EM and + 14.2 % in terms of F1 score .
On the over all setting , our model outperforms both AQA ( + 18.1 % EM / + 18 % F1 ) and Reinforced Reader Ranker ( + 7.8 % EM / + 8.3 % F1 ) .
Both models are reinforcement learning based extensions of existing strong baselines such as BiDAF and Match - LSTM .
Narrative QA reports the results on Narrative QA .
Our proposed model outperforms all baseline systems ( Seq2Seq , ASR , BiDAF ) in the original paper .
On average , there is a ?
+ 5 % improvement across all metrics .
SQuAD reports dev scores 8 of our model against several representative models on the popular SQuAD benchmark .
While our model does not achieve state - of - the - art performance , our model can outperform the base R - NET ( both our implementation as well as the published score ) .
Our model achieves reasonably competitive performance .
Ablation Study
We conduct an ablation study on the New s QA development set .
More specifically , we report the development scores of seven ablation baselines .
In ( 1 ) , we removed the entire DECAPROP architecture , reverting it to an enhanced version of the original R - NET model 9 . In ( 2 ) , we removed DECACORE and passed U 2 to the answer layer instead of M .
In , we removed the DECAENC layer and used a 3 - layered BiRNN instead .
In ( 4 ) , we kept the DECAENC but only compared layer of the same hierarchy and omitted cross hierarchical comparisons .
In ( 5 ) , we removed the Gated Bi-Attention and Gated Self - Attention layers .
Removing these layers simply allow previous layers to pass through .
In ( 6 - 7 ) , we varied n , the number of layers of DECAENC .
Finally , in ( 8 - 9 ) , we varied the FM with linear and nonlinear feed - forward layers . From ( 1 ) , we observe a significant gap in performance between DECAPROP and R - NET .
This demonstrates the effectiveness of our proposed architecture .
Overall , the key insight is that all model components are crucial to DECAPROP .
Notably , the DECAENC seems to contribute the most to the over all performance .
Finally , shows the performance plot of the development EM metric ( New s QA ) over training .
We observe that the superiority of DECAPROP over R - NET is consistent and relatively stable .
This is also observed across other datasets but not reported due to the lack of space .
Related Work
In recent years , there has been an increase in the number of annotated RC datasets such as SQuAD , New s QA , TriviaQA and RACE 8 Early testing of our model was actually done on SQ uAD .
However , since taking part on the heavily contested public leaderboard requires more computational resources than we could muster , we decided to focus on other datasets .
In lieu of reviewer requests , we include preliminary results of our model on SQ u AD dev set .
For fairer comparison , we make several enhancements to the R - NET model as follows :
We replaced the additive attention with scaled dot-product attention similar to ours .
( 2 ) We added shortcut connections after the encoder layer .
( 3 ) We replaced the original Pointer networks with our BiRNN Pointer Layer .
We found that these enhancements consistently lead to improved performance .
The original R - NET performs at ?
2 % lower on News QA . .
Spurred on by the avaliability of data , many neural models have also been proposed to tackle these challenges .
These models include BiDAF , Match - LSTM , DCN / DCN + , R - NET , DrQA , AoA Reader , Reinforced Mnemonic Reader , ReasoNet , AMANDA , R 3 Reinforced Reader Ranker and QANet .
Many of these models innovate at either ( 1 ) the bidirectional attention layer ( BiDAF , DCN ) , ( 2 ) invoking multi-hop reasoning ( Mnemonic Reader , ReasoNet ) , ( 3 ) reinforcement learning ( R 3 , DCN + ) , ( 4 ) self - attention ( AMANDA , R - NET , QANet ) and finally , ( 5 ) improvements at the encoder level ( QANet ) .
While not specifically targeted at reading comprehension , a multitude of pretraining schemes have recently proven to be very effective for language understanding tasks .
Our work is concerned with densely connected networks aimed at improving information flow .
While most works are concerned with computer vision tasks or general machine learning , there are several notable works in the NLP domain .
proposed Densely Connected BiLSTMs for standard text classification tasks .
proposed a co-stacking residual affinity mechanims that includes all pairwise layers of a text matching model in the affinity matrix calculation .
In the RC domain , DCN + used Residual Co - Attention encoders .
QANet used residual self - attentive convolution encoders .
While the usage of highway / residual networks is not an uncommon sight in NLP , the usage of bidirectional attention as a skip - connector is new .
Moreover , our work introduces new cross-hierarchical connections , which help to increase the number of interaction interfaces between P/Q .
Conclusion
We proposed a new Densely Connected Attention Propagation ( DECAPROP ) mechanism .
For the first time , we explore the possibilities of using birectional attention as a skip - connector .
We proposed Bidirectional Attention Connectors ( BAC ) for efficient connection of any two arbitary layers , producing connectors that can be propagated to deeper layers .
This enables a shortened signal path , aiding information flow across the network .
Additionally , the modularity of the BAC allows it to be easily equipped to other models and even other domains .
Our proposed architecture achieves state - of - the - art performance on four challenging QA datasets , outperforming strong and competitive baselines such as Reinforced Reader Ranker ( R 3 ) , AMANDA , BiDAF and R - NET .
