160	22	29	include
160	46	48	GA
160	53	80	reading comprehension model
160	81	85	with
160	86	103	gated - attention
160	106	111	BiDAF
160	118	126	RC model
160	127	131	with
160	132	160	bidirectional attention flow
160	163	166	AQA
160	173	199	reinforced system learning
160	200	212	to aggregate
160	217	224	answers
160	225	237	generated by
160	242	262	re-written questions
160	265	268	R 3
160	275	291	reinforced model
160	292	305	making use of
160	308	314	ranker
160	315	328	for selecting
160	329	337	passages
160	338	346	to train
160	351	359	RC model
184	32	68	https://github.com/shuohangwang/mprc
167	9	12	use
167	15	36	pre-trained R 3 model
167	132	143	to generate
167	148	170	top 50 candidate spans
167	171	174	for
167	179	219	training , development and test datasets
167	229	241	use them for
167	242	257	further ranking
169	0	3	For
169	8	34	coverage - based re-ranker
169	40	43	use
169	44	48	Adam
169	49	60	to optimize
169	65	70	model
171	3	6	set
171	7	20	all the words
171	21	27	beyond
171	28	33	Glove
171	34	36	as
171	37	49	zero vectors
172	7	8	l
172	9	11	to
172	12	15	300
172	18	28	batch size
172	29	31	to
172	32	34	30
172	37	50	learning rate
172	51	53	to
172	54	59	0.002
173	3	7	tune
173	12	31	dropout probability
173	32	36	from
173	37	45	0 to 0.5
173	54	81	number of candidate answers
173	82	85	for
173	86	102	re-ranking ( K )
173	103	105	in
173	106	120	[ 3 , 5 , 10 ]
23	19	26	propose
23	29	35	method
23	36	46	to improve
23	47	63	open - domain QA
23	64	89	by explicitly aggregating
23	90	98	evidence
23	99	110	from across
23	111	128	multiple passages
35	3	12	formulate
35	23	43	evidence aggregation
35	44	46	as
35	50	75	answer re-ranking problem
37	39	42	for
37	43	64	each answer candidate
37	70	93	efficiently incorporate
37	94	112	global information
37	113	117	from
37	118	133	multiple pieces
37	23	25	of
37	137	153	textual evidence
37	154	161	without
37	162	186	significantly increasing
37	191	201	complexity
37	134	136	of
37	209	219	prediction
37	202	204	of
37	227	235	RC model
39	4	14	re-rankers
39	15	18	are
40	2	28	strength - based re-ranker
40	37	42	ranks
40	47	64	answer candidates
40	65	87	according to how often
40	94	102	evidence
40	103	112	occurs in
40	113	131	different passages
43	2	28	coverage - based re-ranker
43	37	49	aims to rank
43	53	69	answer candidate
43	70	76	higher
43	77	79	if
43	84	89	union
43	90	92	of
43	93	109	all its contexts
43	110	112	in
43	113	131	different passages
43	132	143	could cover
43	144	156	more aspects
43	157	168	included in
43	173	181	question
2	48	80	OPEN - DOMAIN QUESTION ANSWERING
16	0	37	Open-domain question answering ( QA )
18	15	31	open - domain QA
181	12	23	showed that
181	24	27	R 3
181	28	36	achieved
181	37	54	F1 56.0 , EM 50.9
181	55	57	on
181	58	69	Wiki domain
181	74	91	F1 68.5 , EM 63.0
181	92	94	on
181	95	105	Web domain
181	117	131	competitive to
181	136	159	state - of - the - arts
190	34	42	see that
190	47	61	full re-ranker
190	68	82	combination of
190	83	103	different re-rankers
190	106	131	significantly outperforms
190	136	161	previous best performance
190	162	164	by
190	167	179	large margin
190	182	195	especially on
190	196	206	Quasar - T
190	211	220	Search QA
192	26	56	our coverage - based re-ranker
192	57	65	achieves
192	66	95	consistently good performance
192	96	98	on
192	103	117	three datasets
191	11	20	our model
191	21	23	is
191	24	35	much better
191	36	40	than
191	45	62	human performance
191	63	65	on
191	70	87	Search QA dataset
