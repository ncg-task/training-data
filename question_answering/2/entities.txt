183	0	8	Memex QA
183	20	26	answer
191	50	69	Logistic Regression
191	70	78	predicts
191	90	94	with
191	95	146	concatenated image , question and metadata features
192	0	16	Embedding + LSTM
192	17	25	utilizes
192	26	66	word embeddings and character embeddings
192	69	79	along with
192	84	106	same visual embeddings
192	107	114	used in
192	115	119	FVTA
194	0	25	Embedding + LSTM + Concat
194	26	38	concatenates
194	43	59	last LSTM output
194	60	64	from
194	65	85	different modalities
194	86	96	to produce
194	101	113	final output
196	0	22	Classic Soft Attention
196	23	27	uses
196	28	85	classic one dimensional question - to - context attention
196	86	98	to summarize
196	99	106	context
196	107	110	for
196	111	129	question answering
198	0	5	DMN +
198	6	8	is
198	13	45	improved dynamic memory networks
198	48	63	which is one of
198	68	96	representative architectures
198	97	109	that achieve
198	110	126	good performance
198	127	129	on
198	134	142	VQA Task
201	0	23	TGIF Temporal Attention
201	24	26	is
201	47	83	spatial - temporal reasoning network
201	84	86	on
201	87	115	sequential animated image QA
211	3	9	encode
211	10	23	GPS locations
211	24	29	using
211	30	35	words
213	4	43	questions , textual context and answers
213	44	47	are
213	48	57	tokenized
213	58	63	using
213	68	91	Stanford word tokenizer
217	7	26	bi-directional LSTM
217	30	38	used for
217	39	52	each modality
217	53	62	to obtain
217	63	89	contextual representations
214	3	6	use
214	7	41	pre-trained Glo Ve word embeddings
214	53	65	fixed during
214	66	74	training
216	14	35	linear transformation
216	36	47	to compress
216	52	65	image feature
216	66	70	into
216	71	86	100 dimensional
223	11	29	AdaDelta optimizer
223	37	58	initial learning rate
223	59	61	of
223	62	65	0.5
223	66	78	to train for
223	79	89	200 epochs
223	90	94	with
223	97	109	dropout rate
223	110	112	of
223	113	116	0.3
215	0	3	For
215	4	27	image / video embedding
215	33	40	extract
215	41	62	fixed - size features
215	63	68	using
215	73	94	pre-trained CNN model
215	97	115	Inception - ResNet
215	118	134	by concatenating
215	139	185	pool5 layer and classification layer 's output
215	186	192	before
215	193	200	softmax
218	57	68	concatenate
218	73	79	output
218	26	28	of
218	83	98	both directions
218	80	82	of
218	106	110	LSTM
218	115	118	get
218	121	138	question matrix Q
218	0	5	Given
218	8	25	hidden state size
218	99	101	of
218	29	30	d
218	42	48	set to
218	49	51	50
219	0	7	R 2 d M
219	12	28	context tensor H
220	11	14	for
220	15	34	all media documents
220	0	10	R 2dV KN 6
221	3	10	reshape
221	15	29	context tensor
221	30	34	into
221	35	48	H ? R 2 d T 6
222	0	9	To select
222	14	33	best hyperparmeters
222	39	54	randomly select
222	55	59	20 %
222	60	62	of
222	67	88	official training set
222	89	91	as
222	96	110	validation set
234	0	4	FVTA
234	5	16	outperforms
234	17	39	other attention models
234	40	50	on finding
234	55	70	relevant photos
234	71	74	for
234	79	87	question
242	3	11	evaluate
242	16	40	FVTA attention mechanism
242	46	59	first replace
242	60	77	our kernel tensor
242	78	82	with
242	83	116	simple cosine similarity function
243	8	17	show that
243	18	44	standard cosine similarity
243	48	59	inferior to
243	60	83	our similarity function
244	0	12	For ablating
244	13	38	intra-sequence dependency
244	44	47	use
244	52	67	representations
244	68	72	from
244	77	90	last timestep
244	91	93	of
244	94	115	each context document
245	13	39	cross sequence interaction
245	45	52	average
245	53	88	all attended context representation
245	89	93	from
245	94	114	different modalities
245	115	121	to get
245	126	146	final context vector
246	0	27	Both aspects of correlation
246	28	30	of
246	35	56	FVTA attention tensor
246	57	75	contribute towards
246	80	100	model 's performance
246	103	108	while
246	109	134	intra-sequence dependency
246	135	140	shows
246	141	156	more importance
247	3	10	compare
247	15	28	effectiveness
247	29	31	of
247	32	66	context - aware question attention
247	67	78	by removing
247	83	101	question attention
247	106	109	use
247	114	127	last timestep
247	128	130	of
247	135	146	LSTM output
247	147	151	from
247	156	164	question
247	165	167	as
247	172	195	question representation
248	3	8	shows
248	13	31	question attention
248	32	40	provides
248	41	59	slight improvement
249	13	18	train
249	19	38	FVTA without photos
249	39	45	to see
249	50	62	contribution
249	63	65	of
249	66	84	visual information
250	4	10	result
250	11	13	is
250	14	24	quite good
257	7	22	MovieQA dataset
257	105	109	with
257	48	50	of
258	3	12	implement
258	13	25	FVTA network
258	26	29	for
258	30	43	Movie QA task
258	49	64	modality number
258	68	86	2 ( video & text )
259	3	6	set
259	11	53	maximum number of movie clips per question
259	54	56	to
259	57	63	N = 20
259	70	106	maximum number of frames to consider
259	107	109	to
259	110	116	F = 10
259	123	169	maximum number of subtitle sentences in a clip
259	170	172	to
259	173	180	K = 100
259	189	202	maximum words
259	203	205	to
259	206	212	V = 10
261	3	6	use
261	11	29	AdaDelta optimizer
261	30	34	with
261	37	46	minibatch
261	47	49	of
261	50	52	16
261	60	81	initial learning rate
261	82	84	of
261	85	88	0.5
261	92	103	trained for
261	104	114	300 epochs
264	0	10	FVTA model
264	11	22	outperforms
264	23	43	all baseline methods
264	48	56	achieves
264	57	79	comparable performance
264	80	82	to
264	87	116	state - of - the - art result
264	119	121	on
264	126	145	MovieQA test server
266	4	12	accuracy
266	13	15	is
266	16	21	0.410
266	24	26	vs
266	27	32	0.387
266	33	35	by
266	36	40	RWMN
266	43	45	on
266	50	64	validation set
266	69	74	0.373
266	77	79	vs
266	80	85	0.363
266	88	90	on
266	95	103	test set
267	40	44	FVTA
267	45	69	consistently outperforms
267	74	100	classical attention models
267	101	110	including
267	111	125	soft attention
267	128	131	MCB
267	136	140	TGIF
32	37	44	propose
32	47	91	focal visual - text attention ( FVTA ) model
32	92	95	for
32	96	111	sequential data
44	13	35	novel attention kernel
44	36	39	for
44	40	43	VQA
44	44	46	on
44	47	65	visual - text data
37	27	31	FVTA
37	38	47	learns to
37	48	56	localize
37	57	77	relevant information
37	78	84	within
37	87	131	few , small , temporally consecutive regions
37	132	136	over
37	141	156	input sequences
37	173	178	infer
37	182	188	answer
37	189	197	based on
37	202	224	cross-modal statistics
37	225	236	pooled from
37	237	250	these regions
38	5	13	proposes
38	16	28	novel kernel
38	29	39	to compute
38	44	60	attention tensor
38	66	80	jointly models
38	85	103	latent information
38	104	106	in
38	107	120	three sources
39	4	28	answer - signaling words
39	29	31	in
39	36	44	question
39	51	71	temporal correlation
39	72	78	within
39	81	89	sequence
39	100	123	cross-modal interaction
39	124	131	between
39	136	150	text and image
40	0	14	FVTA attention
40	15	25	allows for
40	26	46	collective reasoning
40	47	49	by
40	54	70	attention kernel
40	71	83	learned over
40	86	125	few , small , consecutive sub-sequences
40	126	128	of
40	129	143	text and image
2	34	59	Visual Question Answering
13	0	33	Visual question answering ( VQA )
15	15	18	VQA
