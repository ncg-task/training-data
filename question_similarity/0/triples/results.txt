(Contribution||has||Results)
(Results||using||pre-trained FastText embeddings)
(pre-trained FastText embeddings||as||input)
(input||to||our model)
(our model||yields||worse F1score)
(our model||compared with||ELMo contextual embeddings model)
(our model||on||public and private leaderboards)
(public and private leaderboards||with||94.254 and 93.118)
(Results||overcome||weakness)
(weakness||of||Arabic ELMo model)
(Arabic ELMo model||by translating||data)
(data||to||English)
(English||using||Google Translate)
(Arabic ELMo model||results are||much worse)
(much worse||with||88.868 and 87.504 F1 - scores)
(88.868 and 87.504 F1 - scores||on||public and private leaderboards)
(Arabic ELMo model||treating||problem)
(problem||as||English SQS problem)
(Results||has||GRU cells)
(GRU cells||are||most efficient)
(Results||has||Effect of Data Augmentation)
(Effect of Data Augmentation||not||each increment step)
(each increment step||has||positive effect)
(positive effect||on||model 's effectiveness)
(Effect of Data Augmentation||show||each augmentation step)
(each augmentation step||affects||model 's efficiency)
(model 's efficiency||has||negatively)
(Results||has||sequence weighted attention)
(sequence weighted attention||gives||better results)
(better results||by||about 1 point)
(about 1 point||of||F1-score)
(Results||has||ON - LSTM cells)
(ON - LSTM cells||with||chunk size 8)
(ON - LSTM cells||are||most effective)
(most effective||in terms of||all considered measures)
