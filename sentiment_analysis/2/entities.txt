134	0	4	LSTM
134	12	17	takes
134	22	30	sentence
134	31	33	as
134	34	39	input
134	46	52	to get
134	57	78	hidden representation
134	79	81	of
134	82	91	each word
134	43	45	as
135	8	15	regards
135	20	33	average value
135	34	36	of
135	41	54	hidden states
135	62	88	representation of sentence
135	95	107	puts it into
135	108	121	softmax layer
135	122	132	to predict
135	137	148	probability
135	149	151	of
135	152	175	each sentiment polarity
137	0	9	AE - LSTM
137	28	34	models
137	39	44	words
137	45	47	in
137	48	56	sentence
137	57	60	via
137	61	73	LSTM network
137	78	89	concatenate
137	94	110	aspect embedding
137	111	113	to
137	118	150	hidden contextual representation
137	151	166	for calculating
137	171	188	attention weights
137	201	220	employed to produce
137	225	245	final representation
137	246	249	for
137	254	268	input sentence
137	269	277	to judge
137	282	300	sentiment polarity
138	0	11	ATAE - LSTM
138	26	34	extended
138	35	44	AE - LSTM
138	45	57	by appending
138	62	78	aspect embedding
138	79	81	to
138	82	101	each word embedding
138	108	120	to represent
138	125	139	input sentence
140	0	3	IAN
140	10	19	considers
140	24	41	separate modeling
140	42	44	of
140	45	71	aspect terms and sentences
143	0	6	MemNet
143	16	23	applies
143	24	33	attention
143	34	48	multiple times
143	49	51	on
143	56	70	word embedding
143	73	80	so that
143	81	107	more abstractive evidences
143	108	130	could be selected from
143	135	150	external memory
120	25	39	word embedding
120	44	58	initialized by
120	63	87	pre-trained Glove vector
121	8	23	weight matrices
121	28	33	given
121	38	51	initial value
121	52	54	by
121	55	63	sampling
121	64	68	from
121	73	110	uniform distribution U ( ?0.1 , 0.1 )
121	125	131	biases
121	132	142	are set to
121	143	147	zero
123	4	13	dimension
123	14	16	of
123	17	35	position embedding
123	39	45	set to
123	46	49	100
123	52	60	which is
123	61	93	randomly initialized and updated
123	94	100	during
123	105	121	training process
122	21	61	word embedding and aspect term embedding
122	66	72	set to
122	73	76	300
122	101	113	hidden units
122	118	124	set to
122	125	128	200
124	3	6	use
124	7	17	Tensorflow
124	18	30	to implement
124	31	49	our proposed model
124	54	60	employ
124	65	73	Momentum
124	74	76	as
124	81	96	training method
124	99	104	whose
124	105	125	momentum parameter ?
124	129	135	set to
124	136	139	0.9
124	171	192	initial learning rate
124	147	153	set to
124	203	207	0.01
39	46	53	propose
39	56	113	position - aware bidirectional attention network ( PBAN )
39	114	122	based on
39	123	171	bidirectional Gated Recurrent Units ( Bi - GRU )
40	52	56	PBAN
40	62	77	mutually models
40	82	94	relationship
40	95	102	between
40	107	154	sentence and different words in the aspect term
40	155	166	by adopting
40	169	202	bidirectional attention mechanism
42	4	34	Obtaining position information
42	35	37	of
42	38	47	each word
42	48	50	in
42	51	73	corresponding sentence
42	74	82	based on
42	87	106	current aspect term
42	114	124	converting
42	129	149	position information
42	150	154	into
42	155	173	position embedding
43	8	12	PBAN
43	13	24	composes of
43	25	46	two Bi - GRU networks
43	47	69	focusing on extracting
43	74	94	aspectlevel features
43	99	124	sentence - level features
44	14	47	bidirectional attention mechanism
44	48	56	to model
44	61	76	mutual relation
44	77	84	between
44	85	127	aspect term and its corresponding sentence
2	55	88	Aspect - level Sentiment Analysis
18	0	18	Sentiment analysis
145	61	63	on
145	64	94	datasets Restaurant and Laptop
146	7	14	observe
146	20	43	our proposed PBAN model
146	44	52	achieves
146	57	73	best performance
146	74	79	among
146	84	91	methods
160	21	35	by integrating
160	40	102	position information and the bidirectional attention mechanism
160	105	109	PBAN
160	110	118	achieves
160	123	158	state - of - the - art performances
160	172	189	effectively judge
160	194	212	sentiment polarity
160	213	215	of
160	216	237	different aspect term
160	238	240	in
160	245	267	corresponding sentence
160	274	284	to improve
160	289	312	classification accuracy
