(Contribution||has||Hyperparameters)
(Hyperparameters||empirically set||number of memory chains)
(number of memory chains||with||keys of two of them)
(keys of two of them||set to||same embeddings)
(same embeddings||as||target words LOC1 and LOC2)
(number of memory chains||to||6)
(Hyperparameters||pre-process||corpus)
(corpus||with||tokenisation)
(tokenisation||using||NLTK)
(tokenisation||using||case folding)
(Hyperparameters||use||following hyper - parameters)
(following hyper - parameters||for||weight matrices)
(weight matrices||in||both directions)
(following hyper - parameters||for||hidden size)
(hidden size||of||GRU)
(GRU||is||300)
(Hyperparameters||to curb||overfitting)
(overfitting||regularise||last layer)
(last layer||with||L 2 penalty)
(L 2 penalty||on||weights)
(Hyperparameters||has||Training)
(Training||with||FTRL optimiser)
(Training||carried out over||800 epochs)
(Hyperparameters||has||learning rate)
(learning rate||of||0.05)
(Hyperparameters||has||batch size)
(batch size||of||128)
(Hyperparameters||has||Dropout)
(Dropout||with||rate)
(rate||of||0.2)
(Dropout||applied to||output)
(output||in||final classifier)
(Hyperparameters||initialise||our model)
(our model||with||GloVe)
(GloVe||not updated during||training)
(GloVe||has||300 - D)
(GloVe||trained on||42B tokens , 1.9 M vocab)
