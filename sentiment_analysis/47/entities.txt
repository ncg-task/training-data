141	0	8	Majority
141	9	16	assigns
141	21	39	sentiment polarity
141	53	72	largest probability
141	73	75	in
141	80	92	training set
141	99	109	Simple SVM
141	110	112	is
141	115	129	SVM classifier
141	130	134	with
141	135	150	simple features
141	151	158	such as
141	159	179	unigrams and bigrams
141	186	208	Feature - enhanced SVM
141	209	211	is
141	214	228	SVM classifier
141	229	233	with
141	236	275	state - of - the - art feature template
141	276	290	which contains
141	291	306	n-gram features
141	309	323	parse features
141	328	344	lexicon features
141	438	447	TD - LSTM
141	448	454	adopts
141	455	464	two LSTMs
141	465	473	to model
141	478	490	left context
141	491	495	with
141	496	502	target
141	511	524	right context
141	525	529	with
141	530	536	target
143	4	13	AE - LSTM
143	14	16	is
143	20	36	upgraded version
143	37	39	of
143	40	44	LSTM
146	4	15	ATAE - LSTM
146	19	37	developed based on
146	38	47	AE - LSTM
148	4	13	GRNN - G3
148	14	20	adopts
148	23	34	Gated - RNN
148	35	47	to represent
148	48	56	sentence
148	61	64	use
148	67	88	three - way structure
148	89	100	to leverage
148	101	109	contexts
150	0	6	MemNet
150	7	9	is
150	12	31	deep memory network
150	38	47	considers
150	52	72	content and position
150	73	75	of
150	76	82	target
152	0	3	IAN
152	4	24	interactively learns
152	25	35	attentions
152	36	38	in
152	43	63	contexts and targets
152	70	78	generate
152	83	98	representations
152	99	102	for
152	103	123	targets and contexts
131	18	27	dimension
131	28	30	of
131	31	53	word embedding vectors
131	58	78	hidden state vectors
131	79	81	is
131	82	85	300
133	0	48	All out - ofvocabulary words and weight matrices
133	53	76	randomly initialized by
133	79	117	uniform distribution U ( - 0.1 , 0.1 )
133	124	132	all bias
133	137	143	set to
133	144	148	zero
134	0	11	Tensor Flow
134	20	36	for implementing
134	41	61	neural network model
137	4	18	paired t- test
137	22	30	used for
137	35	55	significance testing
132	3	6	use
132	7	22	GloVe 2 vectors
132	23	27	with
132	28	42	300 dimensions
132	43	56	to initialize
132	61	76	word embeddings
135	0	2	In
135	3	17	model training
135	24	37	learning rate
135	41	47	set to
135	48	51	0.1
135	58	64	weight
135	65	68	for
135	69	94	L 2 - norm regularization
135	98	104	set to
135	105	112	1 e - 5
135	119	131	dropout rate
135	135	141	set to
135	142	145	0.5
136	3	8	train
136	13	18	model
136	19	22	use
136	23	60	stochastic gradient descent optimizer
136	61	65	with
136	66	74	momentum
136	75	77	of
136	78	81	0.9
38	71	78	propose
38	81	127	left - center - right separated neural network
38	128	132	with
38	133	175	rotatory attention mechanism ( LCR - Rot )
40	37	65	rotatory attention mechanism
40	69	86	take into account
40	91	102	interaction
40	103	110	between
40	111	131	targets and contexts
40	132	151	to better represent
40	152	172	targets and contexts
39	18	24	design
39	27	64	left - center - right separated LSTMs
39	70	78	contains
39	79	90	three LSTMs
39	93	97	i.e.
39	100	134	left - , center - and right - LSTM
39	150	158	modeling
39	163	174	three parts
39	175	177	of
39	180	186	review
39	189	201	left context
39	204	217	target phrase
39	222	235	right context
41	4	28	target2context attention
41	37	47	to capture
41	52	83	most indicative sentiment words
41	84	86	in
41	87	108	left / right contexts
42	19	43	context2target attention
42	52	62	to capture
42	67	86	most important word
42	87	89	in
42	94	100	target
43	5	13	leads to
43	16	41	two - side representation
43	42	44	of
43	49	55	target
43	58	77	left - aware target
43	82	102	right - aware target
44	13	24	concatenate
44	29	54	component representations
44	55	57	as
44	62	82	final representation
44	83	85	of
44	90	98	sentence
44	103	115	feed it into
44	118	131	softmax layer
44	132	142	to predict
44	147	165	sentiment polarity
2	51	84	Aspect - based Sentiment Analysis
16	15	33	sentiment analysis
17	54	78	sentiment classification
155	7	11	find
155	21	36	Majority method
155	37	39	is
155	44	49	worst
155	68	95	majority sentiment polarity
155	96	104	occupies
155	105	131	53.50 % , 65.00 % and 50 %
155	147	149	on
155	154	202	Restaurant , Laptop and Twitter testing datasets
156	4	20	Simple SVM model
156	21	29	performs
156	30	36	better
156	37	41	than
156	42	50	Majority
159	0	9	Our model
159	10	18	achieves
159	19	47	significantly better results
159	48	52	than
159	53	75	feature - enhanced SVM
161	63	82	basic LSTM approach
161	83	91	performs
161	96	101	worst
162	0	9	TD - LSTM
162	10	17	obtains
162	21	32	improvement
162	33	35	of
162	36	43	1 - 2 %
162	44	48	over
162	49	53	LSTM
162	54	58	when
162	59	73	target signals
162	78	88	taken into
162	89	102	consideration
165	0	6	MemNet
165	7	15	achieves
165	16	30	better results
165	31	35	than
165	36	48	other models
165	49	51	on
165	56	74	Restaurant dataset
166	0	3	IAN
166	4	13	considers
166	14	38	separate representations
166	39	41	of
166	42	49	targets
166	54	61	obtains
166	62	75	better result
166	76	78	on
166	83	97	Laptop dataset
167	0	9	GRNN - G3
167	10	18	achieves
167	19	38	competitive results
167	39	41	on
167	42	54	all datasets
168	22	37	LCR - Rot model
168	38	46	achieves
168	51	63	best results
168	64	66	on
168	71	83	all datasets
168	84	89	among
168	90	100	all models
157	0	16	With the help of
157	17	36	feature engineering
157	43	65	Feature - enhanced SVM
157	66	74	achieves
157	75	94	much better results
