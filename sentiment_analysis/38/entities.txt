49	50	66	attempt to learn
49	70	97	unsupervised representation
49	103	122	accurately contains
49	27	45	sentiment analysis
51	20	28	consider
51	41	106	research benchmark of byte ( character ) level language modelling
51	107	113	due to
51	114	151	its further simplicity and generality
53	3	11	train on
53	14	31	very large corpus
53	32	46	picked to have
53	49	69	similar distribution
53	70	72	as
53	73	93	our task of interest
13	0	23	Representation learning
99	0	25	Review Sentiment Analysis
105	0	39	The representation learned by our model
105	40	48	achieves
105	49	55	91.8 %
105	56	83	significantly outperforming
105	88	104	state of the art
105	105	107	of
105	108	114	90.2 %
105	115	117	by
105	120	137	30 model ensemble
107	3	10	matches
107	15	26	performance
107	27	29	of
107	30	39	baselines
107	40	45	using
107	58	80	dozen labeled examples
107	85	96	outperforms
107	97	117	all previous results
107	118	122	with
107	130	158	few hundred labeled examples
109	81	95	does not reach
109	100	116	state of the art
109	117	119	of
109	120	126	53.6 %
109	54	56	on
109	134	156	fine - grained subtask
109	159	168	achieving
109	169	175	52.9 %
112	0	17	L1 regularization
117	0	7	Fitting
117	10	19	threshold
117	40	48	achieves
117	51	64	test accuracy
117	65	67	of
117	68	75	92.30 %
117	82	93	outperforms
117	96	121	strong supervised results
117	143	150	91.87 %
117	151	153	of
117	154	170	NB - SVM trigram
117	180	191	still below
117	196	228	semi-supervised state of the art
117	229	231	of
117	232	239	94.09 %
118	0	5	Using
118	10	39	full 4096 unit representation
118	40	48	achieves
118	49	56	92.88 %
122	0	16	Capacity Ceiling
124	3	6	try
124	11	19	approach
124	20	22	on
124	27	41	binary version
124	42	44	of
124	49	79	Yelp Dataset Challenge in 2015
127	0	5	Using
127	10	22	full dataset
127	28	35	achieve
127	36	59	95 . 22 % test accuracy
129	4	12	observed
129	13	29	capacity ceiling
129	30	32	is
129	36	77	interesting phenomena and stumbling point
129	78	81	for
129	82	122	scaling our unsupervised representations
134	15	23	there is
134	26	38	notable drop
134	39	41	in
134	46	66	relative performance
134	67	69	of
134	70	82	our approach
134	83	101	transitioning from
134	102	131	sentence to document datasets
136	27	39	labeled data
136	40	49	increases
136	56	67	performance
136	24	26	of
136	75	94	simple linear model
136	98	113	train on top of
136	118	139	static representation
136	140	155	will eventually
136	156	164	saturate
