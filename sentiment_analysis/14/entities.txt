87	0	20	Pre-training Emo2Vec
88	0	43	Emo2 Vec embedding matrix and the CNN model
88	48	65	pre-trained using
88	66	80	hashtag corpus
89	0	23	Parameters of T and CNN
89	24	27	are
89	28	48	randomly initialized
89	53	57	Adam
89	61	69	used for
89	70	82	optimization
91	0	3	For
91	8	18	best model
91	24	27	use
91	32	42	batch size
91	43	45	of
91	46	48	16
91	51	65	embedding size
91	66	68	of
91	69	72	100
91	75	87	1024 filters
91	92	104	filter sizes
91	105	108	are
91	109	123	1 , 3 ,5 and 7
94	0	21	Multi - task training
95	3	7	tune
95	8	22	our parameters
95	23	25	of
95	26	39	learning rate
95	42	59	L2 regularization
96	3	13	early stop
96	14	23	our model
96	24	28	when
96	33	54	averaged dev accuracy
96	55	59	stop
96	60	70	increasing
97	4	14	best model
97	15	19	uses
97	20	33	learning rate
97	34	36	of
97	37	42	0.001
97	45	62	L2 regularization
97	63	65	of
97	66	69	1.0
97	72	82	batch size
97	83	85	of
97	86	88	32
98	3	7	save
98	12	22	best model
98	27	31	take
98	36	51	embedding layer
98	52	54	as
98	55	70	Emo2Vec vectors
14	10	22	demonstrates
14	27	40	effectiveness
14	41	57	of incorporating
14	58	74	sentiment labels
14	75	77	in
14	80	101	wordlevel information
14	102	105	for
14	106	131	sentiment - related tasks
14	132	143	compared to
14	144	165	other word embeddings
24	7	14	propose
24	15	22	Emo2Vec
24	31	34	are
24	35	63	word - level representations
24	64	75	that encode
24	76	95	emotional semantics
24	96	100	into
24	101	138	fixed - sized , real - valued vectors
25	7	23	propose to learn
25	24	31	Emo2Vec
25	32	36	with
25	39	68	multi-task learning framework
25	69	81	by including
25	82	119	six different emotion - related tasks
2	11	54	Learning Generalized Emotion Representation
115	30	38	Emo2 Vec
115	39	56	works better than
115	14	27	CNN embedding
115	71	73	on
115	74	90	14 / 18 datasets
115	93	99	giving
115	100	135	2.6 % absolute accuracy improvement
115	136	139	for
115	144	158	sentiment task
115	163	197	1.6 % absolute f1score improvement
115	198	200	on
115	205	216	other tasks
117	30	35	works
117	36	47	much better
117	48	50	on
117	51	63	all datasets
117	64	70	except
117	71	86	SS - T datasets
117	173	175	on
117	176	201	sentiment and other tasks
117	95	100	gives
117	101	127	3.3 % accuracy improvement
117	132	159	4.7 % f 1 score improvement
124	18	32	not trained by
124	33	60	predicting contextual words
124	15	17	is
124	69	73	weak
124	74	86	on capturing
124	87	117	synthetic and semantic meaning
132	0	16	GloVe + Emo2 Vec
132	17	25	achieves
132	26	45	better performances
132	46	48	on
132	49	61	SOTA results
132	62	64	on
132	65	115	three datasets ( SE0714 , stress and tube tablet )
132	120	137	comparable result
132	138	140	to
132	141	145	SOTA
132	146	148	on
132	149	156	dataset
116	3	8	shows
116	9	28	multi-task training
116	29	44	helps to create
116	45	92	better generalized word emotion representations
116	93	108	than just using
116	111	122	single task
121	16	21	gives
121	22	39	1.3 % improvement
121	40	42	in
121	43	51	accuracy
121	52	55	for
121	60	74	sentiment task
121	79	96	1.1 % improvement
121	97	99	of
121	100	111	f 1 - score
121	112	114	on
121	119	130	other tasks
128	33	45	solely using
128	48	65	simple classifier
128	66	70	with
128	71	95	good word representation
128	96	107	can achieve
128	108	125	promising results
131	0	13	Compared with
131	14	48	GloVe+ DeepMoji , GloVe + Emo2 Vec
131	49	57	achieves
131	58	80	same or better results
131	81	83	on
131	84	100	11 / 14 datasets
131	109	125	on average gives
131	126	143	1.0 % improvement
136	7	16	to detect
136	21	42	corresponding emotion
136	45	79	more attention needs to be paid to
136	80	85	words
