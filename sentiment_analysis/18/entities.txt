27	3	10	propose
27	11	31	two novel approaches
27	32	45	for improving
27	50	83	effectiveness of attention models
28	4	18	first approach
28	32	34	of
40	51	56	model
40	57	68	each target
40	69	71	as
40	74	81	mixture
40	85	104	K aspect embeddings
41	3	6	use
41	10	31	autoencoder structure
41	32	40	to learn
41	41	111	both the aspect embeddings as well as the representation of the target
41	112	114	as
41	117	137	weighted combination
41	138	140	of
41	145	162	aspect embeddings
43	4	25	autoencoder structure
43	29	49	jointly trained with
43	52	97	neural attention - based sentiment classifier
43	98	108	to provide
43	111	137	good target representation
43	138	148	as well as
43	151	164	high accuracy
43	165	167	on
43	172	191	predicted sentiment
46	4	19	second approach
46	20	28	exploits
46	29	50	syntactic information
46	51	63	to construct
46	66	96	syntax - based attention model
50	14	48	syntax - based attention mechanism
50	49	71	selectively focuses on
50	74	103	small subset of context words
50	113	121	close to
50	126	132	target
50	133	135	on
50	140	154	syntactic path
50	164	175	obtained by
50	176	204	applying a dependency parser
50	205	207	on
50	212	227	review sentence
165	6	25	Feature - based SVM
166	3	15	compare with
166	20	36	reported results
166	37	39	of
166	42	52	top system
166	53	55	in
166	56	68	SemEval 2014
168	6	10	LSTM
168	32	40	built on
168	41	63	top of word embeddings
2	33	72	Aspect - Level Sentiment Classification
16	64	97	fine - grained sentiment analysis
179	53	67	our best model
179	68	76	achieves
179	77	96	competitive results
179	97	99	on
179	100	109	D1 and D2
179	110	128	without relying on
179	129	188	so many manually - designed features and external resources
183	8	38	integrated full model over all
183	39	47	achieves
183	52	68	best performance
183	69	86	compared to using
183	87	126	only one of the two proposed approaches
185	8	38	proposed target representation
185	39	41	is
185	42	54	more helpful
185	55	57	on
185	58	96	restaurant domain ( D1 , D3 , and D4 )
185	97	101	than
185	102	122	laptop domain ( D2 )
180	4	17	Compared with
180	18	44	all other neural baselines
180	47	61	our full model
180	62	70	achieves
180	71	122	statistically significant improvements ( p < 0.05 )
180	123	125	on
180	126	163	both accuracies and macro - F1 scores
180	164	167	for
180	168	180	D1 , D3 , D4
181	18	28	LSTM + ATT
181	31	49	all three settings
181	50	52	of
181	53	62	our model
181	63	82	are able to achieve
181	83	134	statistically significant improvements ( p < 0.05 )
181	135	137	on
181	138	150	all datasets
