30	19	26	propose
30	29	92	novel progressive self - supervised attention learning approach
30	93	96	for
30	97	114	neural ASC models
31	14	58	able to automatically and incrementally mine
31	59	92	attention supervision information
31	93	97	from
31	100	115	training corpus
31	131	149	exploited to guide
31	154	162	training
31	163	165	of
31	166	186	attention mechanisms
31	187	189	in
31	190	200	ASC models
32	35	43	roots in
32	69	81	context word
32	82	86	with
32	91	115	maximum attention weight
32	124	139	greatest impact
32	140	142	on
32	147	167	sentiment prediction
32	168	170	of
32	174	188	input sentence
150	3	7	used
150	8	34	pre-trained Glo Ve vectors
150	35	48	to initialize
150	53	68	word embeddings
150	69	73	with
150	74	90	vector dimension
150	91	94	300
151	0	3	For
151	4	31	out - of - vocabulary words
151	37	53	randomly sampled
151	60	70	embeddings
151	71	75	from
151	80	100	uniform distribution
153	0	12	To alleviate
153	13	24	overfitting
153	30	38	employed
153	39	80	dropout strategy ( Hinton et al. , 2012 )
153	81	83	on
153	88	109	input word embeddings
153	110	112	of
153	117	121	LSTM
154	0	29	Adam ( Kingma and Ba , 2015 )
154	34	41	adopted
154	49	58	optimizer
154	59	63	with
154	68	81	learning rate
154	82	87	0.001
156	0	22	All hyper - parameters
156	28	36	tuned on
156	37	75	20 % randomly held - out training data
155	0	17	When implementing
155	18	30	our approach
155	36	51	empirically set
155	56	82	maximum iteration number K
155	83	85	as
155	86	87	5
155	109	112	0.1
155	113	115	on
155	116	131	LAPTOP data set
155	134	137	0.5
155	138	140	on
155	141	154	REST data set
155	159	162	0.1
155	163	165	on
155	166	182	TWITTER data set
2	53	86	Aspect - Level Sentiment Analysis
4	3	50	aspect - level sentiment classification ( ASC )
6	91	101	neural ASC
13	0	47	Aspect - level sentiment classification ( ASC )
13	78	96	sentiment analysis
18	46	49	ASC
174	8	45	both of our reimplemented MN and TNet
174	50	63	comparable to
174	70	85	original models
176	62	72	TNet - ATT
176	8	15	replace
176	20	23	CNN
176	24	26	of
176	27	31	TNet
176	32	36	with
176	40	59	attention mechanism
176	76	96	slightly inferior to
176	97	101	TNet
177	19	26	perform
177	27	53	additional K+1 - iteration
177	54	56	of
177	57	65	training
177	66	68	on
177	221	238	neural ASC models
177	90	101	performance
177	106	131	not changed significantly
183	18	21	use
183	22	69	both kinds of attention supervision information
183	101	112	MN ( + AS )
183	113	135	remarkably outperforms
183	136	138	MN
183	139	141	on
183	142	155	all test sets
