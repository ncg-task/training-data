title
Aspect Level Sentiment Classification with Attention - over - Attention Neural Networks
abstract
Aspect - level sentiment classification aims to identify the sentiment expressed towards some aspects given context sentences .
In this paper , we introduce an attention - over- attention ( AOA ) neural network for aspect level sentiment classification .
Our approach models aspects and sentences in a joint way and explicitly captures the interaction between aspects and context sentences .
With the AOA module , our model jointly learns the representations for aspects and sentences , and automatically focuses on the important parts in sentences .
Our experiments on laptop and restaurant datasets demonstrate our approach outperforms previous LSTM - based architectures .
Introduction
Unlike document level sentiment classification task , aspect level sentiment classification is a more fine - grained classification task .
It aims at identifying the sentiment polarity ( e.g. positive , negative , neutral ) of one specific aspect in its context sentence .
For example , given a sentence " great food but the service was dreadful " the sentiment polarity for aspects " food " and " service " are positive and negative respectively .
Aspect sentiment classification overcomes one limitation of document level sentiment classification when multiple aspects appear in one sentence .
In our previous example , there are two aspects and the general sentiment of the whole sentence is mixed with positive and negative polarity .
If we ignore the aspect information , it is hard to determine the polarity for a specified target .
Such error commonly exists in the general sentiment classification tasks .
In one recent work , Jiang et al. manually evaluated a Twitter sentiment classifier and showed that 40 % of sentiment classification errors are because of not considering targets .
Many methods have been proposed to deal with aspect level sentiment classification .
The typical way is to build a machine learning classifier by supervised training .
Among these machine learning - based approaches , there are mainly two different types .
One is to build a classifier based on manually created features .
The other type is based on neural networks using end - to - end training without any prior knowledge .
Because of its capacity of learning representations from data without feature engineering , neural networks are becoming popular in this task .
Because of advantages of neural networks , we approach this aspect level sentiment classification problem based on long short - term memory ( LSTM ) neural networks .
Previous LSTM - based methods mainly focus on modeling texts separately , while our approach models aspects and texts simultaneously using LSTMs .
Furthermore , the target representation and text representation generated from LSTMs interact with each other by an attention - over - attention ( AOA ) module .
AOA automatically generates mutual attentions not only from aspect - to - text but also text - to - aspect .
This is inspired by the observation that only few words in a sentence contribute to the sentiment towards an aspect .
Many times , those sentiment bearing words are highly correlated with the aspects .
In our previous example , there are two aspects " appetizers " and " service " in the sentence " the appetizers are ok , but the service is slow . "
Based on our language experience , we know that the negative word " slow " is more likely to describe " service " but not the " appetizers " .
Similarly , for an aspect phrase , we also need to focus on the most important part .
That is why we choose AOA to attend to the most important parts in both aspect and sentence .
Compared to previous methods , our model performs better on the laptop and restaurant datasets from SemEval 2014 2 Related work Sentiment Classification Sentiment classification aims at detecting the sentiment polarity for text .
There are various approaches proposed for this research question .
Most existing works use machine learning algorithms to classify texts in a supervision fashion .
Algorithms like Naive Bayes and Support Vector Machine ( SVM ) are widely used in this problem .
The majority of these approaches either rely on n-gram features or manually designed features .
Multiple sentiment lexicons are built for this purpose .
In the recent years , sentiment classification has been advanced by neural networks significantly .
Neural network based approaches automatically learn feature representations and do not require intensive feature engineering .
Researchers proposed a variety of neural network architectures .
Classical methods include Convolutional Neural Networks , Recurrent Neural Networks , Recursive Neural Networks .
These approaches have achieved promising results on sentiment analysis .
Aspect Level Sentiment Classification Aspect level sentiment classification is a branch of sentiment classification , the goal of which is to identify the sentiment polarity of one specific aspect in a sentence .
Some early works designed several rule based models for aspect level sentiment classification , such as .
Nasukawa et al. first perform dependency parsing on sentences , then they use predefined rules to determine the sentiment about aspects .
Jiang et al .
improve the target - dependent sentiment classification by creating several target - dependent features based on the sentences ' grammar structures .
These target - dependent features are further fed into an SVM classifier along with other content features .
Later , kinds of neural network based methods were introduced to solve this aspect level sentiment classification problem .
Typical methods are based on LSTM neural networks .
TD - LSTM approaches this problem by developing two LSTM networks to model the left and right contexts for an aspect target .
This method uses the last hidden states of these two LSTMs for predicting the sentiment .
In order to better capture the important part in a sentence , Wang et al .
use an aspect term embedding to generate an attention vector to concentrate on different parts of a sentence .
Along these lines , Ma et al. use two LSTM networks to model sentences and aspects separately .
They further use the hidden states generated from sentences to calculate attentions to aspect targets by a pooling operation , and vice versa .
Hence their IAN model can attend to both the important parts in sentences and targets .
Their method is similar to ours .
However , the pooling operation will ignore the interaction among word - pairs between sentences and targets , and experiments show our method is superior to their model .
Method
Problem Definition
In this aspect level sentiment classification problem , we are given a sentence s = [ w 1 , w 2 , ... , w i , .. , w j , ... , w n ] and an aspect target t = [ w i , w i + 1 , ... , w i +m ?1 ] .
The aspect target could be a single word or along phrase .
The goal is to classify the sentiment polarity of the aspect target in the sentence .
The over all architecture of our neural model is shown in .
It is mainly composed of four components : word embedding , Bidirectional - Long short - term memory ( Bi - LSTM ) , Attention - over - Attention module and the final prediction .
Word Embedding Given a sentence s = [ w 1 , w 2 , ... , w i , .. , w j , ... , w n ] with length n and a target t = [ w i , w i + 1 , ... , w i +m ?1 ] with length m , we first map each word into a low - dimensional real - value vector , called word embedding .
For each word w i , we can get a vector vi ?
R dw from M V dw , where V is the vocabulary size and d w is the embedding dimension .
After an embedding lookup operation , we get two sets of word vectors [ v 1 ; v 2 ; ... ; v n ] ?
R ndw and [ v i ; v i +1 ; ... ;
v i+m?1 ] ?
R mdw for the sentence and aspect phrase respectively .
Bi- LSTM
After getting the word vectors , we feed these two sets of word vectors into two Bidirectional - LSTM networks respectively .
We use these two Bi - LSTM networks to learn the hidden semantics of words in the sentence and the target .
Each Bi - LSTM is obtained by stacking two LSTM networks .
The advantage of using LSTM is that it can avoid the gradient vanishing or exploding problem and is good at learning long - term dependency .
With an input s = [ v 1 ; v 2 ; ... ; v n ] and a forward LSTM network , we generate a sequence of hidden states ? ?
h s ?
R nd h , where d h is the dimension of hidden states .
We generate another state sequence ? ?
h s by feeding s into another backward LSTM .
In the Bi - LSTM network , the final output hidden states h s ?
R n 2d hare generated by concatenating ? ?
h sand ? ? h s .
We compute the hidden semantic states ht for the aspect target tin the same way .
Attention - over - Attention Given the hidden semantic representations of the text and the aspect target generated by Bi - LSTMs , we calculate the attention weights for the text by an AOA module .
This is inspired by the use of AOA in question answering .
Given the target representation ht ?
R m 2 d hand sentence representation h s ?
R n2d h , we first calculate a pair - wise interaction matrix I = h s h T t , where the value of each entry represents the correlation of a word pair among sentence and target .
With a column - wise softmax and row - wise softmax , we get target - to - sentence attention ?
and sentence - to - target attention ?.
After column - wise averaging ? , we get a target - level attention ? ?
R m , which indicating the important parts in an aspect target .
The final sentence - level attention ? ?
Rn is calculated by a weighted sum of each individual target - to - sentence attention ? , given by equation .
By considering the contribution of each aspect word explicitly , we learn the important weights for each word in the sentence .
Final Classification
The final sentence representation is a weighted sum of sentence hidden semantic states using the sentence attention from AOA module .
We regard this sentence representation as the final classification feature and feed it into a linear layer to project r into the space of targeted C classes .
where W land bl are the weight matrix and bias respectively .
Following the linear layer , we use a softmax layer to compute the probability of the sentence s with sentiment polarity c ?
C towards an aspect a as :
The final predicted sentiment polarity of an aspect target is just the label with the highest probability .
We train our model to minimize the cross - entropy loss with L 2 regularization
where I ( ) is an indicator function . ?
is the L 2 regularization parameter and ?
is a set of weight matrices in LSTM networks and linear layer .
We further apply dropout to avoid overfitting , where we randomly drop part of inputs of LSTM cells .
We use mini-batch stochastic gradient descent with Adam update rule to minimize the loss function with respect to the weight matrices and bias terms in our model .
Experiments Dataset
We experiment on two domain - specific datasets for laptop and restaurant from SemEval 2014 Task 4 .
Experienced annotators tagged the aspect terms of the sentences and their polarities .
Distribution by sentiment polarity category are given in .
Hyperparameters Setting
In experiments , we first randomly select 20 % of training data as validation set to tune the hyperparameters .
All weight matrices are randomly initialized from uniform distribution U ( ?10 ?4 , 10 ?4 ) and all bias terms are set to zero .
The L 2 regularization coefficient is set to 10 ? 4 and the dropout keep rate is set to 0.2 .
The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .
For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .
The dimension of LSTM hidden states is set to 150 .
The initial learning rate is 0.01 for the Adam optimizer .
If the training loss does not drop after every three epochs , we decrease the learning rate by half .
The batch size is set as 25 .
Model Comparisons
We train and evaluate our model on these two SemEval datasets separately .
We use accuracy metric to measure the performance .
In order to further validate the performance of our model , we compare it with several baseline methods .
We list them as follows :
Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .
LSTM uses one LSTM network to model the sentence , and the last hidden state is used as the sentence representation for the final classification .
TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .
The last hidden states of these two LSTM network are concatenated for predicting the sentiment polarity .
AT - LSTM first models the sentence via a LSTM model .
Then it combines the hidden states from the LSTM with the aspect term embedding to generate the attention vector .
The final sentence representation is the weighted sum of the hidden states .
ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .
IAN uses two LSTM networks to model the sentence and aspect term respectively .
It uses the hidden states from the sentence to generate an attention vector for the target , and vice versa .
Based on these two attention vectors , it outputs a sentence representation and a target representation for classification .
Methods
Restaurant Laptop Majority 0.535 0.650 LSTM 0.743 0.665 TD - LSTM 0.756 0.681 AT - LSTM 0.762 0.689 ATAE - LSTM 0.772 0.687 IAN 0.786 0.721 AOA - LSTM 0.812 ( 0.7970.008 ) 0.745 ( 0.7260.008 ) .
Comparison results .
For our method , we run it 10 times and show " best ( meanstd ) " .
Performance of baselines are cited from their original papers .
In our implementation , we found that the performance fluctuates with different random initialization , which is a well - known issue in training neural networks .
Hence , we ran our training algorithms 10 times , and report the average accuracy as well as the best one we got in .
All the baseline methods only reported a single best number in their papers .
On average , our algorithm is better than these baseline methods and our best trained model outperforms them in a large margin .
Case Study
In , We list five examples from the test set .
To analyze which word contributes the most to the aspect sentiment polarity , we visualize the final sentence attention vectors ? in .
The color depth indicates the importance of a word in a sentence , the darker the more important .
In the first two examples , there are two aspects " appetizers " and " service " in the sentence " the appetizers are ok , but the service is slow . "
We can observe that when there are two aspects in the sentence , our model can automatically point to the right sentiment indicating words for each aspect .
Same thing also happens in the third and fourth examples .
In the last example , the aspect is a phrase " boot time . "
From the sentence content " boot time is super fast , around anywhere from 35 seconds to 1 minute , " this model can learn " time " is the most important word in the aspect , which further helps it find out the sentiment indicating part " super fast . " .
Examples of final attention weights for sentences .
The color depth denotes the importance degree of the weight in attention vector ?.
Error Analysis
The first type of major errors comes from non-compositional sentiment expression which also appears in previous works .
For example , in the sentence " it took about parts in the aspect and sentence , which generates the final representation of the sentence .
Experiments on SemEval 2014 datasets show superior performance of our model when compared to those baseline methods .
Our case study also shows that our model learns the important parts in the sentence as well as in the target effectively .
In our error analysis , there are cases that our model can not handle efficiently .
One is the complex sentiment expression .
One possible solution is to incorporate sentences ' grammar structures into the classification model .
Another type of error comes from uncommon idioms .
In future work , we would like to explore how to combine prior language knowledge into such neural network models .
