title
MULTI - MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKS
abstract
Emotion recognition has become an important field of research in human computer interactions and there is a growing need for automatic emotion recognition systems .
One of the directions the research is heading is the use of neural networks which are adept at estimating complex functions that depend on a large number and diverse source of input data .
In this paper we attempt to exploit this effectiveness of neural networks to enable us to perform multimodal emotion recognition on IEMOCAP dataset using data from speech , text , and motions captured from face expressions , rotation and hand movements .
Our approach first identifies best individual architectures for classification on each modality and performs fusion only at the final layer which allows for a more robust and accurate emotion detection .
INTRODUCTION
Emotion is a psycho-physiological process that can be triggered by conscious and / or unconscious perception of objects and situations , associated with multitude of factors such as mood , temperament , personality , disposition , and motivation .
Emotions are very important in human decision handling , interaction and cognitive process .
With the advancement of technology and as our understanding of emotions is advancing , there is a growing need for automatic emotion recognition systems .
Emotion recognition has been studied widely using speech [ 4 ] , text , facial cues , and EEG based brain waves individually .
One of the biggest opensourced multimodal resources available in emotion detection is IEMOCAP dataset which consists of approximately 12 hours of audio - visual data , including facial recordings , speech and text transcriptions .
In this paper we combine these modes to make a stronger and more robust detector for emotions .
We explore various deep learning based architectures to first get the best individual detection accuracy from each of the different modes .
We then combine them in an ensemble based architecture to allow for training across the different modalities using the variations of the better individual models .
Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .
This allows us to individually target each modality and only perform feature fusion at the final stage .
The advantages of our study are two - fold .
First , since we target each modality individually , lack of availability of any modality does not cripple our algorithm and would not require retraining of other modalities but only the prefinal layer .
This also allows our approach to be modular .
Second , we use Motion - capture data instead of Video recording , hence we do not use 3D - Convolutions but 2D - Convolutions which are faster have less memory requirements .
We also use advanced hyperparameter optimization tools to achieve the best possible model configuration depending on our resource constraints .
Our code is open sourced for other researchers to repeat and enhance our study .
RELATED WORKS
Most of the early research on IEMOCAP has concentrated specifically on emotion detection using speech data .
One of the early important papers on this dataset is where they used segment level feature extraction , to feed those features to a MLP based architecture , where the input is 750 dimensional feature vector , followed by 3 hidden layer of 256 neurons each with rectilinear units as non-linearity .
follows and they train long short - term memory ( LSTM ) based recurrent neural network .
First they divide each utterance into small segments with voiced region , then assume that the label sequences of each segment follows a Markov chain .
They extract 32 features for every frame with 12 - dimensional Mel- frequency cepstral coefficients ( MFCC ) with log energy , and their first time derivatives among others .
The network contains 2 hidden layers with 128 BLSTM cells ( 64 forward nodes and 64 backward nodes ) .
Another research we closely follow is , where they use CTC loss function to improve upon RNN based Emotion prediction .
They use 34 features including 12 MFCC , chromagram - based and spectrum properties like flux and rolloff .
For all speech intervals they calculate features in 0.2 second window and moving it with 0.1 second step .
The use of CTC loss helps , as often , almost the whole utterance has no emotion , but emotionality is contained only in a few words or phonemes in an utterance which the CTC loss handles well .
Unlike which uses only the improv data , Chernykh et .
al. use all the session data for the emotion classification .
Multi-modal emotion classification has recently gathered more traction and IEMOCAP remains the significant dataset for this research direction .
The current state - of - art classification on IEMOCAP is provided by which builds on the prior work .
They use 3D - CNN for visual feature extraction , text - CNN for textual features extraction and open SMILE for audio feature extraction .
They use Contextual LSTM Architecture on top of these unimodal input features .
They are then merged with multi-modal contextual LSTM layers which performs feature fusion .
This layer finally feeds to the classification module .
In our paper we adopt a different approach to this study and achieve similar performance with certain advantages .
EXPERIMENTAL SETUP
IEMOCAP has 12 hours of audio- visual data from 10 actors where the recordings follow dialogues between a male and a female actor in both scripted or improvised topics .
After the audio - visual data has been collected it is divided into small utterances of length between 3 to 15 seconds which are then labelled by evaluators .
Each utterance is evaluated by 3 - 4 assessors .
The evaluation form contained 10 options ( neutral , happiness , sadness , anger , surprise , fear , disgust frustration , excited , other ) .
We consider only 4 of them anger , excitement ( happiness ) , neutral and sadness so as to remain consistent with the prior research .
We consider emotions where atleast 2 experts were consistent with their decision , which is more than 70 % of the dataset , consistent with prior research .
Along with the . wav file for the dialogue we also have the transcript for each the utterance .
For each session , one actor wears the Motion Capture ( MoCap ) camera data which records the facial expression , head and hand movements of the actor .
The Mocap data contains column tuples , for facial expressions the tuples are contained in 165 dimensions , 18 for hand positions and 6 for head rotations .
As this Mocap data is very extensive we use it instead of the video recording in the dataset .
These three modes ( Speech , Text , Mocap ) of data form the basis of our multi-modal emotion detection pipeline .
Next we preprocess the IEMOCAP data for these modes .
For the speech data our preprocessing follows the work of .
We use the Fourier frequencies and energy - based features Mel- frequency Cepstral Coefficients ( MFCC ) for a total of 34 features .
They include 13 MFCC , 13 chromagram - based and 8 Time Spectral Features like zero crossing rate , short - term energy , short - term entropy of energy , spectral centroid and spread , spectral entropy , spectral flux , spectral rolloff .
We calculate features in 0.2 second window and moving it with 0.1 second step and with 16 k Hz sample rate .
We keep a maximum of 100 frames or approximately for 10 seconds of the input , and zero pad the extra signal and end up with ( 100 , 34 ) feature vector for each utterance .
We also experiment with delta and double - delta features of MFCC but they do nt produce any performance improvement while adding extra computation overhead .
For the text transcript of each of the utterance we use pretrained Glove embeddings of dimension 300 , along with the maximum sequence length of 500 to obtain a ( 500,300 ) vector for each utterance .
For the Mocap data , for each different mode such as face , hand , head rotation we sample all the feature values between the start and finish time values and split them into 200 partitioned arrays .
We then average each of the 200 arrays along the columns ( 165 for faces , 18 for hands , and 6 for rotation ) , and finally concatenate all of them to obtain ( 200,189 ) dimension vector for each utterance .
The total dataset consists of 4936 dialogues .
For individual modalities we divide our dataset with a randomly chosen 20 % validation splits .
For the final combined model we use 3838 ( 77.7 % ) as our training set , these correspond to first 4 sessions of the data with 8 actors .
We use the final 1098 ( 22.2 % ) dialogues as our test set .
These correspond to Session 5 , with 2 actors ( Male and Female ) .
This ensures we remain speaker agnostic in our predictions .
Unlike we do not use 10 - fold cross validation , since cross validation on Neural Networks is unfeasible due to time and compute requirements .
For HyperParameter Optimization ( HPO ) we use Auptimizer 1 an open - sourced HPO tool .
We use 838 dialogues from 3838 training set as our validation set for HPO .
Once best parameters have been found we use both training and validation to evaluate on the test set .
MODELS
Speech Based Emotion Detection
Our first model - Speech Model1 consists of three layered fully connected MLP layers with 1024 , 512 , 256 hidden neural units with Relu as activation and 4 output neurons with We also try many variations of the speech data including using MelSpectrogram , smaller window ( 0.08 s ) with longer context ( 200 timestamps ) but do not achieve improvements .
To compare our results with prior research we use our best model ( Speech Model4 ) and evaluate it in the manner similar to various conditions of the previous researches .
Like we use only the improvis ation session for both training and testing and achieve similar results .
To compare with [ 5 ] who use the both scripted and improvis ation sessions we again achieve similar results .
One important insight of our results is with minimal preprocessing and no complex loss functions or noise injection into the training , we can easily match prior research 's performance using Attention based Bidirectional LSTMs .
Text based Emotion Recognition
Our task of performing emotion detection using only the text transcripts of our data resembles that of sentiment analysis , a Poria 71.59 %
RESULTS
Combined Multi-Modal Emotion Detection
For our final model we take the best individual models for each modality without their final softmax layers .
The Text Model2 with stacked LSTMs and Glove word embeddings is chosen for text modality , Speech Model4 for the speech modality with 2 stacked bidirections LSTMs with Attention , and combined Mocap Model1 with stacked convolution layers .
We then perform feature fusion by concatenating their final fully connected layers .
We add another final fullyconnected layer with 256 neurons followed by a softmax layer .
This forms our combined final model .
We then perform hyperparamter optimization on this model .
We choose the number of LSTM neurons in Speech Model4 , LSTM neurons in Text Model2 , neurons of the final fully connected layer of the combined model and net Dropout on all the models as hyperparameters .
We use Random proposer to optimize training on the validation set .
We then evaluate the best hyperparameter configuration on the test set , using train and validation set .
Our performance matches the prior state of the art , however the comparison is not fair .
use 10 fold cross validation , while we useless training data in a 77 % - 22 % split .
CONCLUSION
In this paper we perform multimodal emotion recognition on IEMOCAP dataset using data from speech , text , and motions capture and identify best individual architectures for classification on each modality .
We perform fusion only at the final layer which allows for a more robust and accurate emotion detection .
Our approach has certain advantages .
Firstly , since we only perform fusion at the final stage , lack of a modality would only require retraining the final fully connected layer .
Also since we optimize individual modalities our combined model has a modular approach .
This allows any individual model to be replaced by a better model , without affecting rest of the modalities in the combined model .
Secondly , since we use motion captured data and 2D convolutions instead of video recordings and 3D convolutions , we have a faster training and inference time .
