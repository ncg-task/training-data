137	0	8	Majority
137	9	16	assigns
137	21	39	sentiment polarity
137	40	44	with
137	45	70	most frequent occurrences
137	71	73	in
137	78	90	training set
137	91	93	to
137	94	105	each sample
137	106	108	in
137	109	117	test set
139	0	22	Bi - LSTM and Bi - GRU
139	23	28	adopt
139	31	63	Bi - LSTM and a Bi - GRU network
139	64	72	to model
139	77	85	sentence
139	90	93	use
139	98	110	hidden state
139	111	113	of
139	118	128	final word
139	129	132	for
139	133	143	prediction
141	0	9	TD - LSTM
141	10	16	adopts
141	17	26	two LSTMs
141	27	35	to model
141	40	52	left context
141	53	57	with
141	58	64	target
141	73	86	right context
141	87	91	with
141	92	98	target
141	117	122	takes
141	127	140	hidden states
141	141	143	of
141	144	148	LSTM
141	149	151	at
141	152	168	last time - step
141	169	181	to represent
141	186	194	sentence
141	195	198	for
141	199	209	prediction
143	0	6	MemNet
143	7	14	applies
143	15	24	attention
143	25	39	multiple times
143	40	42	on
143	47	62	word embeddings
143	73	79	output
143	80	82	of
143	83	97	last attention
143	101	107	fed to
143	108	115	softmax
143	116	119	for
143	120	130	prediction
145	0	3	IAN
145	18	24	learns
145	25	35	attentions
145	36	38	in
145	43	63	contexts and targets
145	70	79	generates
145	84	99	representations
145	100	103	for
145	104	124	targets and contexts
147	0	3	RAM
147	6	8	is
147	11	34	multilayer architecture
147	35	40	where
147	41	51	each layer
147	52	63	consists of
147	64	93	attention - based aggregation
147	94	96	of
147	97	110	word features
147	117	125	GRU cell
147	126	134	to learn
147	139	162	sentence representation
149	0	9	LCR - Rot
149	10	17	employs
149	18	33	three Bi- LSTMs
149	34	42	to model
149	47	59	left context
149	66	72	target
149	81	94	right context
152	0	10	AOA - LSTM
152	11	21	introduces
152	25	74	attention - over- attention ( AOA ) based network
152	75	83	to model
152	84	105	aspects and sentences
152	106	108	in
152	111	120	joint way
152	125	143	explicitly capture
152	148	159	interaction
152	160	167	between
152	168	197	aspects and context sentences
128	3	6	use
128	7	35	300 - dimension word vectors
128	36	50	pre-trained by
128	51	56	GloVe
129	0	31	All out - of - vocabulary words
129	36	50	initialized as
129	51	63	zero vectors
129	70	80	all biases
129	85	91	set to
129	92	96	zero
130	4	14	dimensions
130	57	63	set to
130	64	67	300
130	15	17	of
130	18	52	hidden states and fused embeddings
131	4	13	dimension
131	14	16	of
131	17	36	position embeddings
131	40	46	set to
131	47	49	50
132	0	5	Keras
132	14	30	for implementing
132	35	55	neural network model
134	4	18	paired t- test
134	22	30	used for
134	35	55	significance testing
133	0	2	In
133	3	17	model training
133	23	26	set
133	31	44	learning rate
133	45	47	to
133	48	53	0.001
133	60	70	batch size
133	71	73	to
133	74	76	64
133	83	95	dropout rate
133	96	98	to
133	99	102	0.5
37	49	56	propose
37	59	118	hierarchical attention based positionaware network ( HAPN )
37	119	122	for
37	123	162	aspect - level sentiment classification
38	2	33	position - aware encoding layer
38	48	61	for modelling
38	66	74	sentence
38	75	85	to achieve
38	90	130	position - aware abstract representation
38	131	133	of
38	134	143	each word
39	18	43	succinct fusion mechanism
39	55	66	proposed to
39	67	71	fuse
39	76	90	information of
39	91	115	aspects and the contexts
39	118	127	achieving
39	132	161	final sentence representation
40	13	17	feed
40	22	54	achieved sentence representation
40	55	59	into
40	62	75	softmax layer
40	76	86	to predict
40	91	109	sentiment polarity
43	34	90	https://github.com/DUT-LiuYang/Aspect-Sentiment-Analysis
2	56	87	Aspect-level Sentiment Analysis
12	15	33	sentiment analysis
155	10	25	TD - LSTM model
155	43	66	shown to be better than
155	67	71	LSTM
155	74	78	gets
155	83	100	worst performance
155	101	103	of
155	104	124	all RNN based models
155	133	141	accuracy
155	142	153	achieved by
155	154	163	TD - LSTM
155	164	166	is
155	167	183	2.94 % and 2.4 %
155	184	194	lower than
155	204	213	Bi - LSTM
160	0	10	Our method
160	11	19	achieves
160	20	30	accuracies
160	31	33	of
160	34	41	82.23 %
160	42	52	as well as
160	53	62	77 . 27 %
160	63	65	on
160	70	99	Restaurant and Laptop dataset
172	42	55	Bi - GRU - PW
172	56	64	performs
172	65	75	even worse
172	76	80	than
172	81	89	Bi - GRU
173	4	12	accuracy
173	13	24	achieved by
173	25	38	Bi - GRU - PW
173	39	41	is
173	42	48	0.72 %
173	49	59	as well as
173	60	66	1.41 %
173	67	77	lower than
173	86	94	Bi - GRU
173	95	97	on
173	102	131	Restaurant and Laptop dataset
180	0	4	HAPN
180	5	13	achieves
180	14	25	improvement
180	26	28	of
180	29	46	0.35 % and 0.78 %
180	47	49	on
180	50	58	accuracy
192	10	38	information fusion operation
192	47	54	used to
192	55	64	calculate
192	69	99	Source2context attention value
193	4	10	output
193	11	13	of
193	14	37	Source2aspect attention
193	46	54	used for
193	55	73	information fusion
195	28	41	Bi - GRU - PE
195	62	71	achieving
195	76	86	accuracies
195	87	89	of
195	90	109	80.89 % and 76.02 %
195	110	112	on
195	117	129	two datasets
159	6	19	Compared with
159	24	54	state - of - the - art methods
159	57	66	our model
159	67	75	achieves
159	80	96	best performance
170	6	17	introducing
170	22	41	position embeddings
170	48	56	accuracy
170	57	63	has an
170	64	72	increase
170	73	75	of
170	76	93	0.62 % and 2.67 %
170	94	96	on
170	97	109	two datasets
