title
Attentional Encoder Network for Targeted Sentiment Classification
abstract
Targeted sentiment classification aims at determining the sentimental tendency towards specific targets .
Most of the previous approaches model context and target words with RNN and attention .
However , RNNs are difficult to parallelize and truncated backpropagation through time brings difficulty in remembering long - term patterns .
To address this issue , this paper proposes an Attentional Encoder Network ( AEN ) which eschews recurrence and employs attention based encoders for the modeling between context and target .
We raise the label unreliability issue and introduce label smoothing regularization .
We also apply pretrained BERT to this task and obtain new stateof - the - art results .
Experiments and analysis demonstrate the effectiveness and lightweight of our model .
Introduction
Targeted sentiment classification is a fine - grained sentiment analysis task , which aims at determining the sentiment polarities ( e.g. , negative , neutral , or positive ) of a sentence over " opinion targets " that explicitly appear in the sentence .
For example , given a sentence " I hated their service , but their food was great " , the sentiment polarities for the target " service " and " food " are negative and positive respectively .
A target is usually an entity or an entity aspect .
In recent years , neural network models are designed to automatically learn useful lowdimensional representations from targets and contexts and obtain promising results .
However , these neural network models are still in infancy to deal with the fine - grained targeted sentiment classification task .
Attention mechanism , which has been successfully used in machine translation , is incorporated to enforce the model to pay more attention to context words with closer semantic relations with the target .
There are already some studies use attention to generate targetspecific sentence representations or to transform sentence representations according to target words .
However , these studies depend on complex recurrent neural networks ( RNNs ) as sequence encoder to compute hidden semantics of texts .
The first problem with previous works is that the modeling of text relies on RNNs .
RNNs , such as LSTM , are very expressive , but they are hard to parallelize and backpropagation through time ( BPTT ) requires large amounts of memory and computation .
Moreover , essentially every training algorithm of RNN is the truncated BPTT , which affects the model 's ability to capture dependencies over longer time scales .
Although LSTM can alleviate the vanishing gradient problem to a certain extent and thus maintain long distance information , this usually requires a large amount of training data .
Another problem that previous studies ignore is the label unreliability issue , since neutral sentiment is a fuzzy sentimental state and brings difficulty for model learning .
As far as we know , we are the first to raise the label unreliability issue in the targeted sentiment classification task .
This paper propose an attention based model to solve the problems above .
Specifically , our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words .
To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels .
We also apply pre-trained BERT to this task and show our model enhances the performance of basic BERT model .
Experimental results on three benchmark datasets show that the proposed model achieves competitive performance and is a lightweight alternative of the best RNN based models .
The main contributions of this work are presented as follows :
1 .
We design an attentional encoder network to draw the hidden states and semantic interactions between target and context words .
2 .
We raise the label unreliability issue and add an effective label smoothing regularization term to the loss function for encouraging the model to be less confident with the training labels .
3 .
We apply pre-trained BERT to this task , our model enhances the performance of basic BERT model and obtains new state - of - the - art results .
4 .
We evaluate the model sizes of the compared models and show the lightweight of the proposed model .
Related Work
The research approach of the targeted sentiment classification task including traditional machine learning methods and neural networks methods .
Traditional machine learning methods , including rule - based methods and statistic - based methods , mainly focus on extracting a set of features like sentiment lexicons features and bag - of - words features to train a sentiment classifier ) .
The performance of these methods highly depends on the effectiveness of the feature engineering works , which are labor intensive .
In recent years , neural network methods are getting more and more attention as they do not need handcrafted features and can encode sentences with low - dimensional word vectors where rich semantic information stained .
In order to incorporate target words into a model , propose TD - LSTM to extend LSTM by using two single - directional LSTM to model the left context and right context of the target word respectively .
design MemNet which consists of a multi-hop attention mechanism with an external memory to capture the importance of each context word concerning the given target .
Multiple attention is paid to the memory represented byword embeddings to build higher semantic information .
propose ATAE - LSTM which concatenates target embeddings with word representations and let targets participate in computing attention weights .
propose RAM which adopts multiple - attention mechanism on the memory built with bidirectional LSTM and nonlinearly combines the attention results with gated recurrent units ( GRU s ) .
propose IAN which learns the representations of the target and context with two attention networks interactively .
Proposed Methodology
The goal of this model is to predict the sentiment polarity of the sentence w cover the target wt . illustrates the over all architecture of the proposed Attentional Encoder Network ( AEN ) , which mainly consists of an embedding layer , an attentional encoder layer , a target - specific attention layer , and an output layer .
Embedding layer has two types :
Glo Ve embedding and BERT embedding .
Accordingly , the models are named AEN - GloVe and AEN - BERT .
Embedding Layer
Glo Ve Embedding
Let L ?
Rd emb | V | to be the pre-trained GloVe embedding matrix , where d emb is the dimension of word vectors and | V | is the vocabulary size .
Then we map each word w i ?
R | V | to it s corresponding embedding vector e i ?
Rd emb 1 , which is a column in the embedding matrix L.
BERT
Embedding
BERT embedding uses the pre-trained BERT to generate word vectors of sequence .
In order to facilitate the training and fine - tuning of BERT model , we transform the given context and target to " [ CLS ] + context + [ SEP ] " and " [ CLS ] + target + [ SEP ] " respectively .
Attentional Encoder Layer
The attentional encoder layer is a parallelizable and interactive alternative of LSTM and is applied to compute the hidden states of the input embeddings .
This layer consists of two submodules : the Multi - Head Attention ( MHA ) and the Point - wise Convolution Transformation ( PCT ) .
Multi - Head Attention
Multi - Head Attention ( MHA ) is the attention that can perform multiple attention function in parallel .
Different from Transformer ( Vaswani et al. , 2017 ) , we use Intra - MHA for introspective context words modeling and Inter - MHA for contextperceptive target words modeling , which is more lightweight and target is modeled according to a given context .
An attention function maps a key sequence k = {k 1 , k 2 , ... , kn } and a query sequence q = {q 1 , q 2 , ... , q m } to an output sequence o:
where f s denotes the alignment function which learns the semantic relevance between q j and k i :
where Watt ?
R 2 d hid are learnable weights .
MHA can learn n head different scores in parallel child spaces and is very powerful for alignments .
The n head outputs are concatenated and projected to the specified hidden dimension d hid , namely ,
Intra - MHA , or multi-head self - attention , is a special situation for typical attention mechanism that q = k.
Given a context embedding e c , we can get the introspective context representation c intra by :
The learned context representation c intra = {c intra 1 , c intra 2 , ... , c intra n } is aware of long - term dependencies .
Inter - MHA is the generally used form of attention mechanism that q is different from k.
Given a context embedding e c and a target embedding e t , we can get the context - perceptive target representation t inter by :
After this interactive procedure , each given target word e t j will have a composed representation selected from context embeddings e c .
Then we get the context - perceptive target words modeling t inter = {t inter 1 , t inter 2 , ... , t inter m }.
Point - wise Convolution Transformation
A Point- wise Convolution
T ransformation ( PCT ) can transform contextual information gathered by the MHA .
Point - wise means that the kernel sizes are 1 and the same transformation is applied to every single token belonging to the input .
Formally , given a input sequence h , PCT is defined as :
where ?
stands for the ELU activation , * is the convolution operator , W 1 pc ?
Rd hid d hid and W 2 pc ?
Rd hid d hid are the learnable weights of the two convolutional kernels , b 1 pc ?
Rd hid and b 2 pc ?
Rd hid are biases of the two convolutional kernels .
Given c intra and t inter , PCTs are applied to get the output hidden states of the attentional encoder layer h c = {h c 1 , h c 2 , ... , h c n } and ht = {h t 1 , ht 2 , ... , ht m } by :
Target - specific Attention Layer
After we obtain the introspective context representation h c and the context - perceptive target representation ht , we employ another MHA to obtain the target - specific context representation h tsc = {h tsc 1 , h tsc 2 , ... , h tsc m } by :
The multi-head attention function here also has its independent parameters .
Output Layer
We get the final representations of the previous outputs by average pooling , concatenate them as the final comprehensive representation , and use a full connected layer to project the concatenated vector into the space of the targeted C classes .
where y ?
RC is the predicted sentiment polarity distribution , W o ?
R 1 C andb o ?
RC are learnable parameters .
Regularization and Model Training
Since neutral sentiment is a very fuzzy sentimental state , training samples which labeled neutral are unreliable .
We employ a Label Smoothing Regularization ( LSR ) term in the loss function .
which penalizes low entropy output distributions .
LSR can reduce overfitting by preventing a network from assigning the full probability to each training example during training , replaces the 0 and 1 targets for a classifier with smoothed values like 0.1 or 0.9 .
For a training sample x with the original ground - truth label distribution q ( k | x ) , we replace q ( k| x ) with
where u ( k ) is the prior distribution over labels , and is the smoothing parameter .
In this paper , we set the prior label distribution to be uniform u ( k ) = 1/C. LSR is equivalent to the KL divergence between the prior label distribution u ( k ) and the network 's predicted distribution p ? .
Formally , LSR term is defined as :
The objective function ( loss function ) to be optimized is the cross - entropy loss with L lsr and L 2 regularization , which is defined as :
where ? ?
RC is the ground truth represented as a one - hot vector , y is the predicted sentiment distribution vector given by the output layer , ?
is the coefficient for L 2 regularization term , and ?
is the parameter set .
Experiments
Datasets and Experimental Settings
We conduct experiments on three datasets :
Se-m Eval 2014 Task 4 2 dataset composed of Restaurant reviews and Laptop reviews , and ACL 14 Twitter dataset gathered by .
These datasets are labeled with three sentiment polarities : positive , neutral and negative .
shows the number of training and test instances in each category .
Word embeddings in AEN - Glo Ve do not get updated in the learning process , but we fine - tune pre-trained BERT 3 in AEN - BERT .
Embedding dimension d dim is 300 for GloVe and is 768 for pretrained BERT .
Dimension of hidden states d hid is set to 300 .
The weights of our model are initialized with Glorot initialization .
During training , we set label smoothing parameter to 0.2 , the coefficient ? of L 2 regularization item is 10 ? 5 and dropout rate is 0.1 .
Adam optimizer ( Kingma and Ba , 2014 ) is applied to update all the parameters .
We adopt the Accuracy and Macro - F1 metrics to evaluate the performance of the model .
Model Comparisons
In order to comprehensively evaluate and analysis the performance of AEN - GloVe , we list 7 baseline models and design 4 ablations of AEN - Glo Ve .
We also design a basic BERT - based model to evaluate the performance of AEN - BERT .
Non - RNN based baselines :
Feature - based SVM is a traditional support vector machine based model with extensive feature engineering .
Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .
MemNet uses multi-hops of attention layers on the context word embeddings for sentence representation to explicitly captures the importance of each context word .
RNN based baselines :
TD - LSTM extends LSTM by using two LSTM networks to model the left context with target and the right context with target respectively .
The left and right target - dependent representations are concatenated for predicting the sentiment polarity of the target .
ATAE - LSTM
( Wang et al. , 2016 ) strengthens the effect of target embeddings , which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification .
IAN learns the representations of the target and context with two LSTMs and attentions interactively , which generates the representations for targets and contexts with respect to each other .
RAM strengthens Mem - Net by representing memory with bidirectional LSTM and using a gated recurrent unit network to combine the multiple attention outputs for sentence representation .
AEN - Glo Ve ablations :
AEN - GloVe w/ o PCT ablates PCT module .
AEN - GloVe w/ o MHA ablates MHA module .
AEN - GloVe w/ o LSR ablates label smoothing regularization .
AEN-GloVe-BiLSTM replaces the attentional encoder layer with two bidirectional LSTM .
Basic BERT - based model :
BERT - SPC feeds sequence " [ CLS ] + context + [ SEP ] + target + [ SEP ] " into the basic BERT model for sentence pair classification task .
shows the performance comparison of AEN with other models .
BERT - SPC and AEN - BERT obtain substantial accuracy improvements , which shows the power of pre-trained BERT on small - data task .
The over all performance of AEN - BERT is better than BERT - SPC , which suggests that it is important to design a downstream network customized to a specific task .
As the prior knowledge in the pre-trained BERT is not specific to any particular domain , further fine - tuning on the specific task is necessary for releasing the true power of BERT .
Main Results
The over all performance of TD - LSTM is not good since it only makes a rough treatment of the target words .
ATAE - LSTM , IAN and RAM are attention based models , they stably exceed the TD - LSTM method on Restaurant and Laptop datasets .
RAM is better than other RNN based models , but it does not perform well on Twitter dataset , which might because bidirectional LSTM is not good at modeling small and ungrammatical text .
Feature - based SVM is still a competitive baseline , but relying on manually - designed features .
Rec - NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments .
Like AEN , Mem Net also eschews recurrence , but its over all performance is not good since it does not model the hidden semantic of embeddings , and the result of the last attention is essentially a linear combination of word embeddings .
Model Analysis
As shown in , the performances of AEN - Glo Ve ablations are incomparable with AEN - Glo Ve in both accuracy and macro - F1 measure .
This result shows that all of these discarded components are crucial for a good performance .
Comparing the results of AEN - GloVe and AEN - Glo Ve w / o LSR , we observe that the accuracy of AEN - Glo Ve w / o LSR drops significantly on all three datasets .
We could attribute this phenomenon to the unreliability of the training samples with neutral sentiment .
The over all performance of AEN - GloVe and AEN - Glo Ve - BiLSTM is relatively close , AEN - Glo Ve performs better on the Restaurant dataset .
More importantly , AEN - Glo Ve has fewer parameters and is easier to parallelize .
To figure out whether the proposed AEN - Glo Ve is a lightweight alternative of recurrent models , we study the model size of each model on the Restaurant dataset .
Statistical results are reported in .
We implement all the compared models base on the same source code infrastructure , use the same hyperparameters , and run them on the same GPU 4 .
RNN - based and BERT - based models indeed have larger model size .
ATAE - LSTM , IAN , RAM , and AEN - GloVe - BiLSTM are all attention based RNN models , memory optimization for these models will be more difficult as the encoded hidden states must be kept simultaneously in memory in order to perform attention mechanisms .
MemNet has the lowest model size as it only has one shared attention layer and two linear layers , it does not calculate hidden states 4 NVIDIA GTX 1080ti. of word embeddings .
AEN - Glo Ve 's lightweight level ranks second , since it takes some more parameters than MemNet in modeling hidden states of sequences .
As a comparison , the model size of AEN - Glo Ve - BiLSTM is more than twice that of AEN - GloVe , but does not bring any performance improvements .
