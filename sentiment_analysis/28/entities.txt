169	0	24	Comparing the results of
169	25	63	AEN - GloVe and AEN - Glo Ve w / o LSR
169	69	76	observe
169	86	94	accuracy
169	95	97	of
169	98	120	AEN - Glo Ve w / o LSR
169	121	126	drops
169	127	140	significantly
169	141	143	on
169	144	162	all three datasets
171	4	24	over all performance
171	25	27	of
171	28	65	AEN - GloVe and AEN - Glo Ve - BiLSTM
171	66	68	is
171	69	85	relatively close
171	88	100	AEN - Glo Ve
171	101	109	performs
171	110	116	better
171	117	119	on
171	124	142	Restaurant dataset
172	19	31	AEN - Glo Ve
172	36	52	fewer parameters
172	60	69	easier to
172	70	81	parallelize
179	0	33	AEN - Glo Ve 's lightweight level
179	34	39	ranks
179	40	46	second
180	22	32	model size
180	33	35	of
180	36	57	AEN - Glo Ve - BiLSTM
180	61	70	more than
180	71	76	twice
180	82	84	of
180	85	96	AEN - GloVe
180	103	117	does not bring
180	118	146	any performance improvements
136	8	14	design
136	17	41	basic BERT - based model
136	42	53	to evaluate
136	58	69	performance
136	70	72	of
136	73	83	AEN - BERT
137	0	25	Non - RNN based baselines
138	0	19	Feature - based SVM
138	20	22	is
138	25	71	traditional support vector machine based model
138	72	76	with
138	77	106	extensive feature engineering
139	0	8	Rec - NN
139	17	21	uses
139	22	27	rules
139	28	40	to transform
139	45	60	dependency tree
139	65	68	put
139	73	87	opinion target
139	88	90	at
139	95	99	root
139	111	117	learns
139	122	145	sentence representation
139	146	152	toward
139	153	159	target
139	160	163	via
139	164	184	semantic composition
139	185	190	using
139	191	204	Recursive NNs
140	0	6	MemNet
140	7	11	uses
140	12	42	multi-hops of attention layers
140	43	45	on
140	50	73	context word embeddings
140	74	77	for
140	78	101	sentence representation
140	102	124	to explicitly captures
140	129	139	importance
140	140	142	of
140	143	160	each context word
141	0	19	RNN based baselines
142	0	9	TD - LSTM
142	10	17	extends
142	18	22	LSTM
142	23	31	by using
142	32	49	two LSTM networks
142	50	58	to model
142	63	75	left context
142	76	80	with
142	81	87	target
142	96	109	right context
142	110	114	with
142	115	121	target
144	0	11	ATAE - LSTM
145	23	34	strengthens
145	39	66	effect of target embeddings
145	69	82	which appends
145	87	104	target embeddings
145	105	109	with
145	110	130	each word embeddings
145	135	138	use
145	139	143	LSTM
145	144	148	with
145	149	158	attention
145	159	165	to get
145	170	190	final representation
145	191	194	for
145	195	209	classification
146	0	3	IAN
146	4	10	learns
146	15	30	representations
146	31	33	of
146	38	56	target and context
146	57	61	with
146	62	86	two LSTMs and attentions
146	103	118	which generates
146	123	138	representations
146	139	142	for
146	143	163	targets and contexts
146	164	179	with respect to
146	180	190	each other
147	0	3	RAM
147	4	15	strengthens
147	16	25	Mem - Net
147	26	41	by representing
147	42	48	memory
147	49	53	with
147	54	72	bidirectional LSTM
147	77	82	using
147	85	113	gated recurrent unit network
147	114	124	to combine
147	129	155	multiple attention outputs
147	156	159	for
147	160	183	sentence representation
148	0	22	AEN - Glo Ve ablations
149	0	20	AEN - GloVe w/ o PCT
149	21	28	ablates
149	29	39	PCT module
150	0	20	AEN - GloVe w/ o MHA
150	21	28	ablates
150	29	39	MHA module
151	0	20	AEN - GloVe w/ o LSR
151	21	28	ablates
151	29	59	label smoothing regularization
152	0	16	AEN-GloVe-BiLSTM
152	17	25	replaces
152	30	55	attentional encoder layer
152	56	60	with
152	61	83	two bidirectional LSTM
153	0	24	Basic BERT - based model
154	0	10	BERT - SPC
154	11	16	feeds
154	17	76	sequence " [ CLS ] + context + [ SEP ] + target + [ SEP ] "
154	77	81	into
154	86	102	basic BERT model
154	103	106	for
154	107	140	sentence pair classification task
126	0	5	shows
126	10	47	number of training and test instances
126	48	50	in
126	51	64	each category
127	0	15	Word embeddings
127	16	18	in
127	19	31	AEN - Glo Ve
127	32	53	do not get updated in
127	58	74	learning process
127	84	95	fine - tune
127	96	112	pre-trained BERT
127	115	117	in
127	118	128	AEN - BERT
128	0	25	Embedding dimension d dim
128	26	28	is
128	29	32	300
128	33	36	for
128	37	42	GloVe
128	50	53	768
128	54	57	for
128	58	73	pretrained BERT
129	0	9	Dimension
129	10	12	of
129	13	32	hidden states d hid
129	36	42	set to
129	43	46	300
130	4	11	weights
130	12	14	of
130	15	24	our model
130	29	45	initialized with
130	46	67	Glorot initialization
132	0	39	Adam optimizer ( Kingma and Ba , 2014 )
132	43	53	applied to
132	54	60	update
132	61	79	all the parameters
131	0	6	During
131	7	15	training
131	21	24	set
131	25	50	label smoothing parameter
131	51	53	to
131	54	57	0.2
131	64	104	coefficient ? of L 2 regularization item
131	105	107	is
131	108	114	10 ? 5
131	119	131	dropout rate
131	132	134	is
131	135	138	0.1
26	11	18	propose
26	22	43	attention based model
27	15	24	our model
27	25	32	eschews
27	33	43	recurrence
27	48	55	employs
27	56	65	attention
27	66	68	as
27	71	94	competitive alternative
27	95	102	to draw
27	107	146	introspective and interactive semantics
27	147	154	between
27	155	179	target and context words
28	0	12	To deal with
28	17	42	label unreliability issue
28	48	54	employ
28	57	87	label smoothing regularization
28	88	100	to encourage
28	105	110	model
28	111	116	to be
28	117	131	less confident
28	132	136	with
28	137	149	fuzzy labels
29	8	13	apply
29	14	30	pre-trained BERT
2	32	65	Targeted Sentiment Classification
12	39	72	fine - grained sentiment analysis
16	76	124	fine - grained targeted sentiment classification
160	4	24	over all performance
160	25	27	of
160	28	37	TD - LSTM
160	38	40	is
160	41	49	not good
160	64	69	makes
160	72	87	rough treatment
160	88	90	of
160	95	107	target words
161	0	25	ATAE - LSTM , IAN and RAM
161	26	29	are
161	30	52	attention based models
161	60	73	stably exceed
161	78	94	TD - LSTM method
161	95	97	on
161	98	128	Restaurant and Laptop datasets
162	0	3	RAM
162	7	18	better than
162	19	41	other RNN based models
162	51	67	does not perform
162	68	72	well
162	73	75	on
162	76	91	Twitter dataset
163	0	19	Feature - based SVM
163	23	28	still
163	31	51	competitive baseline
163	58	68	relying on
163	69	97	manually - designed features
164	0	8	Rec - NN
164	9	13	gets
164	18	36	worst performances
164	37	42	among
164	43	71	all neural network baselines
165	0	4	Like
165	5	8	AEN
165	11	18	Mem Net
165	24	31	eschews
165	32	42	recurrence
165	53	73	over all performance
165	74	76	is
165	77	85	not good
