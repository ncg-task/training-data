149	0	15	Average Context
150	23	25	of
151	4	13	first one
151	16	21	named
151	22	28	AC - S
151	31	39	averages
151	44	56	word vectors
151	57	63	before
151	68	74	target
151	96	101	after
151	106	112	target
151	83	95	word vectors
152	4	14	second one
152	17	22	named
152	23	25	AC
152	28	36	averages
152	61	73	full context
153	0	3	SVM
153	63	65	on
153	66	122	surface features , lexicon features and parsing features
154	0	8	Rec - NN
154	14	26	firstly uses
154	27	32	rules
154	33	45	to transform
154	50	65	dependency tree
154	70	73	put
154	78	92	opinion target
154	93	95	at
154	100	104	root
154	116	124	performs
154	125	145	semantic composition
154	146	150	with
154	151	164	Recursive NNs
154	165	168	for
154	169	189	sentiment prediction
155	0	8	TD- LSTM
155	14	18	uses
155	21	53	forward LSTM and a backward LSTM
155	54	65	to abstract
155	70	81	information
155	82	98	before and after
155	103	109	target
158	0	13	TD - LSTM - A
158	19	28	developed
158	29	38	TD - LSTM
158	50	54	have
158	55	68	one attention
158	69	71	on
158	76	83	outputs
160	0	6	MemNet
160	12	19	applies
160	20	29	attention
160	30	44	multiple times
160	45	47	on
160	52	67	word embeddings
160	78	102	last attention 's output
160	106	112	fed to
160	113	120	softmax
160	121	124	for
160	125	135	prediction
30	19	26	propose
30	29	44	novel framework
30	45	53	to solve
30	64	72	problems
30	73	75	in
30	76	101	target sentiment analysis
31	19	28	framework
31	29	41	first adopts
31	44	72	bidirectional LSTM ( BLSTM )
31	73	83	to produce
31	88	146	memory ( i.e. the states of time steps generated by LSTM )
31	147	151	from
31	156	161	input
32	4	17	memory slices
32	27	48	weighted according to
32	55	73	relative positions
32	74	76	to
32	81	87	target
35	0	13	Our framework
35	14	24	introduces
35	27	36	novel way
35	37	48	of applying
35	49	79	multiple - attention mechanism
35	80	93	to synthesize
35	94	112	important features
35	113	115	in
35	116	145	difficult sentence structures
33	16	19	pay
33	20	39	multiple attentions
33	40	42	on
33	47	73	position - weighted memory
33	78	97	nonlinearly combine
33	102	119	attention results
33	120	124	with
33	127	156	recurrent network , i.e. GRUs
34	13	18	apply
34	19	26	softmax
34	27	29	on
34	34	40	output
34	41	43	of
34	48	59	GRU network
34	60	70	to predict
34	75	84	sentiment
34	85	87	on
34	92	98	target
2	42	67	Aspect Sentiment Analysis
167	29	36	our RAM
167	37	61	consistently outperforms
167	66	82	compared methods
168	0	13	AC and AC - S
168	14	21	perform
168	22	28	poorly
169	0	8	Rec - NN
169	12	23	better than
169	24	33	TD - LSTM
169	38	52	not as good as
169	53	63	our method
172	0	9	TD - LSTM
172	10	18	performs
172	19	35	less competitive
172	36	40	than
172	41	51	our method
173	0	13	TD - LSTM - A
173	19	27	performs
173	28	33	worse
173	34	38	than
173	39	49	our method
175	0	6	MemNet
175	7	13	adopts
175	14	33	multiple attentions
175	34	53	in order to improve
175	58	75	attention results
