title
Multimodal Differential Network for Visual Question Generation
abstract
Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations .
Images can have multiple visual and language contexts thatare relevant for generating questions namely places , captions , and tags .
In this paper , we propose the use of exemplars for obtaining the relevant context .
We obtain this by using a Multimodal Differential Network to produce natural and engaging questions .
The generated questions show a remarkable similarity to the natural questions as validated by a human study .
Further , we observe that the proposed approach substantially improves over state - of - the - art benchmarks on the quantitative metrics ( BLEU , METEOR , ROUGE , and CIDEr ) .
Introduction
To understand the progress towards multimedia vision and language understanding , a visual Turing test was proposed by that was aimed at visual question answering .
Visual ) is a natural extension for VQA .
Current dialog systems as evaluated in show that when trained between bots , AI - AI dialog systems show improvement , but that does not translate to actual improvement for Human - AI dialog .
This is because , the questions generated by bots are not natural ( human - like ) and therefore does not translate to improved human dialog .
Therefore it is imperative that improvement in the quality of questions will enable dialog agents to perform well in human interactions .
Further , show that unanswered questions can be used for improving VQA , Image captioning and Object Classification .
An interesting line of work in this respect is the work of .
Here the au-thors have proposed the challenging task of generating natural questions for an image .
One aspect that is central to a question is the context that is relevant to generate it .
However , this context changes for every image .
As can be seen in , an image with a person on a skateboard would result in questions related to the event .
Whereas for a little girl , the questions could be related to age rather than the action .
How can one have widely varying context provided for generating questions ?
To solve this problem , we use the context obtained by considering exemplars , specifically we use the difference between relevant and irrelevant exemplars .
We consider different contexts in the form of Location , Caption , and Part of Speech tags .
Our method implicitly uses a differential context obtained through supporting and contrasting exemplars to obtain a differentiable embedding .
This embedding is used by a question decoder to decode the appropriate question .
As discussed further , we observe this implicit differential context to perform better than an explicit keyword based context .
The difference between the two approaches is illustrated in .
This also allows for better optimization as we can backpropagate through the whole network .
We provide detailed empirical evidence to support our hypothesis .
As seen in our method generates natural questions and improves over the state - of the - art techniques for this problem .
Figure
2 : Here we provide intuition for using implicit embeddings instead of explicit ones .
As explained in section 1 , the question obtained by the implicit embeddings are natural and holistic than the explicit ones .
To summarize , we propose a multimodal differential network to solve the task of visual question generation .
Our contributions are : ( 1 ) A method to incorporate exemplars to learn differential embeddings that captures the subtle differences between supporting and contrasting examples and aid in generating natural questions .
( 2 ) We provide Multimodal differential embeddings , as image or text alone does not capture the whole context and we show that these embeddings outperform the ablations which incorporate cues such as only image , or tags or place information .
( 3 ) We provide a thorough comparison of the proposed network against state - of - the - art benchmarks along with a user study and statistical significance test .
Related Work
Generating a natural and engaging question is an interesting and challenging task for a smart robot ( like chat - bot ) .
It is a step towards having a natural visual dialog instead of the widely prevalent visual question answering bots .
Further , having the ability to ask natural questions based on different contexts is also useful for artificial agents that can interact with visually impaired people .
While the task of generating question automatically is well studied in NLP community , it has been relatively less studied for image - related natural questions .
This is still a difficult task that has gained recent interest in the community .
Recently there have been many deep learning based approaches as well for solving the textbased question generation task such as .
Further , have proposed a method to generate a factoid based question based on triplet set { subject , relation and ob - ject } to capture the structural representation of text and the corresponding generated question .
These methods , however , were limited to textbased question generation .
There has been extensive work done in the Vision and Language domain for solving image captioning , paragraph generation , Visual Question Answering ( VQA ) and Visual Dialog .
proposed conventional machine learning methods for image have generated descriptive sentences from images with the help of Deep Networks .
There have been many works for solving Visual Dialog .
A variety of methods have been proposed by for solving VQA task including attention - based methods .
However , Visual Question Generation ( VQG ) is a separate task which is of interest in its own right and has not been so well explored .
This is a vision based novel task aimed at generating natural and engaging question for an image .
proposed a method for continuously generating questions from an image and subsequently answering those questions .
The works closely related to ours are that of and .
In the former work , the authors used an encoder - decoder based framework whereas in the latter work , the authors extend it by using a variational autoencoder based sequential routine to ob - tain natural questions by performing sampling of the latent variable .
Approach
Figure 3 : An illustrative example shows the validity of our obtained exemplars with the help of an object classification network , RESNET - 101 .
We see that the probability scores of target and supporting exemplar image are similar .
That is not the case with the contrasting exemplar .
The corresponding generated questions when considering the individual images are also shown .
In this section , we clarify the basis for our approach of using exemplars for question generation .
To use exemplars for our method , we need to ensure that our exemplars can provide context and that our method generates valid exemplars .
We first analyze whether the exemplars are valid or not .
We illustrate this in figure 3 .
We used a pre-trained object classification network on the target , supporting and contrasting images .
We observed that the supporting image and target image have quite similar probability scores .
The contrasting exemplar image , on the other hand , has completely different probability scores .
Exemplars aim to provide appropriate context .
To better understand the context , we experimented by analysing the questions generated through an exemplar .
We observed that indeed a supporting exemplar could identify relevant tags ( cows in ) for generating questions .
We improve use of exemplars by using a triplet network .
This network ensures that the joint image - caption embedding for the supporting exemplar are closer to that of the target image - caption and vice - versa .
We empirically evaluated whether an explicit approach that uses the differential set of tags as a one - hot encoding improves the question generation , or the implicit embedding obtained based on the triplet network .
We observed that the implicit multimodal differential network empirically provided better context for generating questions .
Our understanding of this phenomenon is that both target and supporting exemplars generate similar questions whereas contrasting exemplars generate very different questions from the target question .
The triplet network that enhances the joint embedding thus aids to improve the generation of target question .
These are observed to be better than the explicitly obtained context tags as can be seen in .
We now explain our method in detail .
Method
The task in visual question generation ( VQG ) is to generate a natural language questionQ , for an image I .
We consider a set of pre-generated context C from image I .
We maximize the conditional probability of generated question given image and context as follows :
where ?
is a vector for all possible parameters of our model .
Q is the ground truth question .
The log probability for the question is calculated by using joint probability over {q 0 , q 1 , ..... , q N } with the help of chain rule .
For a particular question , the above term is obtained as :
where N is length of the sequence , and qt is the t th word of the question .
We have removed ? for simplicity .
Our method is based on a sequence to sequence network .
The sequence to sequence network has a text sequence as input and output .
In our method , we take an image as input and generate a natural question as output .
The architecture for our model is shown in .
Our model contains three main modules , ( a ) Representation Module that extracts multimodal features ( b ) Mixture Module that fuses the multimodal representation and ( c ) Decoder that generates question using an LSTM - based language model .
During inference , we sample a question word q i from the softmax distribution and continue sampling until the end token or maximum length for the question is reached .
We experimented with both sampling and argmax and found out that argmax works better .
This result is provided in the supplementary material .
Multimodal Differential
Network
The proposed Multimodal Differential Network ( MDN ) consists of a representation module and a joint mixture module .
Finding Exemplars
We used an efficient KNN - based approach ( k -d tree ) with Euclidean metric to obtain the exemplars .
This is obtained through a coarse quantization of nearest neighbors of the training examples into 50 clusters , and selecting the nearest as supporting and farthest as the contrasting exemplars .
We experimented with ITML based metric learning for image features .
Surprisingly , the KNN - based approach outperforms the latter one .
We also tried random exemplars and different number of exemplars and found that k = 5 works best .
We provide these results in the supplementary material .
Representation Module
We use a triplet network in our representation module .
We refereed a similar kind of work done in ( Patro and Namboodiri , 2018 ) for building our triplet network .
The triplet network consists of three sub-parts : target , supporting , and contrasting networks .
All three networks share the same parameters .
Given an image xi we obtain an embedding g i using a CNN parameterized by a function G ( x i , W c ) where W care the weights for the CNN .
The caption Ci results in a caption embedding f i through an LSTM parameterized by a function F ( C i , W l ) where W l are the weights for the LSTM .
This is shown in part 1 of
Mixture Module
The Mixture module brings the image and caption embeddings to a joint feature embedding space .
The input to the module is the embeddings obtained from the representation module .
We have evaluated four different approaches for fusion viz. , joint , element - wise addition , hadamard and attention method .
Each of these variants receives image features g i & the caption embedding f i , and outputs a fixed dimensional feature vector s i .
The Joint method concatenates g i & f i and maps them to a fixed length feature vector s i as follows :
where g i is the 4096 - dimensional convolutional feature from the FC7 layer of pretrained is to obtain context vectors that bring the supporting exemplar embeddings closer to the target embedding and vice - versa .
This is obtained as follows :
where D ( t ( s i ) , t ( s j ) ) = ||t ( s i ) ? t ( s j ) || 2 2 is the euclidean distance between two embeddings t( s i ) and t(s j ) .
M is the training dataset that contains all set of possible triplets .
T ( s i , s + i , s ? i ) is the triplet loss function .
This is decomposed into two terms , one that brings the supporting sample closer and one that pushes the contrasting sample further .
This is given by
Here D + , D ? represent the euclidean distance between the target and supporting sample , and target and opposing sample respectively .
The parameter ?(=
0.2 ) controls the separation margin between these and is obtained through validation data .
Decoder : Question Generator
The role of decoder is to predict the probability for a question , given s i .
RNN provides a nice way to perform conditioning on previous state value using a fixed length hidden vector .
The conditional probability of a question token at particular time step qt is modeled using an LSTM as used in machine translation .
At time step t , the conditional probability is denoted by
where ht is the hidden state of the LSTM cell at time step t , which is conditioned on all the previously generated words {q 0 , q 1 , ...q N ?1 }.
The word with maximum probability in the probability distribution of the LSTM cell at step k is fed as an input to the LSTM cell at step k + 1 as shown in part 3 of Figure
4 .
At t = ? 1 , we are feeding the output of the mixture module to LSTM.Q = {q 0 , q 1 , ...q N ?1 } are the predicted question tokens for the input image I .
Here , we are usingq 0 andq N ? 1 as the special token START and STOP respectively .
The softmax probability for the predicted question token at different time steps is given by the following equations where LSTM refers to the standard LSTM cell equations :
Where ?
t+ 1 is the probability distribution over all question tokens .
loss is cross entropy loss .
Cost function
Our objective is to minimize the total loss , that is the sum of cross entropy loss and triplet loss over all training examples .
The total loss is :
where M is the total number of samples , ?
is a constant , which controls both the loss .
L triplet is the triplet loss function 5 .
L cross is the cross entropy loss between the predicted and ground truth questions and is given by :
where , N is the total number of question tokens , y t is the ground truth label .
The code for MDN - VQG model is provided 1 .
Variations of Proposed Method
While , we advocate the use of multimodal differential network for generating embeddings that can be used by the decoder for generating questions , we also evaluate several variants of this architecture .
These are as follows :
Tag Net :
In this variant , we consider extracting the part - of - speech ( POS ) tags for the words present in the caption and obtaining a Tag embedding by considering different methods of combining the one - hot vectors .
Further details and experimental results are present in the supplementary .
This Tag embedding is then combined with the image embedding and provided to the decoder network .
Place Net :
In this variant we explore obtaining embeddings based on the visual scene understanding .
This is obtained using a pre-trained that is trained to classify 365 different types of scene categories .
We then combine the activation map for the input image and the VGG - 19 based place embedding to obtain the joint embedding used by the decoder .
Differential Image Network : Instead of using multimodal differential network for generating embeddings , we also evaluate differential image network for the same .
In this case , the embedding does not include the caption but is based only on the image feature .
We also experimented with using multiple exemplars and random exemplars .
Further details , pseudocode and results regarding these are present in the supplementary material .
Dataset
We conduct our experiments on Visual Question Generation ( VQG ) dataset , which contains human annotated questions based on images of MS - COCO dataset .
This dataset was developed for generating natural and engaging questions based on commonsense reasoning .
We use VQG - COCO dataset for our experiments which contains a total of 2500 training images , 1250 validation images , and 1250 testing images .
Each image in the dataset contains five natural questions and five ground truth captions .
It is worth noting that the work of ( Jain et al. , 2017 ) also used the questions from VQA dataset for training purpose , whereas the work by ( Mostafazadeh et al. , 2016 ) uses only the VQG - COCO dataset .
VQA - 1.0 dataset is also built on images from MS - COCO dataset .
It contains a total of 82783 images for training , 40504 for validation and 81434 for testing .
Each image is associated with 3 questions .
We used pretrained caption generation model to extract captions for VQA dataset as the human annotated captions are not therein the dataset .
We also get good results on the VQA dataset ( as shown in ) which shows that our method does n't necessitate the presence of ground truth captions .
We train our model separately for VQG - COCO and VQA dataset .
Inference
We made use of the 1250 validation images to tune the hyperparameters and are providing the results on test set of VQG - COCO dataset .
During inference ,
We use the Representation module to find the embeddings for the image and ground truth caption without using the supporting and contrasting exemplars .
The mixture module provides the joint representation of the target image and ground truth caption .
Finally , the decoder takes in the joint features and generates the question .
We also experimented with the captions generated by an Image - Captioning network ( Karpathy and Fei-Fei , 2015 ) for VQG - COCO dataset and the result for that and training details are present in the supplementary material .
Experiments
We evaluate our proposed MDN method in the following ways :
First , we evaluate it against other variants described in section 4.4 and 4.1.3 .
Second , we further compare our network with stateof - the - art methods for VQA 1.0 and VQG - COCO dataset .
We perform a user study to gauge human opinion on naturalness of the generated question and analyze the word statistics in .
This is an important test as humans are the best .
Although these metrics have not been shown to correlate with ' naturalness ' of the question these still provide a reasonable quantitative measure for comparison .
Here we only provide the BLEU1 scores , but the remaining BLEU -n metric scores are present in the supplementary .
We observe that the proposed MDN provides improved embeddings to the decoder .
We believe that these embeddings capture instance specific differential information that helps in guiding the question generation .
Details regarding the metrics are given in the supplementary material .
Ablation Analysis
We considered different variations of our method mentioned in section 4.4 and the various ways to obtain the joint multimodal embedding as described in section 4.1.3 .
The results for the VQG - COCO test set are given in table
1 .
In this table , every block provides the results for one of the variations of obtaining the embeddings and different ways of combining them .
We observe that the
Baseline and State - of - the - Art
The comparison of our method with various baselines and state - of - the - art methods is provided in module mentioned in section 4.1.3 and also against the state - of - the - art methods .
The Critical Difference ( CD ) for Nemenyi test depends upon the given ?
( confidence level , which is 0.05 in our case ) for average ranks and N ( number of tested datasets ) .
If the difference in the rank of the two methods lies within CD , then they are not significantly different and vice - versa. visualizes the post - hoc analysis using the CD diagram .
From the figure , it is clear that MDN - Joint works best and is statistically significantly different from the state - of - the - art methods ..
The colored lines between the two models represents that these models are not significantly different from each other .
Here every question has different number of responses and hence the threshold which is the half of total responses for each question is varying .
This plot is only for 50 of the 100 questions involved in the survey .
See section 5.4 for more details .
Perceptual Realism
A human is the best judge of naturalness of any question , We evaluated our proposed MDN method using a ' Naturalness ' Turing test ( Zhang et al. , 2016 ) on 175 people .
People were shown an image with 2 questions just as in and were asked to rate the naturalness of both the questions on a scale of 1 to 5 where 1 means ' Least Natural ' and 5 is the ' Most Natural ' .
We provided 175 people with 100 such images from the VQG - COCO validation dataset which has 1250 images .
indicates the number of people who were fooled ( rated the generated question more or equal to the ground truth question ) .
For the 100 images , on an average 59.7 % people were fooled in this experiment and this shows that our model is able to generate natural questions .
Conclusion
In this paper we have proposed a novel method for generating natural questions for an image .
The approach relies on obtaining multimodal differential embeddings from image and its caption .
We also provide ablation analysis and a detailed comparison with state - of - the - art methods , perform a user study to evaluate the naturalness of our generated questions and also ensure that the results are statistically significant .
In future , we would like to analyse means of obtaining composite embeddings .
We also aim to consider the generalis ation of this approach to other vision and language tasks .
A Supplementary Material
Section
B will provide details about training configuration for MDN , Section C will explain the various Proposed Methods and we also provide a discussion in section D regarding some important questions related to our method .
We report BLEU1 , BLEU2 , BLEU3 , BLEU4 , METEOR , ROUGE and CIDER metric scores for VQG - COCO dataset .
We present different experiments with Tag Net in which we explore the performance of various tags ( Noun , Verb , and Question tags ) and different ways of combining them to get the context vectors .
Algorithm 1 Multimodal Differential Network 1 : procedure MDN ( x
i )
2 : Finding Exemplars :
3 :
33 :
A att = V att + f i 34:
S emb = tanh ( W AA att + b A ) 35 : Return S emb 36 : end procedure : VQG - COCO - dataset , Analysis of different number of Exemplars for addition model , hadamard model and joint model , R is random exemplar .
All these experiment are for the differential image network .
k= 5 performs the best and hence we use this value for the results in main paper .
testing images .
Each image in the dataset contains 5 natural questions .
B.2 Training Configuration
We have used RMSPROP optimizer to update the model parameter and configured hyperparameter values to be as follows : learning rate = 0.0004 , batch size = 200 , ? = 0.99 , = 1e ? 8 to train the classification network .
In order to train a triplet model , we have used RM - SPROP to optimize the triplet model model parameter and configure hyper - parameter values to be : learning rate = 0.001 , batch size = 200 , ? = 0.9 , = 1 e ?
8 . We also used learning rate decay to decrease the learning rate on every epoch by a factor given by :
where values of a =1500 and b=1250 are set empirically .
C Ablation Analysis of Model
While , we advocate the use of multimodal differential network ( MDN ) for generating embeddings that can be used by the decoder for generating questions , we also evaluate several variants of this architecture namely ( a ) Differential Image Network , ( b ) Tag net and ( c ) Place net .
These are described in detail as follows :
C.1 Differential Image Network
For obtaining the exemplar image based context embedding , we propose a triplet network consist of three network , one is target net , supporting net and opposing net .
All these three networks designed with convolution neural network and shared the same parameters .
The weights of this network are learnt through end - to - end learning using a triplet loss .
The aim is to obtain latent weight vectors that bring the supporting exemplar close to the target image and enhances the difference between opposing examples .
More formally , given an image xi we obtain an embedding g i using a CNN that we parameterize through a function G ( x i , W c ) where W care the weights of the CNN .
This is illustrated in figure 11 . : Full State - of - the - Art comparison on VQG - COCO Dataset .
The first block consists of the state - of - the - art results , second block refers to the baselines mentioned in State - of - the - art section of main paper and the third block provides the results for the best method for different ablations of our method .
C.2
Tag net
The tag net consists of two parts Context Extractor & Tag Embedding Net .
This is illustrated in .
Extract Context :
The first step is to extract the caption of the image using Neural Talk 2 model .
We find the part - ofs peech ( POS ) tag present in the caption .
POS taggers have been developed for two well known corpuses , the Brown Corpus and the Penn Treebanks .
For our work , we are using the Brown Corpus tags .
The tags are clustered into three category namely Noun tag , Verb tag and Question tags ( What , Where , . . . ) .
Noun tag consists of all the noun & pronouns present in the caption sentence and similarly , verb tag consists of verb & adverbs present in the caption sentence .
The question tags consists of the 7 - well know question words i.e. , why , how , what , when , where , who and which .
Each tag token is represented as a one - hot vector of the dimension of vocabulary size .
For generalization , we have considered 5 tokens from each category of the Tags .
Tag Embedding
Net :
The embedding network consists of word embedding followed by temporal convolutions neural network followed by maxpooling network .
In the first step , sparse high dimensional one - hot vector is transformed to dense low dimension vector using word embedding .
After this , we apply temporal convolution on the word embedding vector .
The uni-gram , bi-gram and tri-gram feature are computed by applying convolution filter of size 1 , 2 and 3 respectability .
Finally , we applied max - pooling on this to get a vector representation of the tags as shown figure 12 .
We concatenated all the tag words fol - lowed by fully connected layer to get feature dimension of 512 .
We also explored joint networks based on concatenation of all the tags , on elementwise addition and element - wise multiplication of the tag vectors .
However , we observed that convolution over max pooling and joint concatenation gives better performance based on CIDer score .
Where , T CNN is Temporally Convolution Neural Network applied on word embedding vector with kernel size three .
C.3
Place net
Visual object and scene recognition plays a crucial role in the image .
Here , places in the image are labeled with scene semantic categories , comprise of large and diverse type of environment in the world , such as ( amusement park , tower , swimming pool , shoe shop , cafeteria , rain - forest , conference center , fishpond , etc . ) .
So we have used different type of scene semantic categories present in the image as a place based context to generate natural question .
A place365 is a convolution neural network is modeled to classify 365 types of scene categories , which is trained on the place2 dataset consist of 1.8 million of scene images .
We have used a pre-trained VGG16 - places 365 network to obtain place based context embedding feature for various type scene categories present in the image .
The context feature F C is obtained by :
Where , p conv is Place 365 CNN .
We have extracted conv5 features of dimension 14x14x512 for attention model and FC8 features of dimension 365 for joint , addition and hadamard model of places 365 .
Finally , we use a linear transformation to obtain a 512 dimensional vector .
We explored using the CONV5 having feature dimension 14x14 512 , FC7 having 4096 and FC8 having feature dimension of 365 of places 365 .
D Ablation Analysis
D.1 Sampling Exemplar : KNN vs ITML
Our method is aimed at using efficient exemplarbased retrieval techniques .
We have experimented with various exemplar methods , such as ITML based metric learning for image features and KNN based approaches .
We observed KNN based approach ( K - D tree ) with Euclidean metric is a efficient method for finding exemplars .
Also we observed that ITML is computationally expensive and also depends on the training procedure .
The We see that both give more or less similar results but since ITML is computationally expensive and the dataset size is also small , it is not that efficient for our use .
All these experiment are for the differential image network for K = 2 only .
D.2 Question Generation approaches :
Sampling vs Argmax
We obtained the decoding using standard practice followed in the literature .
This method selects the argmax sentence .
Also , we evaluated our method by sampling from the probability distributions and provide the results for our proposed MDN - Joint method for VQG dataset as follows :
D.3
How are exemplars improving Embedding
In Multimodel differential network , we use exemplars and train them using a triplet loss .
It is known that using a triplet network , we can learn a representation that accentuates how the image is closer to a supporting exemplar as against the opposing exemplar ( Hoffer and .
The Joint embedding is obtained between the image and language representations .
Therefore the improved representation helps in obtaining an improved context vector .
Further we show that this also results in improving VQG .
D.4
Are exemplars required ?
We had similar concerns and validated this point by using random exemplars for the nearest neighbor for MDN .
( k=R in table
6 ) In this case the method is similar to the baseline .
This suggests that with random exemplar , the model learns to ignore the cue .
D.5
Are captions necessary for our method ?
This is not actually necessary .
In our method , we have used an existing image captioning method to generate captions for images that did not have them .
For VQG dataset , captions were available and we have used that , but , for VQA dataset captions were not available and we have generated captions while training .
We provide detailed evidence with respect to caption - question pairs to ensure that we are generating novel questions .
While the caption generates scene description , our proposed method generates semantically meaningful and novel questions .
Examples for Figure 1 of main paper :
First Image : - Caption -
A young man skateboarding around little cones .
Our Question - Is this a skateboard competition ?
Second Image : - Caption - A small child is standing on a pair of skis .
Our Question : - How old is that little girl ?
D.6 Intuition behind Triplet Network :
The intuition behind use of triplet networks is clear through this paper that first advocated its use .
The main idea is that when we learn distance functions thatare close for similar and far from dissimilar representations , it is not clear that close and far are with respect to what measure .
By incorporating a triplet we learn distance functions that learn that A is more similar to B as compared to C. Learning such measures allows us to bring target image - caption joint embeddings thatare closer to supporting exemplars as compared to contrasting exemplars .
E Analysis of Network E.1 Analysis of Tag Context
Tag is language based context .
These tags are extracted from caption , except question - tags which is fixed as the 7 ' Wh words '
( What , Why ,
Where , Who , When , Which and How ) .
We have experimented with Noun tag , Verb tag and ' Wh - word ' tag as shown in tables .
Also , we have experimented in each tag by varying the number of tags from 1 to 7 .
We combined different tags using 1 Dconvolution , concatenation , and addition of all the tags and observed that the concatenation mechanism gives better results .
As we can see in the table 4 that taking Nouns , Verbs and Wh - Words as context , we achieve significant improvement in the BLEU , METEOR and CIDEr scores from the basic models which only takes the image and the caption respectively .
Taking Nouns generated from the captions and questions of the corresponding training example as context , we achieve an increase of 1.6 % in Bleu Score and 2 % in METEOR and 34.4 % in CIDEr Score from the basic Image model .
Similarly taking Verbs as context gives us an increase of 1.3 % in Bleu Score and 2.1 % in METEOR and 33.5 % in CIDEr Score from the basic Image model .
And the best result comes when we take 3 Wh- Words as context and apply the Hadamard Model with concatenating the 3 WH - words .
Also in we have shown the results when we take more than one words as context .
Here we show that for 3 words i.e 3 nouns , 3 verbs and 3 Wh-words , the Concatenation model performs the best .
In this table the conv model is using 1 D convolution to combine the tags and the joint model combine all the tags .
E.2 Analysis of Context : Exemplars
In Multimodel Differential Network and Differential Image Network , we use exemplar images ( target , supporting and opposing image ) to obtain the differential context .
We have performed the experiment based on the single exemplar ( K= 1 ) , which is one supporting and one opposing image along with target image , based on two exemplar ( K= 2 ) , i.e. two supporting and two opposing image along with single target image .
similarly we have performed experiment for K=3 and K = 4 as shown in table - 6 .
E.3 Mixture Module : Other Variations
Hadamard method uses element - wise multiplication whereas Addition method uses element - wise addition in place of the concatenation operator of the Joint method .
The Hadamard method finds a correlation between image feature and caption feature vector while the Addition method learns a resultant vector .
In the attention method , the output Si is the weighted average of attention probability vector P att and convolutional features G img .
The attention probability vector weights the contribution of each convolutional feature based on the caption vector .
This attention method is similar to work stack attention method .
The attention mechanism is given by :
where G img is the 14x14x512 - dimensional convolution feature map from the fifth convolution layer of VGG - 19 Net of image X i and f i is the caption context vector .
The attention probability vector P att is a 196 - dimensional vector .
WI , WC , WP are the weights and b c , b A , b c is the bias for different layers .
We evaluate the different approaches and provide results for the same .
Here ?
represents element - wise addition .
E.4 Evaluation Metrics
Our task is similar to encoder - decoder framework of machine translation .
we have used same evaluation metric is used in machine translation .
is the first metric to find the correlation between generated question with ground truth question .
BLEU score is used to measure the precision value , i.e
That is how much words in the predicted question is appeared in reference question .
BLEU -n score measures the n-gram precision for counting cooccurrence on reference sentences .
we have evaluated BLEU score from n is 1 to 4 .
The mechanism of score is similar to BLEU - n , where as , it measures recall value instead of precision value in BLEU .
That is how much words in the reference question is appeared in predicted question .
Another version ROUGE metric is ROUGE - L , which measures longest common sub - sequence present in the generated question .
METEOR ( Banerjee and Lavie , 2005 ) score is another useful evaluation metric to calculate the similarity between generated question with reference one by considering synonyms , stemming and paraphrases .
the output of the METEOR score measure the word matches between predicted question and reference question .
In VQG , it compute the word match score between predicted question with five reference question .
CI Der score is a consensus based evaluation metric .
It measure human - likeness , that is the sentence is written by human or not .
The consensus is measured , how often n-grams in the predicted question are appeared in the reference question .
If the n-grams in the predicted question sentence is appeared more frequently in reference question then question is less informative and have low CIDer score .
We provide our results using all these metrics and compare it with existing baselines .
