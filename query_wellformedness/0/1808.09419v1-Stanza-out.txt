title
Identifying Well - formed Natural Language Questions
abstract
Understanding search queries is a hard problem as it involves dealing with " word salad " text ubiquitously issued by users .
However , if a query resembles a well - formed question , a natural language processing pipeline is able to perform more accurate interpretation , thus reducing downstream compounding errors .
Hence , identifying whether or not a query is well formed can enhance query understanding .
Here , we introduce a new task of identifying a well - formed natural language question .
We construct and release a dataset of 25,100 publicly available questions classified into well - formed and non-wellformed categories and report an accuracy of 70.7 % on the test set .
We also show that our classifier can be used to improve the performance of neural sequence - to - sequence models for generating questions for reading comprehension .
Introduction
User issued search queries often do not follow formal grammatical structure , and require specialized language processing .
Traditional natural language processing ( NLP ) tools trained on formal text ( e.g. treebanks ) often have difficulty analyzing search queries ; the lack of regularity in the structure of queries makes it difficult to train models that can optimally process the query to extract information that can help understand the user intent behind the query .
One clear direction to improve query processing is to annotate a large number of queries with the desired annotation scheme .
However , such an annotation can be prohibitively expensive and models trained on such queries might suffer from freshness issues , as the domain and nature of queries evolve frequently .
Another direction is to obtain a paraphrase of the given query that is a grammatical natural language question , and then analyze that paraphrase to extract the required information .
There are available tools and datasets , such as Quora question paraphrases and the Paralex dataset ) - for identifying query paraphrases , but these datasets do not contain information about whether a query is a natural language question or not .
Identifying well - formed natural language questions can also facilitate a more natural interaction between a user and a machine in personal assistants or chatbots or while recommending related queries in search - engines .
Identifying a well - formed question should be easy by parsing with a grammar , such as the English resource grammar , but such grammars are highly precise and fail to parse more than half of web queries .
Thus , in this paper we present a model to predict whether a given query is a well - formed natural language question .
We construct and publicly release a dataset of 25,100 queries annotated with the probability of being a well - formed natural language question ( 2.1 ) .
We then train a feed - forward neural network classifier that uses the lexical and syntactic features extracted from the query on this data ( 2.2 ) .
On a test set of 3,850 queries , we report an accuracy of 70.1 % on the binary classification task .
We also demonstrate that such a query well - formedness classifier can be used to improve the quality of a sequence - to - sequence question generation model by showing an improvement of 0.2 BLEU score in its performance ( 3 ) .
Our dataset ise available for download at http://goo.gl/language/ query-wellformedness .
2 Well - formed Natural Language Question Classifier
In this section we describe the data annotation , and the models used for question well - formedness classification .
Dataset Construction
We use the Paralex corpus that contains pairs of noisy paraphrase questions .
These questions were issued by users in WikiAnswers ( a Question - Answer forum ) and consist of both web - search query like constructs ( " 5 parts of chloroplast ? " ) and well - formed questions ( " What is the punishment for grand theft ? " ) , and thus is a good resource for constructing the question well - formedness dataset .
We select 25,100 queries from the unique list of queries extracted from the corpus such that no two queries in the selected set are paraphrases .
The queries are then annotated into well - formed or non-wellformed questions .
We define a query to be a well - formed natural language question if it satisfies the following :
1 .
Query is grammatical .
2 . Query is an explicit question .
3 . Query does not contain spelling errors .
shows some examples that were shown to the annotators to illustrate each of the above conditions .
Every query was labeled by five different crowdworkers with a binary label indicating whether a query is well - formed or not .
We average the ratings of the five annotators to get the probability of a query being well - formed .. 1 shows some queries with obtained human annotation .
Humans are pretty good at identifying an implicit query ( " Population of owls ... " ) or a simple well - formed question ( " What is released ... " ) , but may miss out on subtle spelling mistakes like " disscovered " or dis agree on whether the determiner " the " is needed before the word " genocide " ( " What countries have genocide happened in ? " ) .
Similar to other NLP tasks like entailment ( Dagan Query ( q ) p wf ( q ) population of owls just in north america ?
what is released when anion is formed ?
1.0 , paraphrasing etc. we rely on the wisdom of the crowd to get such annotations in order to make the data collection scalable and languageindependent .
is the histogram of query wellformedness probability across the dataset .
Interestingly , the number of queries where at least 4 or more annotators agree 1 on well - formedness is large : | {q | 0.8 ? p wf ( q ) ? 0.2 } | = 19206 queries .
These constitute 76.5 % of all queries in the dataset .
The Fleiss ' kappa for measuring agreement among multiple annotators is computed to be ? =
0.52 which shows moderate agreement .
We then randomly divided the dataset in approx . 70 % , 15 % , 15 % ratio into training , development and test sets containing 17500 , 3750 , and 3850 queries respectively .
While testing , we consider every query well - formed where at least 4 out of 5 annotators ( p wf ? 0.8 ) marked it as well - formed .
2
Model
Accuracy ( % ) majority class baseline 61.5 word bi - LSTM baseline 65. 8 question word baseline 54.9 word - 1 65.4 word - 1 , 2 65.5 word - 1 , 2 char - 3 , 4 66.9 word - 1 , 2 POS - 1 , 2 , 3 70.7 word - 1 , 2 char - 3 , 4 POS - 1 , 2 , 3 70.2 Approx. human upper bound 88.4 as a well - formed question gets 54.9 % .
4
Also , we used a single - layer word - level biLSTM encoder with hidden layer of length 50 to encode the question and then use this representation in the softmax layer to predict the label .
This classifier achieved 65.8 % .
Results .
The best performance obtained is 70.7 % while using word - 1 , 2 - grams and POS - 1 , 2 , 3 - grams as features .
Using POS n-grams gave a strong boost of 5.2 points over word unigrams and bigrams .
Although character - 3 , 4 grams gave improvement over word unigrams and bigrams , the performance did not sustain when combined with POS tags .
5
A random sample of 1000 queries from the test set were annotated by one of the authors of the paper with proficiency in English , which matched the gold label with 88.4 % accuracy providing an approximate upper-bound for model performance .
A major source of error is our model 's inability to understand deep semantics and syntax .
For example , " What is the history of dirk bikes ? " is labeled as a non-wellformed question with p wf = 0 by annotators because of the misspelled word " dirk " ( the correct word is " dirt " ) .
However , the POS tagger identifies " dirk " as a noun and as " NN NNS " is a frequent POS - bigram , our model tags it as a well - formed question with p wf = 0.8 , unable to identify that the word does not fit in the context of the question .
Another source of error is the inability to capture long term grammatical dependencies .
For example , in " What sort of work did Edvard Munch made ? " the verb " made " is incorrectly in the past tense instead of present tense .
Our model is unable to capture the relationship between " did " and " made " and thus marks this as a well - formed question .
Experiments
Baselines .
The majority class baseline is 61.5 % which corresponds to all queries being classified non-wellformed .
The question word baseline that classifies any query starting with a question word word n-grams char n-grams POS n -grams pwf ( q ) :
A feed - forward neural network for query well - formedness classification .
Improving Question Generation
Automatic question generation is the task of generating questions that ask about the information or facts present in either a given sentence or paragraph .
present a state - of - theart neural sequence - to - sequence model to generate questions from a given sentence / paragraph .
The model used is an attention - based encoder - decoder network , where the encoder reads in a given text and the decoder is an LSTM RNN that produces the question by predicting one word at a time .
use the SQuAD questionanswering dataset to develop a question generation dataset by pairing sentences from the text with the corresponding questions .
The question generation dataset contains approx 70 k , 10k , and 12 k training , development and test examples .
Their current best model selects the top ranked question from the n-best list produced by the decoder as the output .
We augment their system by training a discriminative reranker with the model score of the question generation model and the wellformedness probability of our classifier as features to optimize BLEU score between the selected question from the 10 - best list and the reference question on the development set .
We then use this reranker to select the best question from the 10 - best list of the test set .
We use the evaluation package released by to compute BLEU - 1 and BLEU - 4 scores .
shows that the reranked question selected using our query well - formedness clas - 6 BLEU - x uses precision computed over [ 1 , x ]- grams . The oracle improvement , by selecting the sentence from the list that maximizes the BLEU - 4 score is 15.2 .
However , its worth noting that an increase in well - formedness does n't guarantee an improved BLEU score , as the oracle sentence maximizing the BLEU score might be fairly non-wellformed .
For example , " who was elected the president of notre dame in ? " has a higher BLEU score to the reference " who was the president of notre dame in 1934 ? " than our wellformed question " who was elected the president of notre dame ? " .
shows a question generation example with the output of as the baseline result and the reranked question using the wellformed probability .
Related Work
We have referenced much of the related work throughout the paper .
We now review another orthogonally related field of work .
Grammatical error correction ( GEC ) is the task of correcting the grammatical errors ( if any ) in apiece of text .
As GEC includes not just identification of ungrammatical text but also correcting the text to produce grammatical text , it s a more complex task .
However , grammatical error prediction is the task of classifying whether or not a sentence is grammatical , which is more closely related to our task as classifying a question as well - formed requires making judgement on both the style and grammar of the text .
Conclusion
We proposed a new task of well - formed natural language question identification and established a strong baseline on a new dataset that can be downloaded at : http://goo.gl/language/ query-wellformedness .
We also showed that question well - formedness information can be a helpful signal in improving state - of - the - art question generation systems .
