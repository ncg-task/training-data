155	26	61	proposed EDD - LG ( shared ) method
155	62	67	works
155	68	78	way better
155	79	83	than
155	88	102	other variants
155	103	114	in terms of
155	115	138	BLEU and METEOR metrics
155	142	151	achieving
155	155	166	improvement
155	167	169	of
155	170	181	8 % and 6 %
155	182	184	in
155	189	195	scores
155	234	237	for
155	238	250	50 K dataset
155	273	285	10 % and 7 %
155	286	288	in
155	293	299	scores
155	313	316	for
155	317	330	100 K dataset
148	3	13	start with
148	14	28	baseline model
148	38	45	take as
148	48	82	simple encoder and decoder network
148	83	87	with
148	88	122	only the local loss ( ED - Local )
149	16	33	experimented with
149	34	79	encoder - decoder and a discriminator network
149	80	84	with
149	85	118	only global loss ( EDD - Global )
149	119	133	to distinguish
149	138	161	ground truth paraphrase
149	162	166	with
149	171	184	predicted one
150	8	30	variation of our model
150	34	38	used
150	39	82	both the global and local loss ( EDD - LG )
152	13	17	make
152	22	35	discriminator
152	36	41	share
152	42	49	weights
152	50	54	with
152	59	66	encoder
152	71	76	train
152	82	89	network
152	90	94	with
152	95	134	both the losses ( EDD - LG ( shared ) )
22	10	21	consists of
22	24	52	sequential encoder - decoder
22	58	60	is
22	61	76	further trained
22	77	82	using
22	85	107	pairwise discriminator
23	4	34	encoder - decoder architecture
23	44	59	widely used for
23	60	79	machine translation
23	84	105	machine comprehension
24	23	30	ensures
24	33	47	' local ' loss
24	56	68	incurred for
24	69	93	each recurrent unit cell
27	0	9	To ensure
27	19	33	whole sentence
27	34	36	is
27	37	54	correctly encoded
27	60	79	make further use of
27	82	107	pair - wise discriminator
27	108	120	that encodes
27	125	139	whole sentence
27	144	151	obtains
27	155	164	embedding
28	3	22	further ensure that
28	31	36	close
28	37	39	to
28	44	77	desired ground - truth embeddings
28	90	93	far
28	94	98	from
28	99	143	other ( sentences in the corpus ) embeddings
29	16	24	provides
29	27	42	' global ' loss
29	48	55	ensures
29	60	78	sentence embedding
29	93	101	close to
29	102	148	other semantically related sentence embeddings
2	0	37	Learning Semantic Sentence Embeddings
4	40	77	obtaining sentence - level embeddings
