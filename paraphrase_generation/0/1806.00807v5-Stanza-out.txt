title
Learning Semantic Sentence Embeddings using Pair- wise Discriminator
abstract
In this paper , we propose a method for obtaining sentence - level embeddings .
While the problem of securing word - level embeddings is very well studied , we propose a novel method for obtaining sentence - level embeddings .
This is obtained by a simple method in the context of solving the paraphrase generation task .
If we use a sequential encoder - decoder model for generating paraphrase , we would like the generated paraphrase to be semantically close to the original sentence .
One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far .
This is ensured by using a sequential pair - wise discriminator that shares weights with the encoder that is trained with a suitable loss function .
Our loss function penalizes paraphrase sentence embedding distances from being too large .
This loss is used in combination with a sequential encoder - decoder network .
We also validated our method by evaluating the obtained embeddings for a sentiment analysis task .
The proposed method results in semantic embeddings and outperforms the state - of - the - art on the paraphrase generation and sentiment analysis task on standard datasets .
These results are also shown to be statistically significant .
Introduction
The problem of obtaining a semantic embedding for a sentence that ensures that the related sentences are closer and unrelated sentences are farther lies at the core of understanding languages .
This would be relevant for a wide variety of machine reading comprehension and related tasks such as sentiment analysis .
Towards this problem , we propose a supervised method that uses a sequential encoder - decoder framework for paraphrase generation .
The task of generating paraphrases is closely related to the task of obtaining semantic sentence embeddings .
In our approach , we aim to ensure that the generated paraphrase embedding should be close to the true corresponding sentence and far from unrelated sentences .
The embeddings so obtained help us to obtain state - of - the - art results for paraphrase generation task .
Our model consists of a sequential encoder - decoder that is further trained using a pairwise discriminator .
The encoder - decoder architecture has been widely used for machine translation and machine comprehension tasks .
In general , the model ensures a ' local ' loss that is incurred for each recurrent unit cell .
It only ensures that a particular word token is present at an appropriate place .
This , however , does not imply that the whole sentence is correctly generated .
To ensure that the whole sentence is correctly encoded , we make further use of a pair - wise discriminator that encodes the whole sentence and obtains an embedding for it .
We further ensure that this is close to the desired ground - truth embeddings while being far from other ( sentences in the corpus ) embeddings .
This model thus provides a ' global ' loss that ensures the sentence embedding as a whole is close to other semantically related sentence embeddings .
This is illustrated in .
We further evaluate the validity of the sentence embeddings by using them for the task of sentiment analysis .
We observe that the proposed sentence embeddings result in state - of - the - art performance for both these tasks .
Our contributions are : a)
We propose a model for obtaining sentence embeddings for solving the paraphrase generation task using a pair - wise discriminator loss added to an encoder - decoder network .
b)
We show that these embeddings can also be used for the sentiment analysis task .
c)
We validate the model using standard datasets with a detailed comparison with state - of - the - art methods and also ensure that the results are statistically significant . :
Pairwise Discriminator based Encoder - Decoder for Paraphrase Generation :
This is the basic outline of our model which consists of an LSTM encoder , decoder and discriminator .
Here the encoders share the weights .
The discriminator generates discriminative embeddings for the Ground Truth - Generated paraphrase pair with the help of ' global ' loss .
Our model is jointly trained with the help of a ' local ' and ' global ' loss which we describe in section 3 .
Related Work
Given the flexibility and diversity of natural language , it has always been a challenging task to represent text efficiently .
There have been several hypotheses proposed for representing the same .
proposed a distribution hypothesis to represent words , i.e. , words which occur in the same context have similar meanings .
One popular hypothesis is the bag - of - words ( BOW ) or Vector Space Model , in which a text ( such as a sentence or a document ) is represented as the bag ( multiset ) of its words .
proposed an extended distributional hypothesis and proposed a latent relation hypothesis , in which a pair of words that co-occur in similar patterns tend to have similar semantic relation .
Word2
Vec is also a popular method for representing every unique word in the corpus in a vector space .
Here , the embedding of every word is predicted based on its context ( surrounding words ) .
NLP researchers have also proposed phrase - level and sentencelevel representations .
have analyzed several approaches to represent sentences and phrases by a weighted average of all the words in the sentence , combining the word vectors in an order given by a parse tree of a sentence and by using matrix - vector operations .
The major issue with BOW models and weighted averaging of word vectors is the loss of semantic meaning of the words , the parse tree approaches can only work for sentences because of its dependence on sentence parsing mechanism .
proposed a method to obtain a vector representation for paragraphs and use it to for some text - understanding problems like sentiment analysis and information retrieval .
Many language models have been proposed for obtaining better text embeddings in Machine Translation , question generation , dialogue generation , text generation and question answering .
For paraphrase generation task , have generated paraphrases using stacked residual LSTM based network .
proposed a encoder - decoder framework for this task . ) explored a VAE approach to generate paraphrase sentences using recurrent neural networks .
used reinforcement learning for paraphrase generation task .
Method
In this paper , we propose a text representation method for sentences based on an encoder - decoder framework using a pairwise discriminator for paraphrase generation and then fine tune these embeddings for sentiment analysis task .
Our model is an extension of seq2seq model for learning better text embeddings .
Overview
Task : In the paraphrase generation problem , given an input sequence of words X = [ x 1 , ... , x L ] , we need to generate another output sequence of words Y = [ q 1 , ... , q T ] that has the same meaning as X .
Here Land
T are not fixed constants .
Our training data consists of M pairs of paraphrases
where X i and Y i are the paraphrase of each other .
Our method consists of three modules as illustrated in : first is a Text Encoder which consists of LSTM layers , second is LSTM - based Text Decoder and last one is an LSTM - based Discriminator module .
These are shown respectively in part 1 , 2 , 3 of .
Our network with all three parts is trained end - to - end .
The weight parameters of encoder and discriminator modules are shared .
Instead of taking a separate discriminator , we shared it with the encoder so that it learns the embedding based on the ' global ' as well as ' local ' loss .
After training , at test time we used encoder to generate feature maps and pass it to the decoder for generating paraphrases .
These text embeddings can be further used for other NLP tasks such as sentiment analysis . :
This is an overview of our model .
It consists of 3 parts :
1 ) LSTM - based Encoder module which encodes a given sentence , 2 ) LSTM - based Decoder Module which generates natural language paraphrases from the encoded embeddings and 3 ) LSTM - based pairwise Discriminator module which shares its weights with the Encoder module and this whole network is trained with local and global loss .
Encoder - LSTM
We use an LSTM - based encoder to obtain a representation for the input question X i , which is represented as a matrix in which every row corresponds to the vector representation of each word .
We use a one - hot vector representation for every word and obtain a word embedding c i for each word using a Temporal CNN ) module that we parameterize through a function G(X i , W e ) where W e are the weights of the temporal CNN .
Now this word embedding is fed to an LSTM - based encoder which provides encoding features of the sentence .
We use LSTM due to its capability of capturing long term memory .
As the words are propagated through the network , the network collects more and more semantic information about the sentence .
When the network reaches the last word ( L th word ) , the hidden state h L of the network provides a semantic representation of the whole sentence conditioned on all the previously generated words ( q 0 , q 1 ... , qt ) .
Question sentence encoding feature f i is obtained after passing through an LSTM which is parameterized using the function F ( C i , W l ) where W l are the weights of the LSTM .
This is illustrated in part 1 of .
Decoder - LSTM
The role of decoder is to predict the probability for a whole sentence , given the embedding of input sentence ( f i ) .
RNN provides a nice way to condition on previous state value using a fixed length hidden vector .
The conditional probability of a sentence token at a particular time step is modeled using an LSTM as used in machine translation .
At time step t , the conditional probability is denoted by P
where ht is the hidden state of the LSTM cell at time step t. ht is conditioned on all the previously generated words ( q 0 , q 1 .. , q t?1 ) and qt is the next generated word .
Generated question sentence featurep d = {p 1 , . . . ,p
T } is obtained by decoder LSTM which is parameterized using the function D ( f i , W dl ) where W dl are the weights of the decoder LSTM .
The output of the word with maximum probability in decoder LSTM cell at step k is input to the LSTM cell at step k + 1 as shown in .
At t = ? 1 , we are feeding the embedding of input sentence obtained by the encoder module .? i = {q 0 , q 1 , ... , q T +1 } are the predicted question tokens for the input X i .
Here , we are usingq 0 andq T + 1 as the special START and STOP token respectively .
The predicted question token ( q i ) is obtained by applying Softmax on the probability distributionp i .
The question tokens at different time steps are given by the following equations where LSTM refers to the standard LSTM cell equations :
Whereq t + 1 is the predicted question token and q t + 1 is the ground truth one .
In order to capture local label information , we use the Cross Entropy loss which is given by the following equation :
Here T is the total number of sentence tokens , P ( q t | q 0 , ..q t?1 ) is the predicted probability of the sentence token , qt is the ground truth token .
Discriminative - LSTM
The aim of the Discriminative - LSTM is to make the predicted sentence embedding f pi and ground truth sentence embedding f g i indistinguishable as shown in .
Here we passp d to the shared encoder - LSTM to obtain f pi and also the ground truth sentence to the shared encoder - LSTM to obtain f g i .
The discriminator module estimates a loss function between the generated and ground truth paraphrases .
Typically , the discriminator is a binary classifier loss , but here we use a global loss , similar to which acts on the last hidden state of the recurrent neural network ( LSTM ) .
The main objective of this loss is to bring the generated paraphrase embeddings closer to its ground truth paraphrase embeddings and farther from the other ground truth paraphrase embeddings ( other sentences in the batch ) .
Here our discriminator network ensures that the generated embedding can reproduce better paraphrases .
We are using the idea of sharing discriminator parameters with encoder network , to enforce learning of embeddings that not only minimize the local loss ( cross entropy ) , but also the global loss .
Suppose the predicted embeddings of a batch is e p = [ f p 1 , f p 2 , ..f p N ]
T , where f pi is the sentence embedding of i th sentence of the batch .
Similarly ground truth batch embeddings are e
The objective of global loss is to maximize the similarity between predicted sentence f pi with the ground truth sentence f g i of i th sentence and minimize the similarity between i th predicted sentence , f pi , with j th ground truth sentence , f g j , in the batch .
The loss is defined as
Gradient of this loss function is given by
Cost function
Our objective is to minimize the total loss , that is the sum of local loss and global loss over all training examples .
The total loss is :
Where M is the total number of examples , L local is the cross entropy loss , L global is the global loss .
Dataset
This dataset consists of sentiment labels for different movie reviews and was first proposed by .
extended this by parsing the reviews to subphrases and then fine - graining the sentiment labels for all the phrases of movies reviews using Amazon Mechanical Turk .
The labels are classified into 5 sentiment classes , namely { Very Negative , Negative , Neutral , Positive , Very Positive } .
This dataset contains a total 126 k phrases for training set , 30 k phrases for validation set and 66 k phrases for test set .
Experiments
We perform experiments to better understand the behavior of our proposed embeddings .
To achieve this , we benchmark Encoder Decoder Discriminator Local - Global ( shared ) ( EDD - LG ( shared ) ) embeddings on two text understanding problems , Paraphrase Generation and Sentiment Analysis .
We use the Quora question pairs dataset 1 for paraphrase generation and Stanford Sentiment Treebank dataset ( Socher et al. , 1 website : https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 2013 ) for sentiment analysis .
In this section we describe the different datasets , experimental setup and results of our experiments .
Paraphrase Generation
Paraphrase generation is an important problem in many NLP applications such as question answering , information retrieval , information extraction , and summarization .
It involves generation of similar meaning sentences .
Experimental Protocols
For the task of Sentiment analysis , we are using a similar method of performing the experiments as used by .
We treat every subphrase in the dataset as a separate sentence and learn their corresponding representations .
We then feed these to a logistic regression to predict the movie ratings .
During inference time , we used a method simialr to in which we freeze the representation of every word and use this to construct a representation for the test sentences which are then fed to a logistic regression for predicting the ratings .
In order to train a sentiment classification model , we have used RMSPROP , to optimize the classification model parameter and we found these hyperparameter
This one is pretty miserable , resorting to string - pulling rather than legitimate character development and intelligent plotting .
S.No Original Question
Ground Truth Paraphrase Generated Paraphrase : Examples of Paraphrase generation on Quora Dataset .
We observe that our model is able to understand abbreviations as well and then ask questions on the basis of that as is the casein the second example .
Ablation Analysis
We experimented with different variations for our proposed method .
We start with baseline model which we take as a simple encoder and decoder network with only the local loss ( ED - Local ) .
Further we have experimented with encoder - decoder and a discriminator network with only global loss ( EDD - Global ) to distinguish the ground truth paraphrase with the predicted one .
Another variation of our model is used both the global and local loss ( EDD - LG ) .
The discriminator is the same as our proposed method , only the weight sharing is absent in this case .
Finally , we make the discriminator share weights with the encoder and train this network with both the losses ( EDD - LG ( shared ) ) .
The analyses are given in table
1 .
Among the ablations , the proposed EDD - LG ( shared ) method works way better than the other variants in terms of BLEU and METEOR metrics by achieving an improvement of 8 % and 6 % in the scores respectively over the baseline method for 50 K dataset and an improvement of 10 % and 7 % in the scores respectively for 100 K dataset .
Baseline and State - of - the - Art Method Analysis
There has been relatively less work on this dataset and the only work which we came across was that of .
We further compare our method EDD - LG ( shared ) model with their VAE - SVG - eq which is the current state - of - the - art on Quora datset .
Also we provide comparisons with other methods proposed by them in table
2 .
As we can see from the table that we achieve a significant improvement of 24 % in BLEU score and 11 % in TER score ( A lower TER score is better ) for 50 K dataset and similarly 22 % in BLEU score and 7.5 % in TER score for 100 K dataset .
Statistical Significance Analysis
We have analysed statistical significance for our proposed embeddings against different ablations and the state - of - the - art methods for the paraphrase generation task .
The Critical Difference ( CD ) for Nemenyi test depends upon the given ?
( confidence level , which is 0.05 in our case ) for average ranks and N ( number of tested datasets ) .
If the difference in the rank of the two methods lies within CD , then they are not significantly different , otherwise they are statistically different .
visualizes the post hoc analysis using the CD diagram .
From the figure , it is clear that our embeddings work best and the results are significantly different from the state - of - the - art methods .
Model
Error Rate ( Fine - Grained )
Naive Bayes 59.0 SVMs 59.3 Bigram Naive Bayes 58.1 Word Vector Averaging 67.3 Recursive Neural Network 56.8 Matrix Vector- RNN 55.6 Recursive Neural Tensor Network 54.3 Paragraph Vector 51.3 EDD - LG ( shared )
( Ours )
35.6 : Performance of our method compared to other approaches on the Stanford Sentiment Treebank Dataset .
The error rates of other methods are reported in Figure 3 : The mean rank of all the models on the basis of BLEU score are plotted on the x - axis .
Here EDD - LG - S refers to our EDD - LG shared model and others are the different variations of our model described in section 4.1.3 and the models on the right are the different variations proposed in .
Also the colored lines between the two models represents that these models are not significantly different from each other .
CD=5.199,p=0.0069
Sentiment Analysis with Stanford Sentiment Treebank ( SST ) Dataset
Tasks and Baselines
In , the authors propose two ways of benchmarking .
We consider the 5 - way finegrained classification task where the labels are { Very Negative , Negative , Neutral , Positive , Very Positive } .
The other axis of variation is in terms of whether we should label the entire sentence or all phrases in the sentence .
In this work we only consider labeling all the phrases .
apply several methods to this dataset and we show their performance in table
4 .
157130
The most hopelessly monotonous film of the year , noteworthy only for the gimmick of being filmed as a single unbroken 87 - minute take .
156368
No good jokes , no good scenes , barely a moment 157880
Although it bangs a very cliched drum at times 159269
They take along time to get to its gasp - inducing ending .
Negative 157144
Noteworthy only for the gimmick of being filmed as a single unbroken 87 - minute 156869
Done a great disservice by alack of critical distance and a sad trust in liberal arts college bumper sticker platitudes 221765
A hero can stumble sometimes .
222069
Spiritual rebirth to bruising defeat 218959
An examination of a society in transition Neutral 221444
A country still dealing with its fascist past 156757
Have to know about music to appreciate the film 's easygoing blend of comedy and romance 157663
A wildly funny prison caper .
157850
This is a movie that 's got oodles of style and substance .
157879
Although it bangs a very cliched drum at times , this crowd - pleaser 's fresh dialogue , energetic music , and good - natured spunk are often infectious .
Positive
156756
You do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance .
157382
Though of particular interest to students and enthusiast of international dance and world music , the film is designed to make viewers of all ages , cultural backgrounds and rhythmic ability want to getup and dance .
162398
A comic gem with some serious sparkles .
156238
Delivers a performance of striking skill and depth 157290
What Jackson has accomplished here is amazing on a technical level .
Very Positive 160925
A historical epic with the courage of its convictions about both scope and detail .
161048
This warm and gentle romantic comedy has enough interesting characters to fill several movies , and it s ample charms should win over the most hard - hearted cynics .
values to be working best for our case : learning rate = 0.00009 , batch size = 200 , ? = 0.9 , = 1e ? 8 .
Results
We report the error rates of different methods in table
4 .
We can clearly see that the performance of bag - of - words or bag - of - n- grams models ( the first four models in the table ) is not up to the mark and instead the advanced methods ( such as Recursive Neural Network ) perform better on sentiment analysis task .
Our method outperforms all these methods by an absolute margin of 15.7 % which is a significant increase considering the rate of progress on this task .
We have also uploaded our models to the online competition on Rotten Tomatoes dataset 2 and obtained an accuracy of 62.606 % on their test - set of 66 K phrases .
We provide 5 examples for each sentiment in table 5 .
We can see clearly that our proposed embeddings are able to get the complete meaning of smaller as well as larger sentences .
For example , our model classifies ' Although it bangs a very cliched drum at times ' as Negative and ' Although it bangs a very cliched drum at times , this crowd - pleaser 's fresh dialogue , energetic music , and good - natured spunk are often infectious . ' as positive showing that it is able to understand the finer details of language .
More results and visualis ations showing the part of the phrase to which the model attends while classifying are present in the appendix .
The link for the project website and code is provided here 3 .
Conclusion
In this paper we have proposed a sentence embedding using a sequential encoder - decoder with a pairwise discriminator .
We have experimented with this text embedding method for paraphrase generation and sentiment analysis .
We also provided experimental analysis which justifies that a pairwise discriminator outperforms the previous state - of - art methods for NLP tasks .
We also performed ablation analysis for our method , and our method outperforms all of them in terms of BLEU , METEOR and TER scores .
We plan to generalize this to other text understanding tasks and also extend the same idea in vision domain .
