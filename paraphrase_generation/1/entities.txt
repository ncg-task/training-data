122	0	13	Residual LSTM
122	14	16	is
122	26	56	current state - of - the - art
122	57	59	on
122	64	78	MSCOCO dataset
123	69	76	compare
123	98	116	standard VAE model
123	117	121	i.e.
123	128	148	unsupervised version
123	163	193	" supervised " variant VAE - S
123	194	196	of
123	201	219	unsupervised model
131	4	13	dimension
131	14	16	of
131	21	37	embedding vector
131	41	47	set to
131	48	51	300
131	71	95	both encoder and decoder
131	38	40	is
131	99	102	600
131	113	135	latent space dimension
131	96	98	is
131	139	143	1100
134	0	10	Batch size
134	14	21	kept at
134	22	24	32
139	0	15	Number of units
139	16	18	in
139	19	23	LSTM
139	28	37	set to be
139	42	56	maximum length
139	57	59	of
139	64	72	sequence
139	73	75	in
139	80	93	training data
132	4	20	number of layers
132	21	23	in
132	28	35	encoder
132	36	38	is
132	39	40	1
132	48	55	decoder
133	0	1	2
133	15	27	trained with
133	28	55	stochastic gradient descent
133	56	60	with
133	61	74	learning rate
133	75	83	fixed at
133	86	103	value of 5 10 ? 5
133	109	121	dropout rate
133	122	124	of
133	125	129	30 %
135	11	22	trained for
135	25	56	predefined number of iterations
135	59	70	rather than
135	73	95	fixed number of epochs
25	19	26	present
25	29	54	deep generative framework
25	55	83	for automatically generating
25	84	95	paraphrases
25	98	103	given
25	106	114	sentence
30	42	51	mechanism
30	52	64	to condition
30	65	78	our VAE model
30	79	81	on
30	86	103	original sentence
30	122	133	to generate
30	138	149	paraphrases
26	4	13	framework
26	14	22	combines
26	27	32	power
26	33	35	of
26	36	64	sequenceto - sequence models
26	67	79	specifically
26	84	117	long short - term memory ( LSTM )
26	124	146	deep generative models
26	149	161	specifically
26	166	197	variational autoencoder ( VAE )
26	200	210	to develop
26	213	262	novel , end - to - end deep learning architecture
26	263	266	for
26	271	275	task
26	276	278	of
26	279	300	paraphrase generation
32	111	121	our method
32	122	132	conditions
32	133	147	both the sides
32	150	154	i.e.
32	155	174	encoder and decoder
32	34	36	of
32	180	183	VAE
32	184	186	on
32	81	108	intermediate representation
32	177	179	of
32	226	240	input question
32	241	257	obtained through
32	258	262	LSTM
35	18	39	deep generative model
35	40	46	enjoys
35	49	78	simple , modular architecture
35	85	97	can generate
35	109	115	single
35	120	166	multiple , semantically sensible , paraphrases
38	27	42	proposed method
38	43	48	where
38	49	63	all variations
38	72	74	of
38	75	100	relatively better quality
38	112	115	are
38	120	144	top beam - search result
38	147	165	generated based on
38	166	177	different z
38	178	190	sampled from
38	193	205	latent space
2	32	53	Paraphrase Generation
5	42	78	generating paraphrases automatically
172	18	29	paraphrases
172	30	42	generated by
172	43	53	our system
172	54	57	are
172	58	71	well - formed
172	74	95	semantically sensible
172	102	123	grammatically correct
196	74	88	average metric
196	44	46	of
196	96	111	VAE - SVG model
196	115	127	able to give
196	130	173	10 % absolute point performance improvement
196	174	177	for
196	182	192	TER metric
187	112	115	for
187	116	130	MSCOCO dataset
189	19	23	have
189	26	49	significant improvement
189	50	56	w.r.t.
189	61	70	baselines
190	0	15	Both variations
190	16	18	of
190	23	39	supervised model
190	40	44	i.e.
190	47	56	VAE - SVG
190	61	75	VAE - SVG - eq
190	76	83	perform
190	84	90	better
190	91	95	than
190	100	122	state - of - the - art
190	123	127	with
190	128	137	VAE - SVG
190	138	148	performing
190	149	164	slightly better
190	165	169	than
190	170	184	VAE - SVG - eq
197	0	3	For
197	8	23	BLEU and METEOR
197	30	42	best results
197	43	46	are
197	47	87	4.7 % and 4 % absolute point improvement
197	88	92	over
197	97	119	state - of - the - art
198	31	44	Quora dataset
199	16	31	both variations
199	32	34	of
199	39	44	model
199	45	52	perform
199	53	73	significantly better
199	74	78	than
199	79	95	unsupervised VAE
199	100	107	VAE - S
200	81	89	increase
200	94	112	training data size
200	19	26	results
200	123	130	improve
201	0	9	Comparing
201	14	21	results
201	22	28	across
201	29	47	different variants
201	48	50	of
201	51	67	supervised model
201	70	84	VAE - SVG - eq
201	85	93	performs
201	98	102	best
203	8	25	experimented with
203	26	48	generating paraphrases
203	49	56	through
203	57	70	beam - search
203	98	112	turns out that
203	113	124	beam search
203	125	133	improves
203	138	145	results
203	146	159	significantly
205	5	14	comparing
205	19	44	best variant of our model
205	45	49	with
205	50	76	unsupervised model ( VAE )
205	86	97	able to get
205	98	155	more than 27 % absolute point ( more than 3 times ) boost
205	156	158	in
205	159	169	BLEU score
205	176	233	more than 19 % absolute point ( more than 2 times ) boost
205	234	236	in
205	237	243	METEOR
205	270	277	VAE - S
205	287	298	able to get
205	301	337	boost of almost 19 % absolute points
205	261	263	in
205	341	357	BLEU ( 2 times )
205	362	392	more than 10 % absolute points
205	393	395	in
205	396	416	METEOR ( 1.5 times )
