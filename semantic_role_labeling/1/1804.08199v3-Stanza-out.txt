title
Linguistically - Informed Self - Attention for Semantic Role Labeling
abstract
Current state - of - the - art semantic role labeling ( SRL ) uses a deep neural network with no explicit linguistic features .
However , prior work has shown that gold syntax trees can dramatically improve SRL decoding , suggesting the possibility of increased accuracy from explicit modeling of syntax .
In this work , we present linguistically - informed self - attention ( LISA ) : a neural network model that combines multi-head self - attention with multi-task learning across dependency parsing , part - ofspeech tagging , predicate detection and SRL .
Unlike previous models which require significant pre-processing to prepare linguistic features , LISA can incorporate syntax using merely raw tokens as input , encoding the sequence only once to simultaneously perform parsing , predicate detection and role labeling for all predicates .
Syntax is incorporated by training one attention head to attend to syntactic parents for each token .
Moreover , if a high - quality syntactic parse is already available , it can be beneficially injected at test time without re-training our SRL model .
In experiments on CoNLL - 2005 SRL , LISA achieves new state - of - the - art performance for a model using predicted predicates and standard word embeddings , attaining 2.5 F1 absolute higher than the previous state - of - the - art on newswire and more than 3.5 F1 on outof - domain data , nearly 10 % reduction in error .
On ConLL- 2012 English SRL we also show an improvement of more than 2.5 F1 .
LISA also out - performs the state - of - the - art with contextually - encoded ( ELMo ) word representations , by nearly 1.0 F1 on news and more than 2.0 F1 on out - of - domain text .
Introduction
Semantic role labeling ( SRL ) extracts a high - level representation of meaning from a sentence , labeling e.g. who did what to whom .
Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems , machine reading and translation .
Though syntax was long considered an obvious prerequisite for SRL systems , recently deep neural network architectures have surpassed syntacticallyinformed models , achieving state - of - the art SRL performance with no explicit modeling of syntax .
An additional benefit of these end - to - end models is that they require just raw tokens and ( usually ) detected predicates as input , whereas richer linguistic features typically require extraction by an auxiliary pipeline of models .
Still , recent work indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather than ignoring it .
indicate that many of the errors made by a syntaxfree neural network on SRL are tied to certain syntactic confusions such as prepositional phrase attachment , and show that while constrained inference using a relatively low - accuracy predicted parse can provide small improvements in SRL accuracy , providing a gold - quality parse leads to substantial gains . incorporate syntax from a high - quality parser ( Kiperwasser and Goldberg , 2016 ) using graph convolutional neural networks , but like they attain only small increases over a model with no syntactic parse , and even perform worse than a syntax - free model on out - of - domain data .
These works suggest that though syntax has the potential to improve neural network SRL models , we have not yet designed an architecture which maximizes the benefits of auxiliary syntactic information .
In response , we propose linguistically - informed self - attention ( LISA ) : a model that combines multi-task learning with stacked layers of multi-head self - attention ; the model is trained to : ( 1 ) jointly predict parts of speech and predicates ; ( 2 ) perform parsing ; and ( 3 ) attend to syntactic parse parents , while ( 4 ) assigning semantic role labels .
Whereas prior work typically requires separate models to provide linguistic analysis , including most syntaxfree neural models which still rely on external predicate detection , our model is truly end - to - end : earlier layers are trained to predict prerequisite parts - of - speech and predicates , the latter of which are supplied to later layers for scoring .
Though prior work re-encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL , we more efficiently encode each sentence only once , predict its predicates , part - of - speech tags and labeled syntactic parse , then predict the semantic roles for all predicates in the sentence in parallel .
The model is trained such that , as syntactic parsing models improve , providing high - quality parses at test time will improve its performance , allowing the model to leverage updated parsing models without requiring re-training .
In experiments on the CoNLL - 2005 and CoNLL - 2012 datasets we show that our linguistically - informed models out - perform the syntax - free state - of - the - art .
On CoNLL - 2005 with predicted predicates and standard word embeddings , our single model out - performs the previous state - of - the - art model on the WSJ test set by 2.5 F1 points absolute .
On the challenging out - of - domain Brown test set , our model improves substantially over the previous state - of - the - art by more than 3.5 F1 , a nearly 10 % reduction in error .
On CoNLL - 2012 , our model gains more than 2.5 F1 absolute over the previous state - of - the - art .
Our models also show improvements when using contextually - encoded word representations , obtaining nearly 1.0 F1 higher than the state - of - the - art on CoNLL - 2005 news and more than 2.0 F1 improvement on out - of - domain text .
1
1 Our implementation in Tensor Flow is available at : http://github.com/strubell/ LISA ) .
Layer r is input for a joint predicate / POS classifier .
Representations from layer r corresponding to predicted predicates are passed to a bilinear operation scoring distinct predicate and role representations to produce per-token SRL predictions with respect to each predicted predicate .
Model
Our goal is to design an efficient neural network model which makes use of linguistic information as effectively as possible in order to perform endto - end SRL .
LISA achieves this by combining :
( 1 ) A new technique of supervising neural attention to predict syntactic dependencies with ( 2 ) multi-task learning across four related tasks .
depicts the over all architecture of our model .
The basis for our model is the Transformer encoder introduced by : we transform word embeddings into contextually - encoded token representations using stacked multi-head self - attention and feedforward layers ( 2.1 ) .
To incorporate syntax , one self - attention head is trained to attend to each token 's syntactic parent , allowing the model to use this attention head as an oracle for syntactic dependencies .
We introduce this syntactically - informed self - attention ( ) in more detail in 2.2 .
Our model is designed for the more realistic setting in which gold predicates are not provided at test - time .
Our model predicts predicates and integrates part - of - speech ( POS ) information into earlier layers by re-purposing representations closer to the input to predict predicate and POS tags us - MatMul :
Figure
2 : Syntactically - informed self - attention for the query word sloth .
Attention weights
A parse heavily weight the token 's syntactic governor , saw , in a weighted average over the token values V parse .
The other attention heads act as usual , and the attended representations from all heads are concatenated and projected through a feed - forward layer to produce the syntacticallyinformed representation for sloth .
ing hard parameter sharing ( 2.3 ) .
We simplify optimization and benefit from shared statistical strength derived from highly correlated POS and predicates by treating tagging and predicate detection as a single task , performing multi-class classification into the joint Cartesian product space of POS and predicate labels .
Though typical models , which re-encode the sentence for each predicate , can simplify SRL to token - wise tagging , our joint model requires a different approach to classify roles with respect to each predicate .
Contextually encoded tokens are projected to distinct predicate and role embeddings ( 2.4 ) , and each predicted predicate is scored with the sequence 's role representations using a bilinear model ( Eqn. 6 ) , producing per-label scores for BIO - encoded semantic role labels for each token and each semantic frame .
The model is trained end - to - end by maximum likelihood using stochastic gradient descent ( 2.5 ) .
Self - attention token encoder
The basis for our model is a multi-head selfattention token encoder , recently shown to achieve state - of - the - art performance on SRL , and which provides a natural mechanism for incorporating syntax , as described in 2.2 .
Our implementation replicates .
The input to the network is a sequence X of T token representations x t .
In the standard setting these token representations are initialized to pretrained word embeddings , but we also experiment with supplying pre-trained ELMo representations combined with task - specific learned parameters , which have been shown to substantially improve performance of other SRL models .
For experiments with gold predicates , we concatenate a predicate indicator embedding pt following previous work .
We project 2 these input embeddings to a representation that is the same size as the output of the self - attention layers .
We then add a positional encoding vector computed as a deterministic sinusoidal function oft , since the self - attention has no innate notion of token position .
We feed this token representation as input to a series of J residual multi-head self - attention layers with feed - forward connections .
Denoting the jth self - attention layer as T ( j ) ( ) , the output of that layer s ( j ) t , and LN ( ) layer normalization , the following recurrence applied to initial input c
gives our final token representations s ( j )
t . Each T ( j ) ( ) consists of : ( a ) multi-head self - attention and ( b ) a feed - forward projection .
The multi-head self attention consists of H attention heads , each of which learns a distinct attention function to attend to all of the tokens in the sequence .
This self - attention is performed for each token for each head , and the results of the H self - attentions are concatenated to form the final self - attended representation for each token .
Specifically , consider the matrix S ( j?1 ) of T token representations at layer j ?
1 . For each attention head h , we project this matrix into distinct key , value and query representations K
h of dimensions T d k , T d q , and T d v , respectively .
We can then multiply Q between each pair of tokens in the sentence .
Following we perform scaled dot -product attention :
We scale the weights by the inverse square root of their embedding dimension and normalize with the softmax function to produce a distinct distribution for each token over all the tokens in the sentence :
( 2 )
These attention weights are then multiplied by
h for each token to obtain the self - attended token representations M
h , the self - attended representation for token tat layer j , is thus the weighted sum with respect tot ( with weights given by A
The outputs of all attention heads for each token are concatenated , and this representation is passed to the feed - forward layer , which consists of two linear projections each followed by leaky ReLU activations .
We add the output of the feed - forward to the initial representation and apply layer normalization to give the final output of self - attention layer j , as in Eqn .
1 .
Syntactically - informed self - attention
Typically , neural attention mechanisms are left on their own to learn to attend to relevant inputs .
Instead , we propose training the self - attention to attend to specific tokens corresponding to the syntactic structure of the sentence as a mechanism for passing linguistic knowledge to later layers .
Specifically , we replace one attention head with the deep bi-affine model of Dozat and Manning ( 2017 ) , trained to predict syntactic dependencies .
Let A parse be the parse attention weights , at layer i .
It s input is the matrix of token representations S ( i?1 ) .
As with the other attention heads , we project S ( i?1 ) into key , value and query representations , denoted K parse , Q parse , V parse .
Here the key and query projections correspond to parent and dependent representations of the tokens , and we allow their dimensions to differ from the rest of the attention heads to more closely follow the implementation of .
Unlike the other attention heads which use a dot product to score key - query pairs , we score the compatibility between K parse and Q parse using a bi-affine operator U heads to obtain attention weights :
These attention weights are used to compose a weighted average of the value representations V parse as in the other attention heads .
We apply auxiliary supervision at this attention head to encourage it to attend to each token 's parent in a syntactic dependency tree , and to encode information about the token 's dependency label .
Denoting the attention weight from token t to a candidate head q as A parse [ t , q ] , we model the probability of token t having parent q as :
using the attention weights A parse [ t ] as the distribution over possible heads for token t.
We define the root token as having a self - loop .
This attention head thus emits a directed graph 3 where each token 's parent is the token to which the attention A parse assigns the highest weight .
We also predict dependency labels using perclass bi-affine operations between parent and dependent representations Q parse and K parse to produce per-label scores , with locally normalized probabilities over dependency labels y dep t given by the softmax function .
We refer the reader to Dozat and Manning ( 2017 ) for more details .
This attention head now becomes an oracle for syntax , denoted P , providing a dependency parse to downstream layers .
This model not only predicts its own dependency arcs , but allows for the injection of auxiliary parse information at test time by simply setting A parse to the parse parents produced by e.g. a state - of - the - art parser .
In this way , our model can benefit from improved , external parsing models without re-training .
Unlike typical multi-task models , ours maintains the ability to leverage external syntactic information .
Multi - task learning
We also share the parameters of lower layers in our model to predict POS tags and predicates .
Following
He et al. , we focus on the end - toend setting , where predicates must be predicted on - the - fly .
Since we also train our model to predict syntactic dependencies , it is beneficial to give the model knowledge of POS information .
While much previous work employs a pipelined approach to both POS tagging for dependency parsing and predicate detection for SRL , we take a multi-task learning ( MTL ) approach , sharing the parameters of earlier layers in our SRL model with a joint POS and predicate detection objective .
Since POS is a strong predictor of predicates 4 and the complexity of training a multi - task model increases with the number of tasks , we combine POS tagging and predicate detection into a joint label space :
For each POS tag TAG which is observed co-occurring with a predicate , we add a label of the form TAG : PREDICATE .
Specifically , we feed the representation s ( r ) t from a layer r preceding the syntacticallyinformed layer p to a linear classifier to produce per-class scores rt for token t.
We compute locally - normalized probabilities using the softmax function :
is a label in the joint space .
Predicting semantic roles
Our final goal is to predict semantic roles for each predicate in the sequence .
We score each predicate against each token in the sequence using a bilinear operation , producing per-label scores for each token for each predicate , with predicates and syntax determined by oracles V and P.
First , we project each token representation s ( J ) t to a predicate - specific representation s pred t and a role - specific representation s role t .
We then provide these representations to a bilinear transformation U for scoring .
So , the role label scores sf t for the token at index t with respect to the predicate at index f ( i.e. token t and frame f ) are given by :
which can be computed in parallel across all semantic frames in an entire minibatch .
We calculate a locally normalized distribution over role labels for token tin frame f using the softmax function :
At test time , we perform constrained decoding using the Viterbi algorithm to emit valid sequences of BIO tags , using unary scores sf t and the transition probabilities given by the training data .
Training
We maximize the sum of the likelihoods of the individual tasks .
In order to maximize our model 's ability to leverage syntax , during training we clamp P to the gold parse ( P G ) and V to gold predicates VG when passing parse and predicate representations to later layers , whereas syntactic head prediction and joint predicate / POS prediction are conditioned only on the input sequence X .
The over all objective is thus :
where ? 1 and ?
2 are penalties on the syntactic attention loss .
We train the model using Nadam ( Dozat , 2016 ) SGD combined with the learning rate schedule in .
In addition to MTL , we regularize our model using dropout .
We use gradient clipping to avoid exploding gradients .
Additional details on optimization and hyperparameters are included in Appendix A.
Related work
Early approaches to SRL focused on developing rich sets of linguistic features as input to a linear model , often combined with complex constrained inference e.g. with an ILP .
showed that constraints could be enforced more efficiently using a clever dynamic program for exact inference .
modeled syntactic parsing and SRL jointly , and jointly modeled SRL and CCG parsing .
were among the first to use a neural network model for SRL , a CNN over word embeddings which failed to out - perform non-neural models .
FitzGerald et al.
successfully employed neural networks by embedding lexicalized features and providing them as factors in the model of .
More recent neural models are syntax - free. , and ( 2018 ) jointly predict SRL spans and predicates in a model based on that of , obtaining state - of - the - art predicted predicate SRL .
Concurrent to this work , and He et al . ( 2018 ) report significant gains on PropBank SRL by training a wide LSTM language model and using a task - specific transformation of its hidden representations ( ELMo ) as a deep , and computationally expensive , alternative to typical word embeddings .
We find that LISA obtains further accuracy increases when provided with ELMo word representations , especially on out - of - domain data .
Some work has incorporated syntax into neural models for SRL . incorporate syntax by embedding dependency paths , and similarly encode syntax using a graph CNN over a predicted syntax tree , out - performing models without syntax on CoNLL - 2009 .
These works are limited to incorporating partial dependency paths between tokens whereas our technique incorporates the entire parse .
Additionally , report that their model does not out - perform syntax - free models on out - of - domain data , a setting in which our technique excels .
MTL is popular in NLP , and others have proposed MTL models which incorporate subsets of the tasks we do , and we build off work that investigates where and when to combine different tasks to achieve the best results .
Our specific method of incorporating supervision into self - attention is most similar to the concurrent work of , who use edge marginals produced by the matrix - tree algorithm as attention weights for document classification and natural language inference .
The question of training on gold versus predicted labels is closely related to learning to search and scheduled sampling , with applications in NLP to sequence labeling and transition - based parsing ( Choi and .
Our approach maybe interpreted as an extension of teacher forcing to MTL .
We leave exploration of more advanced scheduled sampling techniques to future work .
Experimental results
We present results on the CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0 , achieving state - of - the - art results for a single model with predicted predicates on both corpora .
We experiment with both standard pre-trained Glo Ve word embeddings and pre-trained ELMo representations with fine - tuned task - specific parameters in order to best compare to prior work .
Hyperparameters that resulted in the best performance on the validation set were selected via a small grid search , and models were trained for a maximum of 4 days on one TitanX GPU using early stopping on the validation set .
We convert constituencies to dependencies using the Stanford head rules v 3.5 .
A detailed description of hyperparameter settings and data pre-processing can be found in Appendix A.
We compare our LISA models to four strong baselines :
For experiments using predicted predicates , we compare to To compare to more prior work , we present additional results on CoNLL - 2005 with models given gold predicates at test time .
In these experiments we also compare to , the previous state - of - the art SRL model using gold predicates and standard embeddings .
We demonstrate that our models benefit from injecting state - of - the - art predicted parses at test time ( + D&M ) by fixing the attention to parses predicted by Dozat and Manning ( 2017 ) , the winner of the 2017 CoNLL shared task which we re-train using ELMo embeddings .
In all cases , using these parses at test time improves performance .
We also evaluate our model using the gold syntactic parse at test time ( + Gold ) , to provide an upper bound for the benefit that syntax could have for SRL using LISA .
These experiments show that despite LISA 's strong performance , there remains substantial room for improvement .
In 4.3 we perform further analysis comparing SRL models using gold and predicted parses .
lists precision , recall and F1 on the CoNLL - 2005 development and test sets using predicted predicates .
For models using GloVe embeddings , our syntax - free SA model already achieves a new state - of - the - art by jointly predicting predicates , POS and SRL .
LISA with it s own parses performs comparably to SA , but when supplied with D&M parses LISA out - performs the previous state - of - the - art by 2.5 F1 points .
On the out - ofdomain Brown test set , LISA also performs comparably to its syntax - free counterpart with its own parses , but with D&M parses LISA performs exceptionally well , more than 3.5 F1 points higher than He et al ..
Incorporating ELMo em-beddings improves all scores .
The gap in SRL F1 between models using LISA and D&M parses is smaller due to LISA 's improved parsing accuracy ( see 4.2 ) , but LISA with D&M parses still achieves the highest F1 : nearly 1.0 absolute F1 higher than the previous state - of - the art on WSJ , and more than 2.0 F1 higher on Brown .
Semantic role labeling
In both settings LISA leverages domain - agnostic syntactic information rather than over-fitting to the newswire training data which leads to high performance even on out - of - domain text .
To compare to more prior work we also evaluate our models in the artificial setting where gold predicates are provided at test time .
For fair comparison we use GloVe embeddings , provide predicate indicator embeddings on the input and reencode the sequence relative to each gold predicate .
Here LISA still excels : with D&M parses , LISA out - performs the previous state - of - the - art by more than 2 F1 on both WSJ and Brown .
reports precision , recall and F1 on the CoNLL - 2012 test set .
We observe performance similar to that observed on ConLL - 2005 :
Using Glo Ve embeddings our SA baseline already out - performs
He et al. ( 2018 ) by nearly 1.5 F1 .
With its own parses , LISA slightly under-performs our syntax - free model , but when provided with stronger D&M parses LISA outperforms the state - of - the - art by more than 2.5 F1. Like CoNLL - 2005 , ELMo representations improve all models and close the F 1 gap between models supplied with LISA and D&M parses .
On this dataset ELMo also substantially narrows the difference between models with - and without syntactic information .
This suggests that for this challenging dataset , ELMo already encodes much of the information available in the D&M parses .
Yet , higher accuracy parses could still yield improvements since providing gold parses increases F1 by 4 points even with ELMo embeddings .
Parsing , POS and predicate detection
We first report the labeled and unlabeled attachment scores ( LAS , UAS ) of our parsing models on the CoNLL - 2005 and 2012 test sets embeddings comparable to the standalone D&M parser .
The difference in parse accuracy between LISA G and D&M likely explains the large increase in SRL performance we see from decoding with D&M parses in that setting .
In we present predicate detection precision , recall and F1 on the CoNLL - 2005 and 2012 test sets .
SA and LISA with and without ELMo attain comparable scores so we report only LISA + Glo Ve .
We compare to He et al. on CoNLL - 2005 , the only cited work reporting comparable predicate detection F1 .
LISA attains high predicate detection scores , above 97 F1 , on both in - domain datasets , and out - performs He et al. ( 2017 ) by 1.5 - 2 F1 points even on the out - ofdomain Brown test set , suggesting that multi-task learning works well for SRL predicate detection .
Analysis
First we assess SRL F1 on sentences divided by parse accuracy .
Here there is little difference between any of the models , with LISA models tending to perform slightly better than SA .
Both parsers make mistakes on the majority of sentences ( 57 % ) , difficult sentences where SA also performs the worst .
These examples are likely where gold and D&M parses improve the most over other models in over all F1 :
Though both parsers fail to correctly parse the entire sentence , the D&M parser is less wrong ( 87.5 vs. 85.7 average LAS ) , leading to higher SRL F1 by about 1.5 average F1 . Following
He et al. , we next apply a series of corrections to model predictions in order to understand which error types the gold parse resolves :
e.g.
Fix Labels fixes labels on spans matching gold boundaries , and Merge Spans merges adjacent predicted spans into a gold span .
In we see that much of the performance gap between the gold and predicted parses is due to span boundary errors ( Merge Spans , Split Spans and that these errors are due mainly to prepositional phrase ( PP ) attachment mistakes .
We also find this to be the case : shows a breakdown of split / merge corrections by phrase type .
Though the number of corrections decreases substantially across phrase types , the proportion of corrections attributed to PPs remains the same ( approx. 50 % ) even after providing the correct PP attachment to the model , indicating that PP span boundary mistakes are a fundamental difficulty for SRL .
Conclusion
We present linguistically - informed self - attention : a multi- task neural network model that effectively incorporates rich linguistic information for semantic role labeling .
LISA out - performs the state - of the - art on two benchmark SRL datasets , including out - of - domain .
Future work will explore improving LISA 's parsing accuracy , developing better training techniques and adapting to more tasks .
First , we compare the impact of Viterbi decoding with LISA , D&M , and gold syntax trees , finding the same trends across both datasets .
We find that Viterbi has nearly the same impact for LISA , D&M and gold parses :
Gold parses provide little improvement over predicted parses in terms of BIO label consistency .
We also assess SRL F1 as a function of sentence length and distance from span to predicate .
In we see that providing LISA with gold parses is particularly helpful for sentences longer than 10 tokens .
This likely directly follows from the tendency of syntactic parsers to perform worse on longer sentences .
With respect to distance between arguments and predicates , ) , we do not observe this same trend , with all distances performing better with better parses , and especially gold .
A.2 Supplemental results
Due to space constraints in the main paper we list additional experimental results here .
lists development scores on the CoNLL - 2005 dataset with predicted predicates , which follow the same trends as the test data .
A.3 Data and pre-processing details
We initialize word embeddings with 100d pretrained GloVe embeddings trained on 6 billion tokens of Wikipedia and Gigaword .
We evaluate the SRL performance of our models using the srl-eval .
