title
Deep Semantic Role Labeling with Self - Attention
abstract
Semantic Role Labeling ( SRL ) is believed to be a crucial step towards natural language understanding and has been widely studied .
Recent years , end - to - end SRL with recurrent neural networks ( RNN ) has gained increasing attention .
However , it remains a major challenge for RNNs to handle structural information and long range dependencies .
In this paper , we present a simple and effective architecture for SRL which aims to address these problems .
Our model is based on self - attention which can directly capture the relationships between two tokens regardless of their distance .
Our single model achieves F1 = 83.4 on the CoNLL - 2005 shared task dataset and F1 = 82.7 on the CoNLL - 2012 shared task dataset , which outperforms the previous state - of - the - art results by 1.8 and 1.0 F1 score respectively .
Besides , our model is computationally efficient , and the parsing speed is 50 K tokens per second on a single Titan X GPU .
Introduction
Semantic Role Labeling is a shallow semantic parsing task , whose goal is to determine essentially " who did what to whom " , " when " and " where " .
Semantic roles indicate the basic event properties and relations among relevant entities in the sentence and provide an intermediate level of semantic representation thus benefiting many NLP applications , such as Information Extraction , Question Answering , Machine Translation ) and Multi-document Abstractive Summarization .
Semantic roles are closely related to syntax .
Therefore , traditional SRL approaches rely heavily on the syntactic structure of a sentence , which brings intrinsic complexity and restrains these systems to be domain specific .
Recently , end - to - end models for SRL without syntactic inputs achieved promising results on this task .
As the pioneering work , introduced a stacked long short - term memory network ( LSTM ) and achieved the state - of - the - art results .
reported further improvements by using deep highway bidirectional LSTMs with constrained decoding .
These successes involving end - to - end models reveal the potential ability of LSTMs for handling the underlying syntactic structure of the sentences .
Despite recent successes , these RNN - based models have limitations .
RNNs treat each sentence as a sequence of words and recursively compose each word with its previous hidden state .
The recurrent connections make RNNs applicable for sequential prediction tasks with arbitrary length , however , there still remain several challenges in practice .
The first one is related to memory compression problem .
As the entire history is encoded into a single fixed - size vector , the model requires larger memory capacity to store information for longer sentences .
The unbalanced way of dealing with sequential information leads the network performing poorly on long sentences while wasting memory on shorter ones .
The second one is concerned with the inherent structure of sentences .
RNNs lack away to tackle the tree - structure of the inputs .
The sequential way to process the inputs remains the network depth - in - time , and the number of nonlinearities depends on the time - steps .
To address these problems above , we present a deep attentional neural network ( DEEPATT ) for the task of SRL 1 .
Our models rely on the self - attention mechanism which directly draws the global dependencies of the inputs .
In contrast to RNNs , a major advantage of self - attention is that it conducts direct connections between two arbitrary tokens in a sentence .
Therefore , distant elements can interact with each other by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the network .
Self - attention also provides a more flexible way to select , represent and synthesize the information of the inputs and is complementary to RNN based models .
Along with self - attention , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .
Although DEEPATT is fairly simple , it gives remarkable empirical results .
Our single model outperforms the previ-ous state - of - the - art systems on the CoNLL - 2005 shared task dataset and the CoNLL - 2012 shared task dataset by 1.8 and 1.0 F 1 score respectively .
It is also worth mentioning that on the out - of - domain dataset , we achieve an improvement upon the previous end - to - end approach ) by 2.0 F 1 score .
The feed - forward variant of DEEPATT allows significantly more parallelization , and the parsing speed is 50 K tokens per second on a single Titan X GPU .
Semantic Role Labeling
Given a sentence , the goal of SRL is to identify and classify the arguments of each target verb into semantic roles .
For example , for the sentence " Marry borrowed a book from John last week . " and the target verb borrowed , SRL yields the following outputs :
Here ARG0 represents the borrower , ARG1 represents the thing borrowed , ARG2 represents the entity borrowed from , AM - TMP is an adjunct indicating the timing of the action and V represents the verb .
Generally , semantic role labeling consists of two steps : identifying and classifying arguments .
The former step involves assigning either a semantic argument or nonargument for a given predicate , while the latter includes labeling a specific semantic role for the identified argument .
It is also common to prune obvious non-candidates before the first step and to apply post-processing procedure to fix inconsistent predictions after the second step .
Finally , a dynamic programming algorithm is often applied to find the global optimum solution for this typical sequence labeling problem at the inference stage .
In this paper , we treat SRL as a BIO tagging problem .
Our approach is extremely simple .
As illustrated in , the original utterances and the corresponding predicate masks are first projected into real - value vectors , namely embeddings , which are fed to the next layer .
After that , we design a deep attentional neural network which takes the embeddings as the inputs to capture the nested structures of the sentence and the latent dependency relationships among the labels .
On the inference stage , only the topmost outputs of attention sub - layer are taken to a logistic regression layer to make the final decision 2 .
Deep Attentional Neural Network for SRL
In this section , we will describe DEEPATT in detail .
The main component of our deep network consists of N identical layers .
Each layer contains a nonlinear sub - layer followed by an attentional sub - layer .
The topmost layer is the softmax classification layer .
Self - Attention
Self - attention or intra-attention , is a special case of attention mechanism that only requires a single sequence to compute its representation .
Self - attention has been successfully applied to many tasks , including reading comprehension , abstractive summarization , textual entailment , learning task - independent sentence representations , machine translation and language understanding .
In this paper , we adopt the multi-head attention formulation by .
depicts the computation graph of multi-head attention mechanism .
The center of the graph is the scaled dot-product attention , which is a variant of dot -product ( multiplicative ) attention .
Compared with the standard additive attention mechanism which is implemented using a one layer feed - forward neural network , the dot-product attention utilizes matrix production which allows faster computation .
Given a matrix of n query vectors Q ?
R nd , keys K ?
R nd and values V ?
R nd , the scaled dot -product attention computes the attention scores based on the following equation :
where d is the number of hidden units of our network .
The multi-head attention mechanism first maps the matrix of input vectors X ?
R td to queries , keys and values matrices by using different linear projections .
Then h parallel heads are employed to focus on different part of channels :
The computation graph of multi-head self - attention mechanism .
All heads can be computed in parallel using highly optimized matrix multiplication codes .
of the value vectors .
Formally , for the i - th head , we denote the learned linear maps by
, which correspond to queries , keys and values respectively .
Then the scaled dot-product attention is used to compute the relevance between queries and keys , and to output mixed representations .
The mathematical formulation is shown below :
Finally , all the vectors produced by parallel heads are concatenated together to form a single vector .
Again , a linear map is used to mix different channels from different heads :
where M ?
R nd and W ?
R dd .
The self - attention mechanism has many appealing aspects compared with RNNs or CNNs .
Firstly , the distance between any input and output positions is 1 , whereas in RNNs it can be n .
Unlike CNNs , self - attention is not limited to fixed window sizes .
Secondly , the attention mechanism uses weighted sum to produce output vectors .
As a result , the gradient propagations are much easier than RNNs or CNNs .
Finally , the dot -product attention is highly parallel .
In contrast , RNNs are hard to parallelize owing to its recursive computation .
Nonlinear Sub - Layers
The successes of neural networks root in its highly flexible nonlinear transformations .
Since attention mechanism uses weighted sum to generate output vectors , its representational power is limited .
To further increase the expressive power of our attentional network , we employ a nonlinear sub - layer to transform the inputs from the bottom layers .
In this paper , we explore three kinds of nonlinear sub - layers , namely recurrent , convolutional and feed - forward sub - layers .
Recurrent Sub - Layer
We use bidirectional LSTMs to build our recurrent sub - layer .
Given a sequence of input vectors {x t } , two LSTMs process the inputs in opposite directions .
To maintain the same dimension between inputs and outputs , we use the sum operation to combine two represen - tations :
Convolutional Sub - Layer
For convolutional sub - layer , we use the Gated Linear Unit ( GLU ) proposed by .
Compared with the standard convolutional neural network , GLU is much easier to learn and achieves impressive results on both language modeling and machine translation task .
Given two filters W ? R kdd and V ?
R kdd , the output activations of GLU are computed as follows :
The filter width k is set to 3 in all our experiments .
Feed-forward Sub - Layer
The feed - forward sub - layer is quite simple .
It consists of two linear layers with hidden ReLU nonlinearity in the middle .
Formally , we have the following equation :
where W 1 ?
R dh f and W 2 ?
Rh f d are trainable matrices .
Unless otherwise noted , we set hf = 800 in all our experiments .
Deep Topology
Previous works pointed out that deep topology is essential to achieve good performance .
In this work , we use the residual connections proposed by to ease the training of our deep attentional neural network .
Specifically , the output Y of each sub - layer is computed by the following equation :
We then apply layer normalization after the residual connection to stabilize the activations of deep neural network .
Position Encoding
The attention mechanism itself can not distinguish between different positions .
So it is crucial to encode positions of each input words .
There are various ways to encode positions , and the simplest one is to use an additional position embedding .
In this work , we try the timing signal approach proposed by , which is formulated as follows :
timing ( t , 2 i + 1 ) = cos(t/10000 2 i/d ) ( 12 ) The timing signals are simply added to the input embeddings .
Unlike the position embedding approach , this approach does not introduce additional parameters .
Pipeline
The first step of using neural networks to process symbolic data is to represent them by distributed vectors , also called embeddings .
We take the very original utterances and the corresponding predicate masks m as the input features .
mt is set to 1 if the corresponding word is a predicate , or 0 if not .
Formally , in SRL task , we have a word vocabulary V and mask vocabulary C = { 0 , 1 }.
Given a word sequence {x 1 , x 2 , . . . , x T } and a mask sequence {m 1 , m 2 , ... , m T } , each word x t ?
V and its corresponding predicate mask mt ?
C are projected into real - valued vectors e ( x t ) and e ( m t ) through the corresponding lookup table layer , respectively .
The two embeddings are then concatenated together as the output feature maps of the lookup table layers .
Formally speaking , we have x t = [ e ( w t ) , e ( m t ) ] .
We then build our deep attentional neural network to learn the sequential and structural information of a given sentence based on the feature maps from the lookup table layer .
Finally , we take the outputs of the topmost attention sub - layer as inputs to make the final predictions .
Since there are dependencies between semantic labels , most previous neural network models introduced a transition model for measuring the probability of jumping between the labels .
Different from these works , we perform SRL as a typical classification problem .
Latent dependency information is embedded in the topmost attention sub - layer learned by our deep models .
This approach is simpler and easier to implement compared to previous works .
Formally , given an input sequence x = {x 1 , x 2 , . . . , x n } , the log - likelihood of the corresponding correct label sequence y = {y 1 , y 2 , . . . , y n } is
Our model predict the corresponding label y t based on the representation ht produced by the topmost attention sublayer of DEEPATT :
Where W o is the softmax matrix and ?
yt is Kronecker delta with a dimension for each output symbol , so softmax ( W oh t ) T ?
yt is exactly they t 'th element of the distribution defined by the softmax .
Our training objective is to maximize the log probabilities of the correct output labels given the input sequence over the entire training set .
Experiments
We report our empirical studies of DEEPATT on the two commonly used datasets from the CoNLL - 2005 shared task and the CoNLL - 2012 shared task .
Datasets
The CoNLL -2005 dataset takes section 2 - 21 of the Wall Street Journal ( WSJ ) corpus as training set , and section 24 as development set .
The test set consists of section 23 of the WSJ corpus as well as 3 sections from the Brown corpus .
The CoNLL - 2012 dataset is extracted from the OntoNotes v5.0 corpus .
The description and separation of training , development and test set can be found in Pardhan et al ..
Model Setup
Initialization
We initialize the weights of all sub-layers as random orthogonal matrices .
For other parameters , we initialize them by sampling each element from a Gaussian distribution with mean 0 and variance 1 ? d .
The embedding layer can be initialized randomly or using pre-trained word embeddings .
We will discuss the impact of pre-training in the analysis subsection .
3
Settings and Regularization
The settings of our models are described as follows .
The dimension of word embeddings and predicate mask embeddings is set to 100 and the number of hidden layers is set to 10 .
We set the number of hidden units d to 200 .
The number of heads h is set to 8 .
We apply dropout to prevent the networks from over-fitting .
Dropout layers are added before residual connections with a keep probability of 0.8 .
Dropout is also applied before the attention softmax layer and the feed - froward ReLU hidden layer , and the keep probabilities are set to 0.9 .
We also employ label smoothing technique with a smoothing value of 0.1 during training .
Learning Parameter optimization is performed using stochastic gradient descent .
We adopt Adadelta ) ( = 10 6 and ? = 0.95 ) as the optimizer .
To avoid exploding gradients problem , we clip the norm of gradients with a predefined threshold 1.0 .
Each SGD contains a mini-batch of approximately 4096 tokens for the CoNLL - 2005 dataset and 8192 tokens for the CoNLL - 2012 dataset .
The learning rate is initialized to 1.0 .
After training 400 k steps , we halve the learning rate every 100 K steps .
We train all models for 600 K steps .
For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .
Results
In Remarkably , we get 74.1 F 1 score on the out - of - domain dataset , which outperforms the previous state - of - the - art system by 2.0 F 1 score .
On the CoNLL - 2012 dataset , the single model of FFN variant also outperforms the previous state - of - the - art by 1.0 F 1 score .
When ensembling 5 models with FFN nonlinear sub - layers , our approach achieves an F 1 score of 84.6 and 83.9 on the two datasets respectively , which has an absolute improvement of 1.4 and 0.5 over the previous state - of - the - art .
These results are consistent with our intuition that the self - attention layers is helpful to capture structural information and long distance dependencies .
Analysis
In this subsection , we discuss the main factors that influence our results .
We analyze the experimental results on the development set of CoNLL - 2005 dataset .
Model Depth
Previous works show that model depth is the key to the success of end - to - end SRL approach .
Our observations also coincide with previous works .
Rows 1 - 5 of show the effects of different number of layers .
For DEEPATT with 4 layers , our model only achieves 79.9 F 1 score .
Increasing depth consistently improves the performance on the development set , and our best model consists of 10 layers .
For DEEPATT with 12 layers , we observe a slightly performance drop of 0.1 F 1 .
Model Width
We also conduct experiments with different model widths .
We increase the number of hidden units from 200 to 400 and 400 to 600 as listed in rows 1 , 6 and 7 of , and the corresponding hidden size hf of FFN sublayers is increased to 1600 and 2400 respectively .
Increas - Decoding F 1 Speed Argmax Decoding 83.1 50 K Constrained Decoding 83.0
17K : Comparison between argmax decoding and constrained decoding on top of our model .
ing model widths improves the F 1 slightly , and the model with 600 hidden units achieves an F 1 of 83.4 .
However , the training and parsing speed are slower as a result of larger parameter counts .
Word Embedding Previous works found that the performance can be improved by pre-training the word embeddings on large unlabeled data .
We use the GloVe embeddings pre-trained on Wikipedia and Gigaword .
The embeddings are used to initialize our networks , but are not fixed during training .
Rows 1 and 8 of show the effects of additional pre-trained embeddings .
When using pre-trained GloVe embeddings , the F 1 score increases from 79.6 to 83.1 .
Position Encoding From rows 1 , 9 and 10 of we can see that the position encoding plays an important role in the success of DEEPATT .
Without position encoding , the DEEPATT with FFN sub - layers only achieves 20.0 F 1 score on the CoNLL - 2005 development set .
When using position embedding approach , the F 1 score boosts to 79.4 .
The timing approach is surprisingly effective , which outperforms the position embedding approach by 3.7 F 1 score .
Nonlinear Sub - Layers DEEPATT requires nonlinear sublayers to enhance its expressive power .
Row 11 of shows the performance of DEEPATT without nonlinear sublayers .
We can see that the performance of 10 layered DEEP - ATT without nonlinear sub - layers only matches the 4 layered DEEPATT with FFN sub - layers , which indicates that the nonlinear sub - layers are the essential components of our attentional networks .
show the effects of constrained decoding ) on top of DEEPATT with FFN sub - layers .
We observe a slightly performance drop when using constrained decoding .
Moreover , adding constrained decoding slowdown the decoding speed significantly .
For DEEPATT , it is powerful enough to capture the relationships among labels .
Constrained Decoding
Detailed Scores
We list the detailed performance on frequent labels in .
The results of the previous stateof - the - art ) are also shown for comparison .
Compared with , our model shows improvement on all labels except AM - PNC , where He 's model performs better .
shows the results of identifying and classifying semantic roles .
Our model improves the previous state - of - the - art on both identifying correct spans as well :
Comparison with the previous work on identifying and classifying semantic roles .
We list the percentage of correctly identified spans as well as the percentage of correctly classified semantic roles given the gold spans .
as correctly classifying them into semantic roles .
However , the majority of improvements come from classifying semantic roles .
This indicates that finding the right constituents remains a bottleneck of our model . :
Confusion matrix for labeling errors .
Each cell shows the percentage of predicted labels for each gold label .
shows a confusion matrix of our model for the most frequent labels .
We only consider predicted arguments that match gold span boundaries .
Compared with the previous work , our model still confuses ARG2 with AM - DIR , AM - LOC and AM - MNR , but to a lesser extent .
This indicates that our model has some advantages on such difficult adjunct distinction .
pred./ gold
Labeling Confusion
Related work developed the first automatic semantic role labeling system based on FrameNet .
Since then the task has received a tremendous amount of attention .
The focus of traditional approaches is devising appropriate feature templates to describe the latent structure of utterances . ; ; explored the syntactic features for capturing the over all sentence structure .
Combination of different syntactic parsers was also proposed to avoid prediction risk which was introduced by ; ; .
Beyond these traditional methods above , proposed a convolutional neural network for SRL to reduce the feature engineering .
The pioneering work on building an end - to - end system was proposed by , who applied an 8 layered LSTM model which outperformed the previous state - of - the - art system .
improved further with highway LSTMs and constrained decoding .
They used simplified input and output layers compared with .
also proposed a bidirectional LSTM based model .
Without using any syntactic information , their approach achieved the state - of - the - art result on the CoNLL - 2009 dataset .
Our method differs from them significantly .
We choose self - attention as the key component in our architecture instead of LSTMs .
Like , our system take the very original utterances and predicate masks as the inputs without context windows .
At the inference stage , we apply argmax decoding approach on top of a simple logistic regression while chose a CRF approach and chose constrained decoding .
This approach is much simpler and faster than the previous approaches .
Self - Attention Self - attention have been successfully used in several tasks .
used LSTMs and self - attention to facilitate the task of machine reading .
utilized self - attention to the task of natural language inference .
proposed self - attentive sentence embedding and applied them to author profiling , sentiment analysis and textual entailment .
combined reinforcement learning and self - attention to capture the long distance dependencies nature of abstractive summarization .
applied self - attention to neural machine translation and achieved the state - of - the - art results .
Very recently , applied self - attention to language understanding task and achieved the state - of - the - art on various datasets .
Our work follows this line to apply self - attention for learning long distance dependencies .
Our experiments also show the effectiveness of self - attention mechanism on the sequence labeling task .
Conclusion
We proposed a deep attentional neural network for the task of semantic role labeling .
We trained our SRL models with a depth of 10 and evaluated them on the CoNLL - 2005 shared task dataset and the CoNLL - 2012 shared task dataset .
Our experimental results indicate that our models substantially improve SRL performances , leading to the new state - of - theart .
