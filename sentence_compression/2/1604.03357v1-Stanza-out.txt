title
Improving sentence compression by learning to predict gaze
abstract
We show how eye - tracking corpora can be used to improve sentence compression models , presenting a novel multi-task learning algorithm based on multi - layer LSTMs .
We obtain performance competitive with or better than state - of - the - art approaches .
Readers fixate longer at rare words , words that are semantically ambiguous , and words that are mor -
Introduction
Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic summarization , as well as helping poor readers in need of assistive technologies .
This work suggests using eye - tracking recordings for improving sentence compression for text simplification systems and is motivated by two observations : ( i ) Sentence compression is the task of automatically making sentences easier to process by shortening them .
( ii ) Eye -tracking measures such as first - pass reading time and time spent on regressions , i.e. , during second and later passes over the text , are known to correlate with perceived text difficulty .
These two observations recently lead to suggest using eye - tracking measures as metrics in text simplification .
We go beyond this by suggesting that eye - tracking recordings can be used to induce better models for sentence compression for text simplification .
Specifically , we show how to use existing eye - tracking recordings to improve the induction of Long Short - Term Memory models ( LSTMs ) for sentence compression .
Our proposed model does not require that the gaze data and the compression data come from the same source .
Indeed , in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets .
While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users , based on their reading behavior .
Several approaches to sentence compression have been proposed , from noisy channel models over conditional random fields to tree - to - tree machine translation models ( Woodsend and Lapata , 2011 ) .
More recently , successfully used LSTMs for sentence compression on a large scale parallel dataset .
We do not review the literature here , and only compare to .
Our contributions
We present a novel multi-task learning approach to sentence compression using labelled data for sentence compression and a disjoint eye - tracking corpus .
Our method is fully competitive with state - of the - art across three corpora .
Our code is made publicly available at https://bitbucket.org/soegaard/ gaze - mtl 16 .
phologically complex .
These are also words that are likely to be replaced with simpler ones in sentence simplification , but it is not clear that they are words that would necessarily be removed in the context of sentence compression .
show that syntactic complexity ( measured as dependency locality ) is also an important predictor of reading time .
Phrases that are often removed in sentence compressionlike fronted phrases , parentheticals , floating quantifiers , etc. - are often associated with non-local dependencies .
Also , there is evidence that people are more likely to fixate on the first word in a constituent than on its second word .
Being able to identify constituent borders is important for sentence compression , and reading fixation data may help our model learn a representation of our data that makes it easy to identify constituent boundaries .
In the experiments below , we learn models to predict the first pass duration of word fixations and the total duration of regressions to a word .
These two measures constitute a perfect separation of the total reading time of each word split between the first pass and subsequent passes .
Both measures are described below .
They are both discretized into six bins as follows with only non-zero values contributing to the calculation of the standard deviation ( SD ) : 0 : measure = 0 or 1 : measure < 1 SD below reader 's average or 2 : measure < .5 SD below reader 's average or 3 : measure < .5 above reader 's average or 4 : measure > . 5 SD above reader 's average or 5 : measure >
1 SD above reader 's average First pass duration measures the total time spent reading a word first time it is fixated , including any immediately following re-fixations of the same word .
This measure correlates with word length , frequency and ambiguity because long words are likely to attract several fixations in a row unless they are particularly easily predicted or recognized .
This effect arises because long words are less likely to fit inside the fovea of the eye .
Note that for this measure the value 0 indicates that the word was not fixated by this reader .
3
5 Regression duration measures the total time spent fixating a word after the gaze has already left it once .
This measure belongs to the group of late measures , i.e. , measures that are sensitive to the later cognitive processing stages including interpretation and integration of already decoded words .
Since the reader by definition has already had a chance to recognize the word , regressions are associated with semantic confusion and contradiction , incongruence and syntactic complexity , as famously experienced in garden path sentences .
For this measure the value 0 indicates that the word was read at most once by this reader .
See for an example of first pass duration and regression duration annotations for one reader and sentence .
3 Sentence compression using multi - task deep bi - LSTMs
Words FIRST PASS REGRESSIONS
Most recent approaches to sentence compression make use of syntactic analysis , either by operating directly on trees or by incorporating syntactic information in their model .
presented an approach to sentence compression using LSTMs with word embeddings , but without syntactic features .
We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags , in addition to our gaze and compression models .
Bi-directional recurrent neural networks ( bi - RNNs ) read in sequences in both regular and reversed order , enabling conditioning predictions on both left and right context .
In the forward pass , we run the input data through an embedding layer and compute the predictions of the forward and backward states at layers 0 , 1 , . . . , until we compute the softmax predictions for word i based on a linear transformation of the concatenation of the of standard and reverse RNN outputs for location i .
We then calculate the objective function derivative for the sequence using cross - entropy ( logistic loss ) and use backpropagation to calculate gradients and update the weights accordingly .
A deep bi - RNN or klayered bi - RNN is composed of k bi-RNNs that feed into each other such that the output of the ith RNN is the input of the i + 1th RNN .
LSTMs ( Hochreiter and Schmidhuber , 1997 ) replace the cells of RNNs with LSTM cells , in which multiplicative gate units learn to open and close access to the error signal .
Bi - LSTMs have already been used for finegrained sentiment analysis , syntactic chunking , and semantic role labeling .
These and other recent applications of bi - LSTMs were constructed for solving a single task in isolation , however .
We instead train deep bi - LSTMs to solve additional tasks to sentence compression , namely CCG - tagging and gaze prediction , using the additional tasks to regularize our sentence compression model .
Specifically , we use bi - LSTMs with three layers .
Our baseline model is simply this three - layered model trained to predict compressions ( encoded as label sequences ) , and we consider two extensions thereof as illustrated in .
Our first extension , MULTI - TASK - LSTM , includes the gaze prediction task during training , with a separate logistic regression classifier for this purpose ; and the other , CASCADED - LSTM , predicts gaze measures from the inner layer .
Our second extension , which is superior to our first , is basically a one - layer bi - LSTM for predicting reading fixations with a two - layer bi - LSTM on top for predicting sentence compressions .
At each step in the training process of MULTI - TASK - LSTMand CASCADED - LSTM , we choose a random task , followed by a random training instance of this task .
We use the deep LSTM to predict a label sequence , suffer a loss with respect to the true labels , and update the model parameters .
In CASCADED - LSTM , the update for an instance of CCG super tagging or gaze prediction only affects the parameters of the inner LSTM layer .
Both MULTI - TASK - LSTM and CASCADED - LSTM do multi - task learning ) .
In multi - task learning , the induction of a model for one task is used as a regularizer on the induction of a model for another task .
did multitask learning by doing parameter sharing across several deep networks , letting them share hidden layers ; a technique also used by for various NLP tasks .
These models train task - specific classifiers on the output of deep networks ( informed by the task - specific losses ) .
We extend their models by moving to sequence prediction and allowing the task - specific sequence models to also be deep models .
Experiments
Gaze data
We use the Dundee Corpus as our eye -tracking corpus with tokenization and measures similar to the Dundee Treebank .
The corpus contains eye - tracking recordings often native English - speaking subjects reading 20 newspaper articles from The Independent .
We use data from nine subjects for training and one subject for development .
We do not evaluate the gaze prediction because the task is only included as away of regularizing the compression model .
S : Regulators Friday shutdown a small Florida bank , bringing to 119 the number of US bank failures this year amid mounting loan defaults .
T : Regulators shutdown a small Florida bank S : Intel would be building car batteries , expanding its business beyond its core strength , the company said in a statement .
T : Intel would be building car batteries
Compression data
We use three different sentence compression datasets , , , and the publically available subset of .
The first two consist of manually compressed newswire text in English , while the third is built heuristically from pairs of headlines and first sentences from newswire , resulting in the most aggressive compressions , as exemplified in .
We present the dataset characteristics in .
We use the datasets as released by the authors and do not apply any additional pre-processing .
The CCG supertagging data comes from CCGbank , 1 and we use sections 0 - 18 for training and section 19 for development .
Baselines and system
Both the baseline and our systems are three - layer bi - LSTM models trained for 30 iterations with pretrained ( SENNA ) embeddings .
The input and hidden layers are 50 dimensions , and at the output layer we predict sequences of two labels , indicating whether to delete the labeled word or not .
Our baseline ( BASELINE - LSTM ) is a multi - task learning 1 http://groups.inf.ed.ac.uk/ccg/
bi -LSTM predicting both CCG supertags and sentence compression ( word deletion ) at the outer layer .
Our first extension is MULTITASK - LSTM predicting CCG supertags , sentence compression , and reading measures from the outer layer .
CASCADED - LSTM , on the other hand , predicts CCG supertags and reading measures from the initial layer , and sentence compression at the outer layer .
Results and discussion
Our results are presented in .
We observe that across all three datasets , including all three annotations of BROADCAST , gaze features lead to improvements over our baseline 3 - layer bi - LSTM .
Also , CASCADED - LSTM is consistently better than MULTITASK - LSTM . : Results ( F1 ) .
For all three datasets , the inclusion of gaze measures ( first pass duration ( FP ) and regression duration ( Regr. ) ) leads to improvements over the baseline .
All models include CCG - supertagging as an auxiliary task .
Note that BROADCASTwas annotated by three annotators .
The three columns are , from left to right , results on annotators 1 - 3 .
the first sentence .
With the harder datasets , the impact of the gaze information becomes stronger , consistently favouring the cascaded architecture , and with improvements using both first pass duration and regression duration , the late measure associated with interpretation of content .
Our results indicate that multi-task learning can help us take advantage of inherently noisy human processing data across tasks and thereby maybe reduce the need for taskspecific data collection .
