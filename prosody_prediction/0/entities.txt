101	115	158	https://github.com/Helsinki - NLP / prosody
16	59	73	for predicting
16	74	93	prosodic prominence
16	94	98	from
16	99	103	text
16	113	121	based on
16	126	161	recently published Libri TTS corpus
16	164	174	containing
16	175	225	automatically generated prosodic prominence labels
16	226	229	for
16	230	244	over 260 hours
16	248	265	2.8 million words
16	266	268	of
16	269	288	English audio books
16	291	298	read by
16	299	322	1230 different speakers
17	22	29	will be
17	34	68	largest publicly available dataset
17	69	73	with
17	74	94	prosodic annotations
84	44	50	models
85	0	19	BERT - base uncased
85	20	84	3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM )
85	123	151	Minitagger ( SVM ) ) + GloVe
85	152	166	MarMoT ( CRF )
85	167	190	Majority class per word
88	3	6	use
88	11	45	Huggingface PyTorch implementation
88	46	48	of
88	49	53	BERT
88	54	66	available in
88	71	99	pytorch transformers library
88	113	120	further
88	121	132	fine - tune
88	133	139	during
88	140	148	training
90	31	56	smaller BERT - base model
90	57	62	using
90	67	86	uncased alternative
95	9	16	dropout
95	17	19	of
95	20	23	0.2
95	24	31	between
95	36	56	layers of the BiLSTM
91	9	19	batch size
91	20	22	of
91	23	25	32
91	30	41	fine - tune
91	46	51	model
91	52	55	for
91	56	64	2 epochs
89	3	7	take
89	12	29	last hidden layer
89	30	32	of
89	33	37	BERT
89	42	47	train
89	50	91	single fully - connected classifier layer
89	92	94	on
89	95	98	top
89	107	114	mapping
89	119	133	representation
89	99	101	of
89	137	146	each word
89	147	149	to
89	154	160	labels
92	0	3	For
92	4	10	BiLSTM
92	14	17	use
92	18	62	pre-trained 300D Glo Ve 840B word embeddings
97	8	11	SVM
97	15	18	use
97	19	46	Minitagger 4 implementation
97	50	55	using
97	56	70	each dimension
97	71	73	of
97	78	122	pre-trained 300D Glo Ve 840B word embeddings
97	123	125	as
97	126	134	features
97	137	141	with
97	142	158	context - size 1
98	8	46	conditional random field ( CRF ) model
98	50	53	use
98	54	60	MarMot
98	66	70	with
98	75	96	default configuration
94	18	21	add
94	22	57	one fullyconnected classifier layer
94	58	67	on top of
94	72	78	BiLSTM
94	81	88	mapping
94	93	107	representation
94	108	110	of
94	111	120	each word
94	121	123	to
94	128	134	labels
2	0	40	Predicting Prosodic Prominence from Text
4	87	135	predicting prosodic prominence from written text
20	0	18	Prosody prediction
106	0	10	All models
106	11	21	reach over
106	22	26	80 %
106	27	29	in
106	34	61	2 - way classification task
106	68	99	3 - way classification accuracy
106	100	111	stays below
106	112	116	70 %
122	16	32	reached close to
122	39	63	full predictive capacity
122	64	68	with
122	69	103	only 10 % of the training examples
107	4	19	BERTbased model
107	20	24	gets
107	29	45	highest accuracy
107	46	48	of
107	49	66	83.2 % and 68.6 %
107	67	69	in
107	74	114	2 - way and 3 - way classification tasks
111	4	17	3layer BiLSTM
111	18	26	achieves
111	27	33	82.1 %
111	34	36	in
111	41	63	2 - way classification
111	68	74	66.4 %
111	75	77	in
111	82	109	3 - way classification task
112	4	43	traditional feature - based classifiers
112	44	51	perform
112	52	66	slightly below
112	71	92	neural network models
112	95	99	with
112	104	107	CRF
112	108	117	obtaining
112	118	135	81.8 % and 66.4 %
112	136	139	for
112	144	168	two classification tasks
113	4	43	Minitagger SVM model 's test accuracies
113	44	47	are
113	48	62	slightly lower
113	63	67	than
113	72	78	CRF 's
113	79	83	with
113	84	117	80.8 % and 65.4 % test accuracies
114	8	14	taking
114	17	47	simple majority class per word
114	48	53	gives
114	54	60	80.2 %
114	61	64	for
114	69	96	2 - way classification task
114	101	107	62.4 %
114	108	111	for
114	116	143	3 - way classification task
121	0	3	For
121	4	22	most of the models
121	27	46	biggest improvement
121	47	49	in
121	50	61	performance
121	65	78	achieved when
121	79	85	moving
121	86	90	from
121	91	94	1 %
121	95	97	of
121	102	119	training examples
121	120	122	to
121	123	126	5 %
127	140	144	with
127	147	219	manually annotated test set from The Boston University radio news corpus
131	4	16	good results
131	40	47	provide
131	48	63	further support
131	64	67	for
131	72	98	quality of the new dataset
132	21	31	difference
132	32	39	between
132	40	55	BERT and BiLSTM
132	59	70	much bigger
