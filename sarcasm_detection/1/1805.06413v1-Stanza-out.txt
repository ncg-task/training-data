title
CASCADE : Contextual Sarcasm Detection in Online Discussion Forums
abstract
The literature in automated sarcasm detection has mainly focused on lexical , syntactic and semantic - level analysis of text .
However , a sarcastic sentence can be expressed with contextual presumptions , background and commonsense knowledge .
In this paper , we propose CASCADE ( a ContextuAl SarCasm DEtector ) that adopts a hybrid approach of both content and context - driven modeling for sarcasm detection in online social media discussions .
For the latter , CASCADE aims at extracting contextual information from the discourse of a discussion thread .
Also , since the sarcastic nature and form of expression can vary from person to person , CASCADE utilizes user embeddings that encode stylometric and personality features of the users .
When used along with content - based feature extractors such as Convolutional Neural Networks ( CNNs ) , we see a significant boost in the classification performance on a large Reddit corpus .
Introduction
Sarcasm is a linguistic tool that uses irony to express contempt .
Its figurative nature poses a great challenge for affective systems performing sentiment analysis .
Previous research in automated sarcasm detection has primarily focused on lexical , pragmatic cues found in sentences .
Interjections , punctuations , sentimental shifts , etc. , have been considered as major indicators of sarcasm .
When such lexical cues are present in sentences , sarcasm detection can achieve high accuracy .
However , sarcasm is also expressed implicitly , i.e. , without the use of any explicit lexical cues .
Such use of sarcasm also relies on the context which involves the presumption of commonsense and background knowledge of an event .
When it comes to detecting sarcasm in a discussion forum , it may not only require understanding the context of the previous comments but also need necessary external background knowledge about the topic of discussion .
The usage of slangs and informal language also diminishes the reliance on lexical cues .
This particular type of sarcasm is tough to detect .
Contextual dependencies for sarcasm can take many forms .
As an example , a sarcastic post from Reddit 1 , " I 'm sure Hillary would 've done that , lmao. " requires background knowledge about the event , i.e. , Hillary Clinton 's action at the time the post was made .
Similarly , sarcastic posts like " But atheism , yeah * that 's * a religion ! " requires the knowledge that topics like atheism often contain argumentative discussions and are more prone towards sarcasm .
In this work , we attempt the task of sarcasm detection in online discussion forums .
Particularly , we propose a hybrid network , named CASCADE , that utilizes both content and contextual - information required for sarcasm detection .
It starts by processing contextual information in two ways .
First , it performs user profiling to create user embeddings that capture indicative behavioral traits for sarcasm .
Recent findings suggest that such modeling of the user and their preferences , is highly effective for the given task .
It makes use of users ' historical posts to model their writing style ( stylometry ) and personality indicators , which are then fused into comprehensive user embeddings using a multi-view fusion approach , Canonical Correlation Analysis ( CCA ) .
Second , it extracts contextual information from the discourse of comments in the discussion forums .
This is done by document modeling of these consolidated comments belonging to the same forum .
We hypothesize that these discourse features would give the important contextual information , background cues along with topical information required for detecting sarcasm .
After the contextual modeling phase , CASCADE is provided with a comment for sarcasm detection .
It performs content - modeling using a Convolutional Neural Network ( CNN ) to extract its syntactic features .
This CNN representation is then concatenated with the relevant user embedding and discourse features to get the final representation which is used for classification .
The over all contribution of this work can be summarized as :
We propose a novel hybrid sarcasm detector , CASCADE that models content and contextual information .
We model stylometric and personality details of users along with discourse features of discussion forums to learn informative contextual representations .
Experiments on a large Reddit corpus , SARC , demonstrate significant performance improvement over state - of - the - art automated sarcasm detectors .
In the remaining paper , Section 2 compares our model to related works ; Section 3 provides the task description and proposed approach ; here , Section 3.3 explains the process of learning contextual features comprising user embeddings and discourse features ; Section 3.6 presents the hybrid prediction model followed by experimentation details and result analysis in Section 4 ; finally , Section 5 draws conclusion .
Related Work
Automated sarcasm detection is a relatively recent field of research .
The previous works in the literature can be largely classified into two categories , content and context - based sarcasm detection models .
Content - based :
These networks model the problem of sarcasm detection as a standard classification task and try to find lexical and pragmatic indicators to identify sarcasm .
Numerous works have taken this path and presented innovative ways to unearth interesting cues for sarcasm .
investigate sarcasm detection in spoken dialogue systems using prosodic and spectral cues .
use linguistic features like positive predicates , interjections and gestural clues such as emoticons , quotation marks , etc. , use syntactic patterns to construct classifiers .
also study the use of emoticons , mainly amongst tweets .
assert sarcasm to be a contrast to positive sentiment words and negative situations .
use multiple features comprising lexical , pragmatics , implicit and explicit context incongruity .
In the explicit case , they include relevant features to detect thwarted sentimental expectations in the sentence .
For implicit incongruity , they generalize 's work in identifying verb - noun phrases containing contrast in both polarities .
Context - based : Usage of contextual sarcasm has increased in the recent past , especially in online platforms .
Texts found in microblogs , discussion forums , social media , etc. , are plagued by grammatical inaccuracies and contain information which is highly temporal and contextual .
In such scenarios , mining linguistic information becomes relatively inefficient and need arises for additional clues .
demonstrate this need by showing how traditional classifiers fail in instances where humans require additional context .
They also indicate the importance of speaker and / or topical information associated to a text to gather such context .
use additional information by sentiment , emotional and personality representations of the input text .
Previous works have mainly used historical posts of users to understand sarcastic tendencies .
try to find users ' sentiments towards entities in their histories to find contrasting evidence .
utilize sentiments and noun phrases used within a forum to gather context typical to that forum .
Such forum based modeling simulates user-communities .
Our work follows similar motivation where we explore context provided by user profiling and the topical knowledge embedded in the discourse of comments in discussion - forums ( subreddits 2 ) .
perform user modeling by learning embeddings that capture homophily .
This work is closest to our approach given the fact that we too learn user embeddings to acquire context .
However , we take a different approach that involve stylometric and personality description of the users .
Empirical evidence shows that these proposed features are better than previous user modeling approaches .
Moreover , we learn discourse features which has not been explored before in the context of this task .
3 Method
Task Definition
The task involves detection of sarcasm for comments made in online discussion forums , i.e. , Reddit .
Let us denote the set U = {u 1 , ... , u Nu } for N u -users , where each user participates across a subset of N t - discussion forums ( subreddits ) .
For a comment C ij made by the i th user u i in the j th discussion forum t j , the objective is to predict whether the comment posted is sarcastic or not .
Summary of the Proposed Approach
Given the comment C ij to be classified , CASCADE leverages content and context - based information from the comment .
For content - based modeling of C ij , a CNN is used to generate the representation vector ?
c i , j for a comment .
CNNs generate abstract representations of text by extracting location - invariant local patterns .
This vector ?
c i , j captures both syntactic and semantic information useful for the task at hand .
For contextual modeling , CASCADE first learns user embeddings and discourse features of all users and discussion forums , respectively ( Section 3.3 ) .
Following this phase , CASCADE then retrieves the learnt user embedding ?
u i of user u i and discourse feature vector ?
t j of forum t j .
Finally , all three vectors ?
c i , j , ? u i , and ?
t j are concatenated and used for the classification ( Section 3.6 ) .
One might argue that instead of using one CNN , we could use multiple CNN ( explained in ) to get better text representations whenever a comment contains multiple sentences .
However that is out of the scope of this work .
Here , we aim to show the effectiveness of user specific analysis and context - based features extracted from the discourse .
Also the use of a single CNN for text representation helps to consistently compare with the state of the art .
Learning Contextual Features
We now detail the procedures to generate the contextual features , i.e. , user embeddings and discourse features .
The user embeddings try to capture users ' traits that correlate to their sarcastic tendencies .
These embeddings are created considering the accumulated historical posts of each user ( Section 3.4 ) .
Contextual information are also extracted from the discourse of comments within each discussion forum .
These extracted features are named as discourse features ( Section 3.5 ) .
The aim of learning these contextual features is to acquire discriminative information crucial for sarcasm detection .
User Embeddings
To generate user embeddings , we model their stylometric and personality features and then fuse them using CCA to create a single representation .
Below we explain the generation of user embedding ?
u i , for the i th user u i .
also summarizes the over all architecture for this user profiling .
Stylometric features
People possess their own idiolect and authorship styles , which is reflected in their writing .
These styles are generally affected by attributes such as gender , diction , syntactic influences , etc. ) and present behavioral patterns which aid sarcasm detection .
We use this motivation to learn stylometric features of the users by consolidating their online comments into documents .
We first gather all the comments by a user and create a document by appending them using a special delimiter < END >.
An unsupervised representation learning method ParagraphVector is then applied on this document .
This method generates a fixed - sized vector for each user by performing the auxiliary task of predicting the words within the documents .
The choice of ParagraphVector is governed by multiple reasons .
Apart from its ability to effectively encode a user 's writing style , it has the advantage of applying to variable lengths of text .
ParagraphVector also has been shown to perform well for sentiment classification tasks .
The existence of synergy between sentiment and sarcastic orientation of a sentence also promotes the use of this method .
We now describe the functioning of this method .
Every user- document and all words within them are first mapped to unique vectors such that each vector is represented by a column in matrix D ?
R ds Nu and W s ?
R ds V , respectively .
Here , d sis the embedding size and V represents the size of the vocabulary .
Continuous - bag - of - words approach is then performed where a target word is predicted given the word vectors from its context - window .
The key idea here is to use the document vector of the associated document as part of the context words .
More formally , given a user - document d i for user u i comprising a sequence of n i - words w 1 , w 2 , ... , w n i , we calculate the average log probability of predicting each word within a sliding context window of size k s .
This average log probability is :
To predict a word within a window , we take the average of all the neighboring context word vectors along with the document vector ?
d i and use a neural network with softmax prediction :
Here , ? y = [y 1 , ... , y V ] is the output of the neural network , i.e. ,
.. , ?
w t+ks taken from D and W s .
Hierarchical softmax is used for faster training .
Finally , after training , D learns the users ' document vectors which represent their stylometric features .
Personality features
Discovering personality from text has numerous NLP applications such as product recognition , mental health diagnosis , etc . .
Described as a combination of multiple characteristics , personality detection helps in identifying behavior , thought patterns of an individual .
To model the dependencies of users ' personality with their sarcastic nature , we include personality features in the user embeddings .
Previously , also utilize personality features in sentences .
However , we take a different and more - involved approach of extracting the personality features of a user instead .
For user u i , we iterate over all the vi - comments { S 1 u i , ... ,
S vi u i } written by them .
For each S j u i , we provide the comment as an input to a pre-trained Convolutional Neural Network ( CNN ) which has been trained on a multi-label personality detection task .
Specifically , the CNN is pre-trained on a benchmark corpus developed by which contains 2 , 400 essays and is labeled with the Big - Five personality traits , i.e. , Openness , Conscientiousness , Extraversion , Agreeableness , and Neuroticism ( OCEAN ) .
After the training , this CNN model is used to infer the personality traits present in each comment .
This is done by extracting the activations of the CNN 's last hidden layer vector which we call as the personality vector ?
p j u i .
The expectation over the personality vectors for all vi - comments made by the user is then defined as the over all personality feature vector ?
pi of user u i :
CNN : Here , we describe the CNN that generates the personality vectors .
Given a user 's comment , which is a text S = [ w 1 , ... , w n ] composed of n words , each word w i is represented as a word embedding ?
w i ?
R dem using the pre-trained FastText embeddings .
A single - layered CNN is then modeled on this input sequence S Post v
Post 1
Post v : The figure describes the process of user profiling .
Stylometric and Personality embeddings are generated and then fused in a multi-view setting using CCA to get the user embeddings .
Sand extracts h k - gram features at each instance .
This creates a feature map vector ?
m k of size R S ?h k +1 , whose each entry m k , j is obtained as :
here , bk ?
R is the bias and ?(? ) is a non-linear activation function .
M feature maps are created from each filter F k giving a total of 3 M feature maps as output .
Following this , a max - pooling operation is performed across the length of each feature map .
Thus , for all M feature maps computed from
o is projected onto a dense layer with d p neurons followed by the final sigmoid - prediction layer with 5 classes denoting the five personality traits .
We use sigmoid instead of softmax to facilitate multi-label classification .
This is calculated as ,
W 1 ? R dp3 M , W 2 ? R 5 dp , ? b 1 ?
R dp and ?
b 2 ?
R 5 are parameters and ?( . ) represents non-linear activation .
Fusion
We take a multi-view learning approach to combine both stylometric and personality features into a comprehensive embedding for each user .
We use Canonical Correlation Analysis ( CCA ) to perform this fusion .
CCA captures maximal information between two views and creates a combined representation .
In the event of having more than two views , fusion can be performed using an extension of CCA called Generalized CCA ( see .
Canonical Correlation
Analysis :
Let us consider the learnt stylometric embedding matrix D ?
R ds Nu and personality embedding matrix P ?
R dp
Nu containing the respective embedding vectors of user u i in their i th columns .
The matrices are then mean - centered and standardized across all user columns .
We call these new matrices as X 1 and X 2 , respectively .
Let the correlation matrix for X 1 be R 11 = X 1 X 1 T ? R dsds , for X 2 be R 22 = X 2 X 2 T ?
R dpdp and the cross - correlation matrix between them be R 12 = X 1 X 2 T ?
R dsdp .
For each user u i , the objective of CCA is to find the linear projections of both embedding vectors that have a maximum correlation .
We create K such projections , i.e. , K- canonical variate pairs such that each pair of projection is orthogonal with respect to the previous pairs .
This is done by constructing :
where , A 1 ?
R ds K , A 2 ?
R dp K and W T W = Z T Z = I .
To maximize correlation between W and Z , optimal A 1 and A 2 are calculated by performing singular value decomposition as : It can be seen that ,
Once optimal A 1 and A 2 are calculated , over all user embedding ?
u i ?
R K of user u i is generated by fusion of ? d i and ?
pi as :
Discourse Features
Similar to how a user influences the degree of sarcasm in a comment , we assume that the discourse of comments belonging to a certain discussion forum contain contextual information relevant to the sarcasm classification .
They embed topical information that selectively incur bias towards degree of sarcasm in the comments of a discussion .
For example , comments on political leaders or sports matches are generally more susceptible to sarcasm than natural dis asters .
Contextual information extracted from the discourse of a discussion can also provide background knowledge or cues about the topic of that discussion .
To extract the discourse features , we take a similar approach of document modeling performed for stylometric features ( Section 3.4.1 ) .
For all N t - discussion forums , we compose each forum 's document by appending the comments within them .
As before , ParagraphVector is employed to generate discourse representations for each document .
We denote the learnt feature vector of j th forum t j as ? t j ?
R dt .
Final Prediction
Following the extraction of text representation ?
c i , j for comment C i , j and retrieval of user embedding ?
u i for author u i and discourse feature vector ?
t j for discussion forum t j , we concatenate all three vectors to form the unified text
Here , ? refers to concatenation .
The CNN used for extraction of ?
c i , j has the same design as the CNN we used to extract personality features described in Section 3.4.2 .
Finally , ?
i , j is projected to the output layer having two neurons with a softmax activation .
This gives a softmax - probability over whether a comment is sarcastic or not .
This probability estimate is then used to calculate the categorical cross - entropy which is used as the loss function :
Here , N is the number of comments in the training set , y i is the one - hot vector ground truth of the i th comment and ?
i , j is it s predicted probability of belonging to class j.
Experimental Results
Dataset
We perform our experiments on a large - scale self - annotated corpus for sarcasm , SARC 3 .
This dataset contains more than a million examples of sarcastic / non - sarcastic statements made in the social media site Reddit .
Reddit comprises of topic - specific discussion forums , also known as subreddits , each titled by a post .
In each forum , users communicate either by commenting to the titled post or other 's comments , resulting in a tree - like conversation structure .
This structure can be unraveled to a linear format , thus creating a discourse of the comments by keeping the topological constraints intact .
Each comment is accompanied with its author details and parent comments ( if any ) which is subsequently used for our contextual processing .
It is important to note that almost all comments in the SARC dataset are composed of a single sentence .
We consider three variants of the SARC dataset in our experiments .
Main balanced :
This is the primary dataset which contains a balanced distribution of both sarcastic and non-sarcastic comments .
The dataset contains comments from 1246058 users ( 118940 in training and 56118 in testing set ) distributed across 6534 forums ( 3868 in training and 2666 in testing set ) .
Main imbalanced :
To emulate real - world scenarios where the sarcastic comments are typically lesser than non-sarcastic ones , we use an imbalanced version of the Main dataset .
Specifically , we maintain a 20 ? 80 ratio ( approx . ) between the sarcastic and non-sarcastic comments in both training / testing sets .
Pol :
To further test the effectiveness of our user embeddings , we perform experiments on a subset of Main , comprising of forums associated with the topic of politics .
The choice of using SARC for our experiments comes with multiple reasons .
First , this corpus is the first of its kind that was purposely developed to investigate the necessity of contextual information in sarcasm classification .
This characteristic aligns well with the main goal of this paper .
Second , the large size of the corpus allows for statistically - relevant analyses .
Third , the dataset annotations contain a small false - positive rate for sarcastic labels thus providing reliable annotations .
Also , its self - annotation scheme rules out the annotation errors induced by third - party annotators .
Finally , the corpus structure provides meta-data ( e.g. , user information ) for its comments , which is useful for contextual modeling .
Training details
We holdout 10 % of the training data for validation .
Hyper- parameter tuning is performed using this validation set through RandomSearch .
To optimize the parameters , Adam optimizer ( Kingma and Ba , 2014 ) is used , starting with an initial learning rate of 1e ? 4 .
The learnable parameters in the network consists of ? = { U d , D , W
Training termination is decided using early stopping technique with a patience of 12 .
For the batched - modeling of comments in CNNs , each comment is either restricted or padded to 100 words for uniformity .
The optimal hyper - parameters are found to be {d s , d p , d t , K} = 100 , d em = 300 , k s = 2 , M = 128 , and ? = ReLU ( Implementation details are provided in the supplementary ) .
?
5 % : significantly better than CUE - CNN .
Baseline Models
Here we describe the state - of - the - art methods and baselines that we compare CASCADE with .
Bag - of - Words :
This model uses a comment 's word - counts as features in a vector .
The size of the vector is the vocabulary size of the training dataset .
CNN : We compare our model with this individual CNN version .
This CNN is capable of modeling only the content of a comment .
The architecture is similar to the CNN used in CASCADE ( see Section 3.2 ) .
CNN - SVM :
This model proposed by consists of a CNN for content modeling and other pre-trained CNNs for extracting sentiment , emotion and personality features from the given comment .
All the features are concatenated and fed into an SVM for classification .
CUE - CNN :
This method proposed by also models user embeddings with a method akin to ParagraphVector .
Their embeddings are then combined with a CNN thus forming the CUE - CNN model .
We compare with this model to analyze the efficiency of our embeddings as opposed to theirs .
Released software 4 is used to produce results on the SARC dataset .
presents the performance results on the SARC datasets .
CASCADE manages to achieve major improvement across all datasets with statistical significance .
The lowest performance is obtained by the Bag - of - words approach whereas all neural architectures outperform it .
Amongst the neural networks , the CNN baseline receives the least performance .
CASCADE comfortably beats the state - of - the - art neural models CNN - SVM and CUE - CNN .
Its improved performance on the Main imbalanced dataset also reflects its robustness towards class imbalance and establishes it as a real - world deployable network .
We further compare our proposed user -profiling method with that of CUE - CNN , with absolute differences shown in the bottom row of .
Since CUE - CNN generates its user embeddings using a method similar to the ParagraphVector , we test the importance of personality features being included in our user profiling .
As seen in the table , CASCADE without personality features drops in performance to a range similar to CUE - CNN .
This suggests that the combination of stylometric and personality features are indeed crucial for the improved performance of CASCADE .
Results
Ablation Study
We experiment on multiple variants of CASCADE so as to analyze the importance of the various features present in its architecture .
provides the results of all the combinations .
First , we test performance for the content - based CNN only ( row 1 ) .
This setting provides the worst relative performance with almost 10 % lesser accuracy than optimal .
Next , we include contextual features to this network .
Here , the effect of discourse features is primarily seen in the Pol dataset getting an increase of 3 % in F1 ( row 2 ) .
A major boost in performance is observed ( 8 ? 12 % accuracy and F1 ) when user embeddings are introduced ( row 5 ) .
Visualization of the user embedding cluster ( Section 4.6 ) provides insights for this positive trend .
Overall , CASCADE consisting of CNN with user embeddings and contextual discourse features provide the best performance in all three datasets ( row 6 ) .
We challenge the use of CCA for the generation of user embeddings and thus replace it with simple concatenation .
This however causes a significant drop in performance ( row 3 ) .
Improvement is not observed even when discourse features are used with these concatenated user embeddings ( row 4 ) .
We assume the increase in parameters caused by concatenation for this performance degradation .
CCA on the other hand creates succinct representations with maximal information , giving better results .
User Embedding Analysis
We investigate the learnt user embeddings in more detail .
In particular , we plot random samples of users on a 2D - plane using t- SNE .
The users who have greater sarcastic comments ( atleast 2 more than the other type ) are termed as sarcastic users ( colored red ) .
Conversely , the users having lesser sarcastic comments are called non-sarcastic users ( colored green ) .
Equal number of users from both the categories are plotted .
We aim to analyze the reason behind the performance boost provided by the user embeddings as shown in .
We see in that both the user types belong to similar distributions .
However , the sarcastic users have a greater spread than the non-sarcastic ones ( red belt around the green region ) .
This is also evident from the variances of the distributions where the sarcastic distribution comprises of 10.92 variance as opposed to 5.20 variance of the non-sarcastic distribution .
We can infer from this observation that the user embeddings belonging to this non-overlapping red-region provide discriminative information regarding the sarcastic tendencies of their users .
Case Studies
Results demonstrate that discourse features provide an improvement over baselines , especially on the Pol dataset .
This signifies the greater role of the contextual cues for classifying comments in this dataset over the other dataset variants used in our experiment .
Below , we present a couple of cases from the Pol dataset where our model correctly identifies the sarcasm which is evident only with the neighboring comments .
The previous state - of - the - art CUE - CNN , however , misclassifies them .
For the comment Whew , I feel much better now ! , it s sarcasm is evident only when it s previous comment is seen
So all of the US presidents are terrorists for the last 5 years .
The comment The part where Obama signed it .
does n't seem to be sarcastic until looked upon as a remark to its previous comment
What part of this would be unconstitutional ?.
Such observations indicate the impact of discourse features .
However , sometimes contextual cues from the previous comments are not enough and misclassifications are observed due to lack of necessary commonsense and background knowledge about the topic of discussion .
There are also other cases where our model fails despite the presence of contextual information from the previous comments .
During exploration , this is primarily observed for contextual comments which are very long .
Thus , sequential discourse modeling using RNNs maybe better suited for such cases .
Also , in the case of user embeddings , CASCADE Main Pol user dis - balanced imbalanced cca concat .
course Acc. F1 Acc.
F1
Acc. misclassifications were common for users with lesser historical posts .
In such scenarios , potential solutions would be to create user networks and derive information from similar users within the network .
These are some of the issues which we plan to address in future work .
Conclusion
In this paper we introduce Contextual Sarcasm Detector called as CASCADE which leverages both content and contextual information for the classification .
For contextual details , we perform user profiling along with discourse modeling from comments in discussion threads .
When this information is used jointly with a CNN - based textual model , we obtain state - of - the - art performance on a large - scale Reddit corpus .
Our results show that discourse features along with user embeddings play a crucial role in the performance of sarcasm detection .
