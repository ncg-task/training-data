159	113	123	evaluating
159	124	149	several simplified models
159	71	76	model
160	4	24	first simplification
160	28	34	to use
160	45	52	without
160	57	82	input attention mechanism
160	87	91	with
160	96	119	pooling attention layer
161	4	10	second
161	11	18	removes
161	19	44	both attention mechanisms
162	4	9	third
162	10	17	removes
162	18	41	both forms of attention
162	59	63	uses
162	66	92	regular objective function
162	93	101	based on
162	106	127	inner product s = r w
162	128	131	for
162	134	159	sentence representation r
162	164	190	relation class embedding w
163	3	15	observe that
163	16	43	all three of our components
163	44	51	lead to
163	52	75	noticeable improvements
163	76	80	over
163	87	96	baselines
149	3	6	use
149	11	38	word2 vec skip - gram model
149	39	47	to learn
149	48	76	initial word representations
149	77	79	on
149	80	89	Wikipedia
150	0	14	Other matrices
150	19	35	initialized with
150	36	49	random values
150	50	59	following
150	62	83	Gaussian distribution
151	3	8	apply
151	11	37	cross-validation procedure
151	38	40	on
151	45	58	training data
151	59	68	to select
151	69	93	suitable hyperparameters
24	3	10	propose
24	13	35	novel CNN architecture
27	4	20	CNN architecture
27	21	30	relies on
27	33	70	novel multi-level attention mechanism
27	71	81	to capture
27	87	114	entity - specific attention
27	117	134	primary attention
27	135	137	at
27	142	153	input level
27	156	171	with respect to
27	176	191	target entities
27	198	235	relation - specific pooling attention
27	238	257	secondary attention
27	258	273	with respect to
27	278	294	target relations
29	7	16	introduce
29	19	70	novel pair - wise margin - based objective function
29	76	82	proves
29	83	91	superior
29	92	94	to
29	95	118	standard loss functions
2	0	23	Relation Classification
153	3	15	observe that
153	20	53	novel attentionbased architecture
153	54	62	achieves
153	63	97	new state - of - the - art results
154	0	17	Att - Input - CNN
154	18	32	relies only on
154	37	53	primal attention
154	54	56	at
154	61	72	input level
154	75	85	performing
154	86	108	standard max - pooling
154	109	114	after
154	119	136	convolution layer
154	137	148	to generate
154	153	171	network output w O
156	0	49	Our full dual attention model Att - Pooling - CNN
156	50	58	achieves
156	62	91	even more favorable F1- score
156	92	94	of
156	95	99	88 %
155	0	4	With
155	5	22	Att - Input - CNN
155	28	35	achieve
155	39	47	F1-score
155	48	50	of
155	51	57	87.5 %
155	73	86	outperforming
155	138	172	an SVM - based approach ( 82.2 % )
155	188	223	wellknown CR - CNN model ( 84.1 % )
155	272	303	newly released DRNNs ( 85.8 % )
