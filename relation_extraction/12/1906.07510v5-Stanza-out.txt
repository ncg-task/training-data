title
Attention Guided Graph Convolutional Networks for Relation Extraction
abstract
Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text .
However , how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question .
Existing approaches employing rule based hard - pruning strategies for selecting relevant partial dependency structures may not always yield optimal results .
In this work , we propose Attention Guided Graph Convolutional Networks ( AGGCNs ) , a novel model which directly takes full dependency trees as inputs .
Our model can be understood as a soft - pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task .
Extensive results on various tasks including cross - sentence n-ary relation extraction and large - scale sentence - level relation extraction show that our model is able to better leverage the structural information of the full dependency trees , giving significantly better results than previous approaches .
1
Case study and attention visualization of this example are provided in the supplementary material .
The deletion mutation on exon - 19 of EGFR gene was present in 16 patients , while the L858E point mutation on exon - 21 was noted .
All patients were treated response .
with gefitinib and showed a partial ROOT DET NN PREP _
ON PREP_OF NN NSUBJ COP NUM PREP_IN NN NN DET MARK PREP_ ON AUXPASS NSUBJPASS ADVCL DET NSUBJPASS PREP_WITH CONJ_AND DOBJ DET AMOD NEXT ROOT
Introduction
Relation extraction aims to detect relations among entities in the text .
It plays a significant role in a variety of natural language processing applications including biomedical knowledge discovery , knowledge base population and question answering .
shows an example about expressing a relation sensitivity among three entities L858E , EGFR and gefitinib in two sentences .
Most existing relation extraction models can be categorized into two classes : sequence - based and dependency - based .
Sequence - based models operate only on the word sequences 2014 ; , whereas dependencybased models incorporate dependency trees into the models .
Compared to sequence - based models , dependency - based models are able to capture non-local syntactic relations thatare obscure from the surface form alone .
Various pruning strategies are also proposed to distill the dependency information in order to further improve the performance .
apply neural networks only on the shortest dependency path between the entities in the full tree .
reduce the full tree to the subtree below the lowest common ancestor ( LCA ) of the entities .
apply graph convolutional networks ( GCNs ) model over a pruned tree .
This tree includes tokens thatare up to distance K away from the dependency path in the LCA subtree .
However , rule - based pruning strategies might eliminate some important information in the full tree .
shows an example in cross - sentence n- ary relation extraction that the key tokens partial response would be excluded if the model only takes the pruned tree into consideration .
1
Ideally , the model should be able to learn how to maintain a balance between including and excluding information in the full tree .
In this paper , we propose the novel Attention Guided Graph Convolutional Networks ( AGGCNs ) , which operate directly on the full tree .
Intuitively , we develop a " soft pruning " strategy that transforms the original dependency tree into a fully connected edgeweighted graph .
These weights can be viewed as the strength of relatedness between nodes , which can be learned in an end - to - end fashion by using self - attention mechanism .
In order to encode a large fully connected graph , :
An example dependency tree for two sentences expressing a relation ( sensitivity ) among three entities .
The shortest dependency path between these entities is highlighted in bold ( edges and tokens ) .
The root node of the LCA subtree of entities is present .
The dotted edges indicate tokens K = 1 away from the subtree .
Note that tokens partial response off these paths ( shortest dependency path , LCA subtree , pruned tree when K = 1 ) .
we next introduce dense connections ) to the GCN model following .
For GCNs , L layers will be needed in order to capture neighborhood information that is L hops away .
A shallow GCN model may not be able to capture non-local interactions of large graphs .
Interestingly , while deeper GCNs can capture richer neighborhood information of a graph , empirically it has been observed that the best performance is achieved with a 2 - layer model .
With the help of dense connections , we are able to train the AGGCN model with a large depth , allowing rich local and non-local dependency information to be captured .
Experiments show that our model is able to achieve better performance for various tasks .
For the cross - sentence relation extraction task , our model surpasses the current state - of - the - art models on multi-class ternary and binary relation extraction by 8 % and 6 % in terms of accuracy respectively .
For the large - scale sentence - level extraction task ( TACRED dataset ) , our model is also consistently better than others , showing the effectiveness of the model on a large training set .
Our code is available at https://github.com/Cartus / AGGCN_TACRED
2 Our contributions are summarized as follows :
We propose the novel AGGCNs that learn a " soft pruning " strategy in an end - to - end fashion , which learns how to select and discard information .
Combining with dense connections , our AGGCN model is able to learn a better graph representation .
Our model achieves new state - of - the - art results without additional computational over - 2 Implementation is based on Pytorch .
head when compared with previous GCNs .
3 Unlike tree - structured models ( e.g. , Tree - LSTM ) , it can be efficiently applied over dependency trees in parallel .
Attention Guided GCNs
In this section , we will present the basic components used for constructing our AGGCN model .
GCNs
GCNs are neural networks that operate directly on graph structures ( Kipf and Welling , 2017 ) .
Here we mathematically illustrate how multi - layer GCNs work on a graph .
Given a graph with n nodes , we can represent the graph with an n n adjacency matrix A. extend GCNs for encoding dependency trees by incorporating directionality of edges into the model .
They add a self - loop for each node in the tree .
Opposite direction of a dependency arc is also included , which means A ij = 1 and A ji = 1 if there is an edge going from node i to node j , otherwise A ij = 0 and A ji = 0 .
The convolution computation for node i at the l - th layer , which takes the input feature representation h ( l?1 ) as input and outputs the induced representation h
i , can be defined as :
where W ( l ) is the weight matrix , b ( l ) is the bias vector , and ?
is an activation function ( e.g. , RELU ) .
h
i is the initial input x i , where xi ?
Rd and d is the input feature dimension . :
The AGGCN model is shown with an example sentence and its dependency tree .
It is composed of M identical blocks and each block has three types of layers as shown on the right .
Every block takes node embeddings and adjacency matrix that represents the graph as inputs .
Then N attention guided adjacency matrices are constructed by using multi-head attention as shown at bottom left .
The original dependency tree is transformed into N different fully connected edge - weighted graphs ( self - loops are omitted for simplification ) .
Numbers near the edges represent the weights in the matrix .
Resulting matrices are fed into N separate densely connected layers , generating new representations .
Top left shows an example of the densely connected layer , where the number ( L ) of sub - layers is 3 ( L is a hyper -parameter ) .
Each sub - layer concatenates all preceding outputs as the input .
Eventually , a linear combination is applied to combine outputs of N densely connected layers into hidden representations .
Attention Guided Layer
The AGGCN model is composed of M identical blocks as shown in .
Each block consists of three types of layers : attention guided layer , densely connected layer and linear combination layer .
We first introduce the attention guided layer of the AGGCN model .
As we discuss in Section 1 , most existing pruning strategies are predefined .
They prune the full tree into a subtree , based on which the adjacency matrix is constructed .
In fact , such strategies can also be viewed as a form of hard attention , where edges that connect nodes not on the resulting subtree will be directly assigned zero weights ( not attended ) .
Such strategies might eliminate relevant information from the original dependency tree .
Instead of using rule - based pruning , we develop a " soft pruning " strategy in the attention guided layer , which assigns weights to all edges .
These weights can be learned by the model in an end - to - end fashion .
In the attention guided layer , we transform the original dependency tree into a fully connected edge - weighted graph by constructing an attention guided adjacency matrix .
Each corresponds to a certain fully connected graph and each entr ?
A ij is the weight of the edge going from node i to node j.
As shown in , ( 1 ) represents a fully connected graph G ( 1 ) .
can be constructed by using self - attention mechanism , which is an attention mechanism that captures the interactions between two arbitrary positions of a single sequence .
Once we get , we can use it as the input for the computation of the later graph convolutional layer .
Note that the size of is the same as the original adjacency matrix A ( n n ) .
Therefore , no additional computational overhead is involved .
The key idea behind the attention guided layer is to use attention for inducing relations between nodes , especially for those connected by indirect , multi-hop paths .
These soft relations can be captured by differentiable functions in the model .
Here we compute by using multi-head attention , which allows the model to jointly attend to information from different representation subspaces .
The calculation involves a query and a set of key - value pairs .
The output is computed as a weighted sum of the values , where the weight is computed by a function of the query with the corresponding key .
where Q and K are both equal to the collective representation h ( l?1 ) at layer l ?
1 of the AG - GCN model .
The projections are parameter matrices W Q i ?
R dd and W K i ?
R dd . ( t ) is the t - th attention guided adjacency matrix corresponding to the t- th head .
Up to N matrices are constructed , where N is a hyper - parameter .
shows an example that the original adjacency matrix is transformed into multiple attention guided adjacency matrices .
Accordingly , the input dependency tree is converted into multiple fully connected edge - weighted graphs .
In practice , we treat the original adjacency matrix as an initialization so that the dependency information can be captured in the node representations for later attention calculation .
The attention guided layer is included starting from the second block .
Densely Connected Layer
Unlike previous pruning strategies , which lead to a resulting structure that is smaller than the original structure , our attention guided layer outputs a larger fully connected graph .
Following , we introduce dense connections ) into the AGGCN model in order to capture more structural information on large graphs .
With the help of dense connections , we are able to train a deeper model , allowing rich local and non-local information to be captured for learning a better graph representation .
Dense connectivity is shown in .
Direct connections are introduced from any layer to all its preceding layers .
Mathematically , we first define g ( l ) j as the concatenation of the initial node representation and the node representations produced in layers 1 , , l ?
1 :
(
In practice , each densely connected layer has L sub - layers .
The dimensions of these sub - layers d hidden are decided by Land the input feature dimension d.
In AGGCNs , we used hidden = d/ L. For example , if the densely connected layer has 3 sub - layers and the input dimension is 300 , the hidden dimension of each sub - layer will bed hidden = d/L = 300/3 = 100 .
Then we concatenate the output of each sub - layer to form the new representation .
Therefore , the output dimension is 300 ( 3 100 ) .
Different from the GCN model whose hidden dimension is larger than or equal to the input dimension , the AGGCN model shrinks the hidden dimension as the number of layers increases in order to improves the parameter efficiency similar to DenseNets ) .
Since we have N different attention guided adjacency matrices , N separate densely connected layers are required .
Accordingly , we modify the computation of each layer as follows ( for the t- th matrix ( t ) ) :
where t = 1 , ... , N and t selects the weight matrix and bias term associated with the attention guided adjacency matrix .
The column dimension of the weight matrix increases by d hidden per sub - layer , i.e. , W
Linear Combination Layer
The AGGCN model includes a linear combination layer to integrate representations from N different densely connected layers .
Formally , the output of the linear combination layer is defined as :
where h out is the output by concatenating outputs from N separate densely connected layers , i.e. , h out = [ h ( 1 ) ; ... ; h ( N ) ] ?
R d N .
W comb ? R ( dN )d is a weight matrix and b comb is a bias vector for the linear transformation .
AGGCNs for Relation Extraction
After applying the AGGCN model over the dependency tree , we obtain hidden representations of all tokens .
Given these representations , the goal of relation extraction is to predict a relation among entities .
Following , we concatenate the sentence representation and entity representations to get the final representation for classification .
First we need to obtain the sentence representation h sent .
It can be computed as :
where h mask represents the masked collective hidden representations .
Masked here means we only select representations of tokens thatare not entity tokens in the sentence . f :
R dn ?
R d 1 is a max pooling function that maps from n output vectors to 1 sentence vector .
Similarly , we can obtain the entity representations .
For the i - th entity , it s representation he i can be computed as :
where he i indicates the hidden representation corresponding to the i - th entity .
4 Entity representations will be concatenated with sentence representation to form a new representation .
Following , we apply a feed - forward neural network ( FFNN ) over the concatenated representations inspired by relational reasoning works :
where hf inal will betaken as inputs to a logistic regression classifier to make a prediction .
3 Experiments
Data
We evaluate the performance of our model on two tasks , namely , cross - sentence n-ary relation extraction and sentence - level relation extraction .
For the cross - sentence n-ary relation extraction task , we use the dataset introduced in , which contains 6,987 ternary relation instances and 6,087 binary relation instances extracted from PubMed .
5
Most instances contain multiple sentences and each instance is assigned with one of the five labels , including : " resistance or nonresponse " , " sensitivity " , " response " , " resistance " and " None " .
We consider two specific tasks for evaluation , i , e. , binary - class n-ary relation extraction and multi-class n-ary relation extraction .
For binary - class n-ary relation extraction , we follow to binarize multi-class labels by grouping the four relation classes as " Yes " and treating " None " as " No " .
For the sentence - level relation extraction task , we follow the experimental settings in to evaluate our model on the TACRED dataset and Semeval - 10 Task 8 .
With over 106K instances , the TACRED dataset introduces 41 relation types and a special " no relation " type to describe the relations between the mention pairs in instances .
Subject mentions are categorized into person and organization , while object mentions are categorized into 16 fine - grained types , including date , location , etc .
Semeval - 10 Task 8 is a public dataset , which contains 10,717 instances with 9 relations and a special " other " class .
The number of entities is fixed in n-ary relation extraction task .
It is 3 for the first dataset and 2 for the second .
The dataset is available at https://github.com/
freesunshine0316/nary-grn
Setup
We tune the hyper - parameters according to results on the development sets .
For the cross - sentence nary relation extraction task , we use the same data split used in , while for the sentence - level relation extraction task , we use the same development set from
We choose the number of heads N for attention guided layer from { 1 , 2 , 3 , 4 } , the block number M from { 1 , 2 , 3 } , the number of sub - layers L in each densely connected layer from { 2 , 3 , 4 }.
7 Through preliminary experiments on the development sets , we find that the combinations ( N = 2 , M = 2 , L 1 = 2 , L 2 = 4 , d hidden = 340 ) 8 and ( N = 3 , M = 2 , L 1 = 2 , L 2 = 4 , d hidden = 300 ) give the best results on cross - sentence n-ary relation extraction and sentence - level relation extraction , respectively .
Glo Ve vectors are used as the initialization for word embeddings .
Models are evaluated using the same metrics as previous work .
We report the test accuracy averaged over five cross validation folds for the cross - sentence n-ary relation extraction task .
For the sentence - level relation extraction task , we report the micro-averaged F 1 scores for the TA - CRED dataset and the macro -averaged F 1 scores for the SemEval dataset .
For TACRED dataset , we report the mean test F 1 score by using 5 models from independent runs .
Results on Cross - Sentence n- ary Relation Extraction
For cross - sentence n- ary relation extraction task , we consider three kinds of models as baselines :
1 ) a feature - based classifier based on shortest dependency paths between all entity pairs , 2 ) Graph - structured LSTM methods , including Graph LSTM , bidirectional DAG LSTM ( Bidir DAG LSTM ) and Graph State LSTM ( GS GLSTM ) .
These methods extend LSTM to encode graphs constructed from input sentences with dependency edges , 3 ) Graph convolutional networks ( GCN ) with pruned trees , 6 https://nlp.stanford.edu/projects/
tacred /
7 8 Similar to , each layer has two different sub - layers .
We use the 300 - dimensional Glove word vectors trained on the Common Crawl corpus https://nlp. stanford.edu/projects/glove/
Model
Binary - class
Multi-class
T B T B Single Cross Single Cross Cross Cross
Feature - Based 74.7 77.7 73.9 75.2 -- SPTree -- 75.9 75.9 -- Graph LSTM-EMBED 80.6 74.3 76.5 -- Graph LSTM-FULL 77.9 80.7 75.6 76.7 --00000000000000000 + multi-task - 82.0 - 78.5
-- Bidir DAG LSTM 75.6 77.3 76.9 76.4 51.7 50.7 GS GLSTM
80 : Average test accuracies in five - fold validation for binary - class n-ary relation extraction and multi-class n-ary relation extraction .
" T " and " B " denote ternary drug - gene-mutation interactions and binary drug-mutation interactions , respectively .
Single means that we report the accuracy on instances within single sentences , while Cross means the accuracy on all instances .
K in the GCN models means that the preprocessed pruned trees include tokens up to distance K away from the dependency path in the LCA subtree .
which have shown efficacy on the relation extraction task 10 .
Additionally , we follow to consider the tree - structured LSTM method ( SPTree ) ( Miwa and Bansal , 2016 ) on drug-mutation binary relation extraction .
Main results are shown in .
We first focus on the binary - class n-ary relation extraction task .
For ternary relation extraction ( first two columns in ) , our AGGCN model achieves accuracies of 87.1 and 87.0 on instances within single sentence ( Single ) and on all instances ( Cross ) , respectively , which outperform all the baselines .
More specifically , our AG - GCN model surpasses the state - of - the - art Graphstructured LSTM model ( GS GLSTM ) by 6.8 and 3.8 points for the Single and Cross settings , respectively .
Compared to GCN models , our model obtains 1.3 and 1.2 points higher than the best performing model with pruned tree ( K=1 ) .
For binary relation extraction ( third and fourth columns in ) , AGGCN consistently outperforms GS GLSTM and GCN as well .
These results suggest that , compared to previous full tree based methods , e.g. , GS GLSTM , AGGCN is able to extract more information from the underlying graph structure to learn a more expressive representation through graph convolutions .
AGGCN also performs better than GCNs , although its performance can be boosted via pruned trees .
We believe this is because of the combination of densely connected layer and attention guided layer .
The dense connections could facilitate information propagation in large graphs , enabling AGGCN to efficiently learn from long - distance dependencies without pruning techniques .
Meanwhile , the attention guided layer can further distill relevant information and filter out noises from the representation learned by the densely connected layer .
We next show the results on the multi -class classification task ( last two columns in ) .
We follow ) to evaluate our model on all instances for both ternary and binary relations .
This fine - grained classification task is much harder than coarse - grained classification task .
As a result , the performance of all models degrades a lot .
However , our AGGCN model still obtains 8.0 and 5.7 points higher than the GS GLSTM model for ternary and binary relations , respectively .
We also notice that our AGGCN achieves a better test accuracy than all GCN models , which further demonstrates its ability to learn better representations from full trees .
Results on Sentence - level Relation Extraction
We now report the results on the TACRED dataset for the sentence - level relation extraction task in Model PR F1
LR 73.5 49.9 59.4 SDP - LSTM Tree-LSTM ** 66.0 59.2 62.4 PA- LSTM 65.7 64.5 65.1 GCN 69.8 59.0 64.0 C-GCN 69.9 63.3 66.4 AGGCN ( ours ) 69.9 60.9 65.1 C - AGGCN ( ours ) 73.1 64.2 68.2
Model F1
C- AGGCN
( Full tree ) 68.2 C - AGGCN ( K=2 ) 67.5 C - AGGCN ( K=1 ) 67.9 C - AGGCN ( K=0 ) 67.0 model with them .
We can observe that AGGCN outperforms GCN by 1.1 F1 points .
We speculate that the limited improvement is due to the lack of contextual information about word order or dis ambiguation .
Similar to C - GCN , we extend our AGGCN model with a bidirectional LSTM network to capture the contextual representations which are subsequently fed into AGGCN layers .
We term the modified model as C - AGGCN .
Our C - AGGCN model achieves an F1 score of 68.2 , which outperforms the state - ofart C - GCN model by 1.8 points .
We also notice that AGGCN and C - AGGCN achieve better precision and recall scores than GCN and C - GCN , respectively .
The performance gap between GCNs with pruned trees and AGGCNs with full trees empirically show that the AGGCN model is better at distinguishing relevant from irrelevant information for learning a better graph representation .
We also evaluate our model on the SemEval dataset under the same settings as .
Results are shown in .
This dataset is much smaller than TACRED ( only 1/10 of TA - CRED in terms of the number of instances ) .
Our C - AGGCN model ( 85.7 ) consistently outperforms the C - GCN model ( 84.8 ) , showing the good generalizability .
Analysis and Discussion
Ablation Study .
We examine the contributions of two main components , namely , densely connected layers and attention guided layers , using the best - performing C - AGGCN model on the TA - CRED dataset .
shows the results .
We can observe that adding either attention guided layers or densely connected layers improves the performance of the model .
This suggests that both layers can assist GCNs to learn better information aggregations , producing better representations for graphs , where the attention - guided layer seems to be playing a more significant role .
We also notice that the feed - forward layer is effective in our model .
Without the feed - forward layer , the result drops to an F1 score of 67.8 .
Performance with Pruned Trees .
shows the performance of the C - AGGCN model with pruned trees , where K means that the pruned trees include tokens thatare up to distance K away from the dependency path in the LCA subtree .
We can observe that all the C - AGGCN models with varied values of K are able to outperform the state - of - the - art C - GCN model ( reported in ) .
Specifically , with the same setting as K =1 , C - AGGCN surpasses C - GCN by 1.5 points of F1 score .
This demonstrates that , with the combination of densely connected layer and attention guided layer , C - AGGCN can learn better representations of graphs than C - GCN for downstream tasks .
In addition , we notice that the performance of C - AGGCN with full trees outperforms all C - AGGCNs with pruned trees .
These results further show the superiority of " soft pruning " strategy over hard pruning strategy in utilizing full tree information .
Performance against Sentence Length .
shows the F 1 scores of three models under different sentence lengths .
We partition the sentence length into five classes , [ 40 , 50 ) , ? 50 ) .
In general , C - AGGCN with full trees outperforms C - AGGCN with pruned trees and C - GCN against various sentence lengths .
We also notice that C - AGGCN with pruned trees performs better than C - GCN in most cases .
Moreover , the improvement achieved by C - AGGCN with pruned trees decays when the sentence length increases .
Such a performance degradation can be avoided by using full trees , which provide more information of the underlying graph structures .
Intuitively , with the increase of the sentence length , the dependency graph becomes larger as more nodes are included .
This suggests that C - AGGCN can benefit more from larger graphs ( full tree ) .
Performance against Training Data Size .
shows the performance of C - AGGCN and C - GCN against different training settings .
We consider five training settings ( 20 % , 40 % , 60 % , 80 % , 100 % of the training data ) .
C - AGGCN consistently outperforms C - GCN under the same amount of training data .
When the size of training data increases , we can observe that the performance gap becomes more obvious .
Particularly , using 80 % of the training data , the C - AGGCN model is able to achieve a F 1 score of 66.5 , higher than C - GCN trained on the whole dataset .
These results demonstrate that our model is more effective in terms of using training resources .
Related Work
Our work builds on a rich line of recent efforts on relation extraction models and graph convolutional networks .
Relation Extraction .
Early research efforts are based on statistical methods .
Tree- based kernels and dependency path - based kernels are explored to extract the relation .
construct maximal cliques of entities to predict relations .
include syntactic features to a statistical classifier .
Recently , sequencebased models leverages different neural networks to extract relations , including convolutional neural networks , recurrent neural networks the combination of both and transformer .
Dependency - based approaches also try to incorporate structural information into the neural models .
first split the dependency graph into two DAGs , then extend the tree LSTM model over these two graphs for n-ary relation extraction .
Closest to our work , use graph recurrent networks to directly encode the whole dependency graph without breaking it .
The contrast between our model and theirs is reminiscent of the contrast between CNN and RNN .
Various pruning strategies have also been proposed to distill the dependency information in order to further improve the performance .
adapt neural models to encode the shortest dependency path .
apply LSTM model over the LCA subtree of two entities .
combine the shortest dependency path and the dependency subtree .
adopt a path - centric pruning strategy .
Unlike these strategies that remove edges in preprocessing , our model learns to assign each edge a different weight in an end - to - end fashion .
Graph Convolutional
Networks .
Early efforts that attempt to extend neural networks to deal with arbitrary structured graphs are introduced by .
Subsequent efforts improve its computational efficiency with local spectral convolution techniques .
Our approach is closely related to the GCNs ( Kipf and Welling , 2017 ) , which restrict the filters to operate on a first - order neighborhood around each node .
More recently , proposed graph attention networks ( GATs ) to summarize neighborhood states by using masked selfattentional layers .
Compared to our work , their motivations and network structures are different .
In particular , each node only attends to its neighbors in GATs whereas AG - GCNs measure the relatedness among all nodes .
The network topology in GATs remains the same , while fully connected graphs will be builtin AG - GCNs to capture long - range semantic interactions .
Conclusion
We introduce the novel Attention Guided Graph Convolutional Networks ( AGGCNs ) .
Experimental results show that AGGCNs achieve state - of the - art results on various relation extraction tasks .
Unlike previous approaches , AGGCNs operate directly on the full tree and learn to distill the useful information from it in an end - to - end fashion .
There are multiple venues for future work .
One natural question we would like to ask is how to make use of the proposed framework to perform improved graph representation learning for graph related tasks .
Supplemental Material
A Case Study shows an instance in cross - sentence nary relation extraction task , which suggests that tumors with L858E mutation in EGFR gene partially responds to the drug gefitinib .
The relation between these three entities is sensitivity .
We apply path - centric pruning on these dependency trees .
The top one in shows the pruned tree when K = 0 and the bottom one shows the pruned tree when K = 1 .
The state - of - the - art model , GCN over pruned tree ( K = 0 and K = 1 ) predict the relation to be response rather than sensitivity .
We hypothesize the reason is that the pruned tree miss the crucial information ( i.e. , partial response ) .
The GCN model is notable to capture the interactions between removed tokens between entities , since these tokens are not in the resulting structure .
Our AGGCN model predict the correct relation for this instance by including the attention guided layer , which is able to distill relevant information from the full tree in an end - to - end fashion .
As shown in , we visualize the attention scores of two heads in the attention guided layer .
We can observe that relevant tokens including entity tokens and tokens that help to predict the correct relation ( i.e. , showed a partial response ) can be attended by other tokens , especially the entity tokens .
On the other hand , it is worthwhile to filter out noises from the dependency tree , especially when it is large .
In the pruned tree shown in , most stop words are removed directly in order to eliminate irrelevant information .
In the AGGCN model , stop words including on , of , in , with , an , were also have much lower scores .
We believe the AGGCN model is able to maintain a balance between including and excluding information in the full tree for learning a better graph representation .
compare their Graph State LSTM model with the bidirectional DAG LSTM model against different sentence lengths and different maximal number of neighbors in order to better evaluate their model .
Following the same setting , we compare our the test accuracies of the AGGCN model and Graph State LSTM ( GS GLSTM ) under these two settings on crosssentence n-ary relation extraction task as shown
B Additional Analysis
AUXPASS
The deletion mutation on exon - 19 of EGFR gene was present in 16 patients , while the L858E point mutation on exon - 21 was noted .
All patients were treated response .
with gefitinib and showed a partial
All patients were treated response .
with gefitinib and showed a partial in .
Accuracy against sentence length
We can observe that AGGCN consistently outperforms GS GLSTM .
Specifically , the performance of GS GLSTM drops when the sentence is short , while the performance of AGGCN is stable .
This shows that the superiority of AGGCN over GS GLSTM in utilizing context information .
Accuracy against maximal number of neighbors
Intuitively , it is easier to model graphs containing nodes with more neighbors , because these nodes can serve as a " supernode " that allow more efficient information exchange ) .
The AGGCN model performs well under the inputs having lower maximal number of neighbors .
We hypothesize that is because the attention guided layer convert the original graph into a fully connected graph , which encourages the information propagation .
