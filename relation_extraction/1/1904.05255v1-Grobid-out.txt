title
Simple BERT Models for Relation Extraction and Semantic Role Labeling
abstract
We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.
Introduction
Relation extraction and semantic role labeling (SRL) are two fundamental tasks in natural language understanding. The task of relation extraction is to discern whether a relation exists between two entities in a sentence. For example, in the sentence "Obama was born in Honolulu", "Obama" is the subject entity and "Honolulu" is the object entity. The task of a relation extraction model is to identify the relation between the entities, which is per:city of birth (birth city for a person). For SRL, the task is to extract the predicate-argument structure of a sentence, determining "who did what to whom", "when", "where", etc. Both capabilities are useful in several downstream tasks such as question answering and open information extraction.
State-of-the-art neural models for both tasks typically rely on lexical and syntactic features, such as part-of-speech tags , syntactic trees, and global decoding constraints. In particular, argue that syntactic features are necessary to achieve competitive performance in dependency-based SRL.  also showed that dependency tree features can further improve relation extraction performance. Although syntactic features are no doubt helpful, a known challenge is that parsers are not available for every language, and even when available, they may not be sufficiently robust, especially for out-of-domain text, which may even hurt performance.
Recently, the NLP community has seen excitement around neural models that make heavy use of pretraining based on language modeling. The latest development is BERT, which has shown impressive gains in a wide variety of natural language tasks ranging from sentence classification to sequence labeling. A natural question follows: can we leverage these pretrained models to further push the state of the art in relation extraction and semantic role labeling, without relying on lexical or syntactic features? The answer is yes. We show that simple neural architectures built on top of BERT yields state-of-the-art performance on a variety of benchmark datasets for these two tasks. The remainder of this paper describes our models and experimental results for relation extraction and semantic role labeling in turn.

BERT for Relation Extraction

Model
The standard formulation of semantic role labeling decomposes into four subtasks: predicate detection, predicate sense dis ambiguation, argument identification, and argument classification. There are two representations for argument annotation: span-based and dependency-based. Semantic banks such as PropBank usually represent arguments as syntactic constituents (spans), whereas the CoNLL 2008 and 2009 shared tasks propose dependency-based SRL, where the goal is to identify the syntactic heads of arguments rather than the entire span. Here, we follow to unify these two annotation schemes into one framework, without any declarative constraints for decoding. For several SRL benchmarks, such as, and 2012, the predicate is given during both training and testing. Thus, in this paper, we only discuss predicate dis ambiguation and argument identification and classification.
Predicate sense dis ambiguation. The predicate dis ambiguation task is to identify the correct meaning of a predicate in a given context. As an example, for the sentence "Barack Obama went to Paris", the predicate went has sense "motion" and has sense label 01. We formulate this task as sequence labeling. The input sentence is fed into the WordPiece tokenizer, which splits some words into sub-tokens. The predicate token is tagged with the sense label. Following the original BERT paper, two labels are used for the remaining tokens: 'O' for the first (sub-)token of any word and 'X' for any remaining fragments. We feed the sequences into the BERT encoder to obtain the contextual representation H. A "predicate indicator" embedding is then concatenated to the contextual representation to distinguish the predicate tokens from nonpredicate ones. The final prediction is made using a one-hidden-layer MLP over the label set.
Argument identification and classification. This task is to detect the argument spans or argument syntactic heads and assign them the correct semantic role labels. In the above example, "Barack Obama" is the ARG1 of the predicate went, meaning the entity in motion.
Formally, our task is to predict a sequence z given a sentence-predicate pair (X , v) as input,   where the label set draws from the cross of the standard BIO tagging scheme and the arguments of the predicate (e.g., B-ARG1). The model architecture is illustrated in, at the point in the inference process where it is outputting a tag for the token "Barack". In order to encode the sentence in a predicate-aware manner, we design the input as [[CLS] sentence [SEP] predicate], allowing the representation of the predicate to interact with the entire sentence via appropriate attention mechanisms. The input sequence as described above is fed into the BERT encoder. The contextual representation of the sentence ([CLS] sentence [SEP]) from BERT is then concatenated to predicate indicator embeddings, followed by a one-layer BiLSTM to obtain hidden states G = [g 1 , g 2 , ..., g n ]. For the final prediction on each token g i , the hidden state of predicate g p is concatenated to the hidden state of the token g i , and then fed into a one-hidden-layer MLP classifier over the label set.

Experiments
We evaluate our model on the TAC Relation Extraction Dataset (TACRED) , a standard benchmark dataset for relation extraction. In our experiments, the hidden sizes of the LSTM and MLP are 768 and 300, respectively, and the position embedding size is 20. The learning rate is 5 ? 10 ?5 . The BERT base-cased model is used in our experiments. Embeddings for the masks (e.g., SUBJ-LOC) are randomly initialized and fine-tuned during the training process, as well as the position embeddings.
Results on the TACRED test set are shown in. Our model outperforms the works of  and, which use GCNs (Kipf and Welling, 2016) and variants to encode syntactic tree information as external features. leverage the pretrained language model GPT and achieves better recall than our system. In terms of F 1 , our system obtains the best known score among individual models, but our score is still below that of the interpolation model of  because of lower recall.

BERT for Semantic Role Labeling

Experimental Setup
We conduct experiments on two SRL tasks:   and the predicate indicator embedding size is 10. The learning rate is 5 ? 10 ?5 . BERT base-cased and large-cased models are used in our experiments. The position embeddings are randomly initialized and fine-tuned during the training process.

Dependency-Based SRL Results
Predicate sense dis ambiguation. The predicate sense dis ambiguation subtask applies only to the CoNLL 2009 benchmark. In this line of research on dependency-based SRL, previous papers seldom report the accuracy of predicate dis ambiguation separately (results are often mixed with argument identification and classification), causing difficulty in determining the source of gains. Here, we report predicate dis ambiguation accuracy in. Figures from some systems are missing because they only report end-to-end results.
Our end-to-end results are shown in. We see that the BERT-LSTM-large model (using the predicate sense dis ambiguation results from above) yields large F 1 score improvements over the existing state of the art, and beats existing ensemble models as well. This is achieved without using any linguistic features and declarative decoding constraints.

Span-Based SRL Results
Our span-based SRL results are shown in Table 5. We see that the BERT-LSTM-large model achieves the state-of-the-art F 1 score among single models and outperforms the ensemble model on the CoNLL 2005 in-domain and out-of-domain tests. However, it falls short on the CoNLL 2012 benchmark because the model of obtains very high precision.
They are able to achieve this with a more complex decoding layer, with human-designed constraints such as the "Overlap Constraint" and "Number Constraint".

Conclusions
Based on this preliminary study, we show that BERT can be adapted to relation extraction and semantic role labeling without syntactic features and human-designed constraints. While we concede that our model is quite simple, we argue this is a feature, as the power of BERT is able to simplify neural architectures tailored to specific tasks. Nevertheless, these results provide strong baselines and foundations for future research. Many natural follow-up questions emerge: Can syntactic features be re-introduced to further improve results? Can multitask learning be used to simultaneously benefit relation extraction and semantic role labeling? We are actively working on answering these and additional questions.