title
Generating Text through Adversarial Training using Skip - Thought Vectors
abstract
In the past few years , various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks ( GANs ) .
GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer .
In the field of Natural Language Processing , word embeddings such as word2vec and GLoVe are state - of - the - art methods for applying neural network models on textual data .
Attempts have been made for utilizing GANs with word embeddings for text generation .
This work presents an approach to text generation using Skip - Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures .
The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used .
Introduction
Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation .
Early techniques for generating text conditioned on some input information were template or rule - based engines , or probabilistic models such as n-gram .
In recent times , state - of - the - art results on these tasks have been achieved by recurrent and convolutional neural network models trained for likelihood maximization .
This work pro-Code available at : https://github.com/enigmaeth/skip-thought-gan poses an approach for text generation using Generative Adversarial Networks with Skip - Thought vectors .
GANs are a class of neural networks that explicitly train a generator to produce high - quality samples by pitting against an adversarial discriminative model .
GANs output differentiable values and hence the task of discrete text generation has to use vectors as differentiable inputs .
This is achieved by training the GAN with sentence embedding vectors produced by Skip - Thought , a neural network model for learning fixed length representations of sentences .
Related Works
Deep neural network architectures have demonstrated strong results on natural language generation tasks .
Recurrent neural networks using combinations of shared parameter matrices across time - steps with different gating mechanisms for easing optimization have found some success in modeling natural language .
Another approach is to use convolutional neural networks that reuse kernels across time - steps with attention mechanism to perform language generation tasks .
Supervised learning with deep neural networks in the framework of encoder - decoder models has become the state - of - the - art methods for approaching NLP problems .
Recent text generation models use a wide variety of GANs such as gradient policy based sequence generation framework and an actor-critic conditional GAN to fill missing text conditioned on surrounding text for performing natural language generation tasks .
Other architectures such as those proposed in with RNN and variational autoencoder generator with CNN discriminator and in arXiv : 1808.08703v2 [ cs. CL ] 13 Nov 2018 with leaky discriminator to guide generator through high - level extracted features have also shown great results .
Using adversarial examples of word and character level embeddings for natural language text generation has been explored in .
Models trained using generative adversarial networks or variational autoencoders have been shown to learn representations of continuous structures by leveraging deep latent variables such as text embeddings .
This work explores injecting sentence embeddings produced using the Skip Thought architecture into GANs with different setups .
Skip - Thought Generative
Adversarial Network ( STGAN )
In literature corpora ( eg : fantasy novels , sci - fi novels ) , the vocabulary does not vary significantly across the authors , but the manner of expression does , which is best captured at the level of sentences than words .
The approach that this work takes in generating sentences with the writing style of one author is to make the adversarial model approximate the distribution of all sentences ( rather than words or characters ) in a latent space using skip - thought architecture .
The previous attempts on text generation have used the character and word - level embeddings instead with GANs , for example , in .
This section introduces Skip - Thought Generative Adversarial Network with a background on models that it is based on .
The Skip - Thought model induces embedding vectors for sentences present in training corpus .
These vectors constitute the real distribution for the discriminator network .
The generator network produces sentence vectors similar to those from the encoded real distribution .
The generated vectors are sampled over training and decoded to produce sentences using a Skip - Thought decoder conditioned on the same text corpus .
Skip - Thought Vectors
Skip - Thought is an encoder - decoder framework with an unsupervised approach to train a generic , distributed sentence encoder .
The encoder maps sentences sharing semantic and syntactic properties to similar vector representations and the decoder reconstructs the surrounding sentences of an encoded passage .
The sentence encoding approach draws inspiration from the skip - gram model in producing vector representations using previous and next sentences .
The Skip - Thought model uses an RNN encoder with GRU activations and an RNN decoder with conditional GRU , the combination being identical to the RNN encoder - decoder of ( Cho et al. , 2014 ) used in neural machine translation .
Skip - Thought Architecture
For a given sentence tuple ( s i?1 , s i , s i + 1 ) , let wt i denote the t- th word for sentence s i , and let x ti denote it s word embedding .
The model has three components : Encoder .
Encoded vectors for a sentence s i with N words w i , w i + 1 ,... , w n are computed by iterating over the following sequence of equations :
where ht i is a hidden state at each time step and interpreted as a sequence of words w 1 i , ... , w n i , t is the proposed state update at time t , z t is the update gate and rt is the reset gate .
Both update gates take values between zero and one .
Decoder .
A neural language model conditioned on the encoder output hi serves as the decoder .
Bias matrices C z , Cr , C are introduced for the update gate , reset gate and hidden state computation by the encoder .
Two decoders are used in parallel , one each for sentences s i + 1 and s i ?
1 . The following equations are iterated over for decoding :
Objective .
For the same tuple of sentences , objective function is the sum of log-probabilities for the forward and backward sentences conditioned on the encoder representation :
Generative Adversarial
Networks
Generative Adversarial
Networks are deep neural net architectures comprised of two networks , contesting with each other in a zero - sum game framework .
For a given data , GANs can mimic learning the underlying distribution and generate artificial data samples similar to those from the real distribution .
Generative Adversarial Networks consists of two players -a Generator and a Discriminator .
The generator G tries to produce data close to the real distribution P ( x ) from some stochastic distribution P ( z ) termed as noise .
The discriminator D 's objective is to differentiate between real and generated data G ( z ) .
The two networks - generator and discriminator compete against each other in a zero - sum game .
The minimax strategy dictates that each network plays optimally with the assumption that the other network is optimal .
This leads to Nash equilibrium which is the point of convergence for GAN model .
Objective . have formulated the minimax game for a generator G , discriminator D adversarial network with value function V ( G , D ) as :
Model Architecture
The STGAN model uses a deep convolutional generative adversarial network , similar to the one used in .
The generator network is updated twice for each discriminator network update to prevent fast convergence of the discriminator network .
The Skip - Thought encoder for the model encodes sentences with length less than 30 words using 2400 GRU units with word vector dimensionality of 620 to produce 4800 - dimensional combineskip vectors . .
The combine - skip vectors , with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model , are used as they have been found to be the best performing in the experiments
1 .
The decoder uses greedy decoding tak -
Improving Training and Loss
The training process of a GAN is notably difficult and several improvement techniques such as batch normalization , feature matching , historical averaging and unrolling GAN have been suggested for making the training more stable .
Training the Skip - Thought GAN often results in mode dropping with a parameter setting where it outputs a very narrow distribution of points .
To overcome this , it uses minibatch discrimination by looking at an entire batch of samples and modeling the distance between a given sample and all the other samples present in that batch .
The minimax formulation for an optimal discriminator in a vanilla GAN is Jensen - Shannon Distance between the generated distribution and the real distribution .
GANs can be conditioned on data attributes to generate samples .
In this experiment , both the generator and discriminator are conditioned on Skip - Thought encoded vectors .
The dataset comprises of 70,000 sentences chosen from the BookCorpus dataset , which belong to one series of fantasy novels of a particular author of English language .
This selection implies that the author 's word choice , sentence structure , figurative language , and sentence arrangement are consistent and well - represented across the dataset .
Conditioning on this high - level outline gives more robustness to the model in terms of generated samples .
The encoder converts the dataset with a training / test / validation split of 5/1/1 into vectors to be used as real samples for discriminator .
The decoded sentences are used to evaluate model performance under corpus level BLEU - 2 , BLEU - 3 and BLEU - 4 metrics , once using only test set as reference and then entire corpus as reference .
Table 1 compares these results for different architectures that have been experimented within this paper against baselines of using character level RNNs and LSTMs 2 .
The comparison is also made to RNNs and LSTMs fed with Skip Thought embeddings .
The models generate sentence vectors which are again decoded to produce sentences in english for computing the BLEU metrics .
Language Generation .
Language generation is done on a dataset comprising simple English sentences referred to as CMU - SE 3 in .
The CMU - SE dataset consists of 44,016 sentences with a vocabulary of 3,122 words .
For encoding , the vectors are extracted in batches of sentences having the same length .
The samples represent how mode collapse is manifested when using least - squares distance f- measure without minibatch discrimination .
Table 2 ( a ) contains sentences generated from STGAN using least - squares distance in which there was no mode collapse observed , while 2 ( b ) contains examples wherein it is observed .
Table 2 ( c ) shows generated sentences using gradient penalty regularizer ( GAN - GP ) .
using Wasserstein distance f - measure as WGAN ) and 2 ( e ) contains samples when using a gradient penalty regularizer term as WGAN - GP .
Proposed Oracle Experiment .
The performance of this approach in generating sentences with the writing style of one certain author can be measured through an experiment with the help of a set of people .
This set is familiar with the writing style but has n't read all the works of the author .
This prevents them from being certain whether a sentence in question has or has not occurred in any work of the said author .
The generated samples mixed with real sentences are chosen from a random pool and form the evaluation set given to such an audience who marks if the sentences are fake or not .
This data run through a scoring metric would determine a model 's performance .
An experiment along similar lines is currently being carried out and will be included in the future revision .
Further Work
Another performance metric that can be computed for this setup has been described in which is a parallel work to this .
Simple CFG 4 and more complex ones like Penn Treebank CFG generate samples which are used as input to GAN and the model is evaluated by computing the diversity and accuracy of generated samples conforming to the given CFG .
Skip - Thought sentence embeddings can be used to generate images with GANs conditioned on text vectors for text - to - image conversion tasks like those achieved in .
These embeddings have also been used to Models like neuralstoryteller 5 which use these sentence embeddings can be experimented with generative adversarial networks to generate unique samples .
