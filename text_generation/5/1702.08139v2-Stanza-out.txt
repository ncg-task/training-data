title
Improved Variational Autoencoders for Text Modeling using Dilated Convolutions
abstract
Recent work on generative text modeling has found that variational autoencoders ( VAE ) with LSTM decoders perform worse than simpler LSTM language models ( Bowman et al. , 2015 ) .
This negative result is so far poorly understood , but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder .
In this paper , we experiment with a new type of decoder for VAE : a dilated CNN .
By changing the decoder 's dilation architecture , we control the size of context from previously generated words .
In experiments , we find that there is a trade - off between contextual capacity of the decoder and effective use of encoding information .
We show that when carefully managed , VAEs can outperform LSTM language models .
We demonstrate perplexity gains on two datasets , representing the first positive language modeling result with VAE .
Further , we conduct an in - depth investigation of the use of VAE ( with our new decoding architecture ) for semi-supervised and unsupervised labeling tasks , demonstrating gains over several strong baselines .
Introduction
Generative models play an important role in NLP , both in their use as language models and because of their ability to effectively learn from unlabeled data .
By parameterzing generative models using neural nets , recent work has proposed model classes thatare particularly expressive and can pontentially model a wide range of phenomena in language and other modalities .
We focus on a specific instance 1 Carnegie Mellon University .
Correspondence to : Zichao Yang < zichaoy@cs.cmu.edu >.
Proceedings of the 34 th International Conference on Machine Learning , Sydney , Australia , PMLR 70 , 2017 .
Copyright 2017 by the author ( s ) .
of this class : the variational autoencoder 1 ( VAE ) .
The generative story behind the VAE ( to be described in detail in the next section ) is simple :
First , a continuous latent representation is sampled from a multivariate Gaussian .
Then , an output is sampled from a distribution parameterized by a neural decoder , conditioned on the latent representation .
The latent representation ( treated as a latent variable during training ) is intended to give the model more expressive capacity when compared with simpler neural generative models - for example , conditional language models .
The choice of decoding architecture and final output distribution , which connect the latent representation to output , depends on the kind of data being modeled .
The VAE owes it s name to an accompanying variational technique ) that has been successfully used to train such models on image data .
The application of VAEs to text data has been far less successful .
The obvious choice for decoding architecture for a textual VAE is an LSTM , a typical workhorse in NLP .
However , found that using an LSTM - VAE for text modeling yields higher perplexity on held - out data than using an LSTM language model .
In particular , they observe that the LSTM decoder in VAE does not make effective use of the latent representation during training and , as a result , VAE collapses into a simple language model .
Related work has used simpler decoders that model text as a bag of words .
Their results indicate better use of latent representations , but their decoders can not effectively model longer - range dependencies in text and thus underperform in terms of final perplexity .
Motivated by these observations , we hypothesize that the contextual capacity of the decoder plays an important role in whether VAEs effectively condition on the latent representation when trained on text data .
We propose the use of a dilated CNN as a decoder in VAE , inspired by the recent success of using CNNs for audio , image and language modeling ( van den .
In contrast with prior work where extremely large CNNs are used , we exploit the dilated CNN for its flexibility in varying the amount of conditioning context .
In the two extremes , depending on the choice of dilation , the CNN decoder can reproduce a simple MLP using a bags of words representation of text , or can reproduce the long - range dependence of recurrent architectures ( like an LSTM ) by conditioning on the entire history .
Thus , by choosing a dilated CNN as the decoder , we are able to conduct experiments where we vary contextual capacity , finding a sweet spot where the decoder can accurately model text but does not yet overpower the latent representation .
We demonstrate that when this trade - off is correctly managed , textual VAEs can perform substantially better than simple LSTM language models , a finding consistent with recent image modeling experiments using variational lossy autoencoders .
We goon to show that VAEs with carefully selected CNN decoders can be quite effective for semi-supervised classification and unsupervised clustering , outperforming several strong baselines ( from ) on both text categorization and sentiment analysis .
Our contributions are as follows :
First , we propose the use of a dilated CNN as a new decoder for VAE .
We then empirically evaluate several dilation architectures with different capacities , finding that reduced contextual capacity leads to stronger reliance on latent representations .
By picking a decoder with suitable contextual capacity , we find our VAE performs better than LSTM language models on two data sets .
We also explore the use of dilated CNN VAEs for semi-supervised classification and find they perform better than strong baselines from .
Finally , we verify that the same framework can be used effectively for unsupervised clustering .
Model
In this section , we begin by providing background on the use of variational autoencoders for language modeling .
Then we introduce the dilated CNN architecture that we will use as a new decoder for VAE in experiments .
Finally , we describe the generalization of VAE that we will use to conduct experiments on semi-supervised classification .
Background on Variational Autoencoders
Neural language models typically generate each token x t conditioned on the entire history of previously generated tokens :
State - of - the - art language models often parametrize these conditional probabilities using RNNs , which compute an evolving hidden state over the text which is used to predict each x t .
This approach , though effective in modeling text , does not explicitly model variance in higher - level properties of entire utterances ( e.g. topic or style ) and thus can have difficulty with heterogeneous datasets .
propose a different approach to generative text modeling inspired by related work on vision .
Instead of directly modeling the joint probability p ( x ) as in Equation 1 , we specify a generative process for which p ( x ) is a marginal distribution .
Specifically , we first generate a continuous latent vector representation z from a multivariate Gaussian prior p ? ( z ) , and then generate the text sequence x from a conditional distribution p ? ( x |z ) parameterized using a neural net ( often called the generation model or decoder ) .
Because this model incorporates a latent variable that modulates the entire generation of each whole utterance , it maybe better able to capture high - level sources of variation in the data .
Specifically , in contrast with Equation 1 , this generating distribution conditions on latent vector representation z:
To estimate model parameters ?
we would ideally like to maximize the marginal probability p ? ( x ) = p ? ( z ) p ? ( x |z ) dz .
However , computing this marginal is intractable for many decoder choices .
Thus , the following variational lower bound is often used as an objective :
Here , q ? ( z|x ) is an approximation to the true posterior ( often called the recognition model or encoder ) and is parameterized by ?.
Like the decoder , we have a choice of neural architecture to parameterize the encoder .
However , unlike the decoder , the choice of encoder does not change the model class - it only changes the variational approximation used in training , which is a function of both the model parameters ?
and the approximation parameters ?.
Training seeks to optimize these parameters jointly using stochastic gradient ascent .
A final wrinkle of the training procedure involves a stochastic approximation to the gradients of the variational objective ( which is itself intractable ) .
We omit details here , noting only that the final distribution of the posterior approximation q ? ( z|x ) is typically assumed to be Gaussian so that a re-parametrization trick can be used , and refer readers to .
Training Collapse with Textual VAEs
Together , this combination of generative model and variational inference procedure are often referred to as a variational autoencoder ( VAE ) .
We can also view the VAE as a regularized version of the autoencoder .
Note , however , that while VAEs are valid probabilistic models whose likelihood can be evaluated on held - out data , autoencoders are not valid models .
If only the first term of the VAE variational bound E q ? ( z |x ) [ log p ? ( x |z ) ] is used as an objective , the variance of the posterior probability q ? ( z|x ) will become small and the training procedure reduces to an autoencoder .
It is the KL - divergence term , KL ( q ? ( z|x ) ||p ? ( z ) ) , that discourages the VAE memorizing each x as a single latent point .
While the KL term is critical for training VAEs , historically , instability on text has been evidenced by the KL term becoming vanishingly small during training , as observed by .
When the training procedure collapses in this way , the result is an encoder that has duplicated the Gaussian prior ( instead of a more interesting posterior ) , a decoder that completely ignores the latent variable z , and a learned model that reduces to a simpler language model .
We hypothesize that this collapse condition is related to the contextual capacity of the decoder architecture .
The choice encoder and decoder depends on the type of data .
For images , these are typically MLPs or CNNs .
LSTMs have been used for text , but have resulted in training collapse as discussed above .
Here , we propose to use a dilated CNN as the decoder instead .
In one extreme , when the effective contextual width of a CNN is very large , it resembles the behavior of LSTM .
When the width is very small , it behaves like a bag - ofwords model .
The architectural flexibility of dilated CNNs allows us to change the contextual capacity and conduct experiments to validate our hypothesis : decoder contextual capacity and effective use of encoding information are directly related .
We next describe the details of our decoder .
Dilated Convolutional Decoders
The typical approach to using CNNs used for text generation is similar to that used for images , but with the convolution applied in one dimension .
We take this approach herein defining our decoder .
One dimensional convolution :
For a CNN to serve as a decoder for text , generation of x t must only condition on past tokens x <t .
Applying the traditional convolution will break this assumption and use tokens x ?t as inputs to predict x t .
In our decoder , we avoid this by simply shifting the input by several slots ( van den .
With a convolution with filter size of k and using n layers , our effective filter size ( the number of past tokens to condition to in predicting x t ) would be ( k ? 1 ) n + 1 . Hence , the filter size would grow linearly with the depth of the network .
Dilation : Dilated convolution was introduced to greatly increase the effective receptive field size without increasing the computational cost .
With dilation d , the convolution is applied so that d ?
1 inputs are skipped each step .
Causal convolution can be seen a special case with d = 1 .
With dilation , the effective receptive size grows exponentially with network depth .
In , we show dilation of sizes of 1 and 2 in the first and second layer , respectively .
Suppose the dilation size in the i - th layer is d i and we use the same filter size kin all layers , then the effective filter size is ( k ?
1 ) id i + 1 .
The dilations are typically set to double every layer d i + 1 = 2 d i , so the effective receptive field size can grow exponentially .
Hence , the contextual capacity of a CNN can be controlled across a greater range by manipulating the filter size , dilation size and network depth .
We use this approach in experiments .
Residual connection :
We use residual connection to speedup convergence and enable training of deeper models .
We use a residual block ( shown to the right ) similar to that of .
We use three convolutional layers with filter size 11 , 1 k , 11 , respectively , and ReLU activation be-tween convolutional layers .
Overall architecture :
Our VAE architecture is shown in .
We use LSTM as the encoder to get the posterior probability q ( z | x ) , which we assume to be diagonal Gaussian .
We parametrize the mean and variance ? with LSTM output .
We sample z from q ( z| x ) , the decoder is conditioned on the sample by concatenating z with every word embedding of the decoder input .
Semi-supervised VAE
In addition to conducting language modeling experiments , we will also conduct experiments on semi-supervised classification of text using our proposed decoder .
In this section , we briefly review semi-supervised VAEs of ) that incorporate discrete labels as additional variables .
Given the labeled set ( x , y) ?
D Land the unlabeled set x ? D U , proposed a model whose latent representation contains continuous vector z and discrete label y:
The semi-supervised VAE fits a discriminative network q ( y|x ) , an inference network q ( z|x , y) and a generative network p ( x |y , z ) jointly as part of optimizing a variational lower bound similar that of basic VAE .
For labeled data ( x , y ) , this bound is :
?
KL ( q ( z|x , y ) || p ( z ) ) + log p ( y ) = L ( x , y ) + log p ( y ) .
For unlabeled data x , the label is treated as a latent variable , yielding :
Combining the labeled and unlabeled data terms , we have the over all objective as :
where ?
controls the trade off between generative and discriminative terms .
Gumbel - softmax :
Jang et al. ; propose a continuous approximation to sampling from a categorical distribution .
Let u be a categorical distribution with probabilities ? 1 , ? 2 , ... , ? c .
Samples from u can be approximated using :
where g i follows Gumbel ( 0 , 1 ) .
The approximation is accurate when ? ?
0 and smooth when ? >
0 . In experiments , we use Gumbel - Softmax to approximate the samples from p ( y|x ) to reduce the computational cost .
As a result , we can directly back propagate the gradients of U ( x ) to the discriminator network .
We anneal ?
so that sample variance is small when training starts and then gradually decrease ? .
Unsupervised clustering :
In this section we adapt the same framework for unsupervised clustering .
We directly minimize the objective U ( x ) , which is consisted of two parts : reconstruction loss and KL regularization on q ( y|x ) .
The first part encourages the model to assign x to label y such that the reconstruction loss is low .
We find that the model can easily get stuck in two local optimum : the KL term is very small and q ( y|x ) is close to uniform distribution or the KL term is very large and all samples collapse to one class .
In order to make the model more robust , we modify the KL term by :
That is , we only minimize the KL term when it is large enough .
Experiments
Data sets
Since we would like to investigate VAEs for language modeling and semi-supervised classification , the data sets should be suitable for both purposes .
We use two large scale document classification data sets : Yahoo Answer and Yelp15 review , representing topic classification and sentiment classification data sets respectively ..
We report negative log likelihood ( NLL ) and perplexity ( PPL ) on the test set .
The KL component of NLL is given in parentheses .
Size indicates the effective filter size .
VAE + init indicates pretraining of only the encoder using an LSTM LM .
Model configurations and Training details
We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders .
For CNNs , we explore several different configurations .
We set the convolution filter size to be 3 and gradually increase the depth and dilation from [ 1 , 2 , 4 ] , ] to .
They represent small , medium and large model and we name them as SCNN , MCNN and LCNN .
We also explore a very large model with dilations and name it as VLCNN .
The effective filter size are 15 , 63 , 125 and 187 respectively .
We use the last hidden state of the encoder LSTM and feed it though an MLP to get the mean and variance of q ( z|x ) , from which we sample z and then feed it through an MLP to get the starting state of decoder .
For the LSTM decoder , we follow to use it as the initial state of LSTM and feed it to every step of LSTM .
For the CNN decoder , we concatenate it with the word embedding of every decoder input .
The architecture of the Semi-supervised VAE basically follows that of the VAE .
We feed the last hidden state of the encoder LSTM through a two layer MLP then a softmax to get q ( y|x ) .
We use Gumbel - softmax to sample y from q ( y|x ) .
We then concatenate y with the last hidden state of encoder LSTM and feed them throught an MLP to get the mean and variance of q ( z|y , x ) .
y and z together are used as the starting state of the decoder .
We use a vocabulary size of 20 k for both data sets and set the word embedding dimension to be 512 .
The LSTM dimension is 1024 .
The number of channels for convolutions in CNN decoders is 512 internally and 1024 externally , as shown in Section 2.3 .
We select the dimension of z from .
We find our model is not sensitive to this parameter .
We use Adam to optimize all models and the learning rate is selected from [ 2e - 3 , 1 e - 3 , 7.5 e - 4 ] and ?
1 is selected from [ 0.5 , 0.9 ] .
Empirically , we find learning rate 1e - 3 and ?1 = 0.5 to perform the best .
We select dropout ratio of LSTMs ( both encoder and decoder ) from [ 0.3 , 0.5 ] .
Following , we also use drop word for the LSTM decoder , the drop word ratio is selected from [ 0 , 0.3 , 0.5 , 0.7 ] .
For the CNN decoder , we use a dropout ratio of 0.1 at each layer .
We do not use drop word for CNN decoders .
We use batch size of 32 and all model are trained for 40 epochs .
We start to half the learning rate every 2 epochs after epoch 30 .
Following , we use KL cost annealing strategy .
We set the initial weight of KL cost term to be 0.01 and increase it linearly until a given iteration T .
We treat T as a hyper parameter and select it from [ 10 k , 40 k , 80 k ] .
Language modeling results
The results for language modeling are shown in .
We report the negative log likelihood ( NLL ) and perplexity ( PPL ) of the test set .
For the NLL of VAEs , we decompose it into reconstruction loss and KL divergence and report the KL divergence in the parenthesis .
To better visualize these results , we plot the results of Yahoo data set ( We can see that LSTM - VAE is worse than LSTM - LM in terms of NLL and the KL term is nearly zero , which verifies the finding of .
When we use CNNs as the decoders for VAEs , we can see improvement over pure CNN LMs .
For SCNN , MCNN and LCNN , the VAE results improve over LM results from 345.3 to 337.8 , 338.3 to 336.2 , and 335.4 to 333.9 respectively .
The improvement is big for small models and gradually decreases as we increase the decoder model contextual capacity .
When the model is as large as VLCNN , the improvement diminishes and the VAE result is almost the same with LM result .
This is also reflected in the KL term , SCNN - VAE has the largest KL of 13.3 and VLCNN - VAE has the smallest KL of 0.7 .
When LCNN is used as the decoder , we obtain an optimal trade off between using contextual information and latent representation .
LCNN - VAE achieves a NLL of 333.9 , which improves over LSTM - LM with NLL of 334.9 .
We find that if we initialize the parameters of LSTM encoder with parameters of LSTM language model , we can improve the VAE results further .
This indicates better encoder model is also a key factor for VAEs to work well .
Combined with encoder initialization , LCNN - VAE improves over LSTM - LM from 334.9 to 332.1 in NLL and from 66.2 to 63.9 in PPL .
Similar results for the sentiment data set are shown in .
LCNN - VAE improves over LSTM - LM from 362.7 to 359.1 in NLL and from 42.6 to 41.1 in PPL .
Latent representation visualization :
In order to visualize the latent representation , we set the dimension of z to be 2 and plot the mean of posterior probability q ( z | x ) , as shown in .
We can see distinct different characteristics of topic and sentiment representation .
In , we can see that documents of different topics fall into different clusters , while in , documents of different ratings form a continuum , they lie continuously on the xaxis as the review rating increases . :
Semi-supervised VAE ablation results on Yahoo .
We report both the NLL and classification accuracy of the test data .
Accuracy is in percentage .
Number of labeled samples is fixed to be 500 .
Semi-supervised VAE results
Motivated by the success of VAEs for language modeling , we continue to explore VAEs for semi-supervised learning .
Following that of , we set the number of labeled samples to be 100 , 500 , 1000 and 2000 respectively .
Ablation Study :
At first , we would like to explore the effect of different decoders for semi-supervised classification .
We fix the number of labeled samples to be 500 and report both classification accuracy and NLL of the test set of Yahoo data set in 5 .
We can see that SCNN - VAE - Semi has the best classification accuracy of 65.5 .
The accuracy decreases as we gradually increase the decoder contextual capacity .
On the other hand , LCNN - VAE - Semi has the best NLL result .
This classification accuracy and NLL trade off once again verifies our conjecture : with small contextual window size , the decoder is forced to use the encoder information , hence the latent representation is better , they denotes the LSTM is initialized with a sequence autoencoder and a language model .
learned .
Comparing the NLL results of Comparison with related methods :
We compare Semisupervised VAE with the methods from , which represent the previous state - of - the - art for semisupervised sequence learning .
pre-trains a classifier by initializing the parameters of a classifier with that of a language model or a sequence autoencoder .
They find it improves the classification accuracy significantly .
Since SCNN - VAE - Semi performs the best according to Table 5 , we fix the decoder to be SCNN in this part .
The detailed comparison is in .
We can see that semisupervised VAE performs better than LM - LSTM and LA - LSTM from .
We also initialize the encoder of the VAE with parameters from LM and find classification accuracy further improves .
We also see the advantage of SCNN - VAE - Semi over LM - LSTM is greater when the number of labeled samples is smaller .
The advantage decreases as we increase the number of labeled samples .
When we set the number of labeled samples to be 25 k , the SCNN - VAE - Semi achieves an accuracy of 70.4 , which is similar to LM - LSTM with an accuracy of 70.5 .
Also , SCNN - VAE - Semi performs better on Yahoo data set than Yelp data set .
For Yelp , SCNN - VAE - Semi is a little bit worse than LM - LSTM if the number of labeled samples is greater than 100 , but becomes better when we initialize the encoder .
explains this observation .
It shows the documents are coupled together and are harder to classify .
Also , the latent representation contains information other than sentiment , which may not be useful for classification .
Unsupervised clustering results
We also explored using the same framework for unsupervised clustering .
We compare with the baselines that ex - tract the feature with existing models and then run Gaussian Mixture Model ( GMM ) on these features .
We find empirically that simply using the features does not perform well since the features are high dimensional .
We run a PCA on these features , the dimension of PCA is selected from .
Since GMM can easily get stuck in poor local optimum , we run each model ten times and report the best result .
We find directly optimizing U ( x ) does not perform well for unsupervised clustering and we need to initialize the encoder with LSTM language model .
The model only works well for Yahoo data set .
This is potentially because shows that sentiment latent representations does not fall into clusters .
?
in Equation 5 is a sensitive parameter , we select it from the range between 0.5 and 1.5 with an interval of 0.1 .
We use the following evaluation protocol : after we finish training , for cluster i , we find out the validation sample x n from cluster i that has the best q ( y i | x ) and assign the label of x n to all samples in cluster i .
We then compute the test accuracy based on this assignment .
The detailed results are in .
We can see SCNN - VAE - Unsup + init performs better than other baselines .
LSTM+ GMM performs very bad probably because the feature dimension is 1024 and is too high for GMM , even though we already used PCA to reduce the dimension .
Conditional text generation
With the semi-supervised VAE , we are able to generate text conditional on the label .
Due to space limitation , we only show one example of 1 star the food was good but the service was horrible .
took forever to get our food .
we had to ask twice for our check after we got our food .
will not return .
2 star the food was good , but the service was terrible .
took forever to get someone to take our drink order .
had to ask 3 times to get the check .
food was ok , nothing to write about .
3 star came here for the first time last night .
food was good .
service was a little slow .
food was just ok .
4 star food was good , service was a little slow , but the food was pretty good .
i had the grilled chicken sandwich and it was really good .
will definitely be back !
5 star food was very good , service was fast and friendly .
food was very good as well .
will be back !:
Text generated by conditioning on sentiment label .
generated reviews conditioning on review rating in .
For each group of generated text , we fix z and vary the label y , while picking x via beam search with a beam size of 10 .
Related work
Variational inference via the re-parameterization trick was initially proposed by and since then , VAE has been widely adopted as generative model for images .
Our work is inline with previous works on combining variational inferences with text modeling .
is the first work to combine VAE with language model and they use LSTM as the decoder and find some negative results .
On the other hand , ) models text as bag of words , though improvement has been found , the model can not be used to generate text .
Our work fills the gaps between them .
applies variational inference to dialogue modeling and machine translation and found some improvement in terms of generated text quality , but no language modeling results are reported .
embedded variational units in every step of a RNN , which is different from our model in using global latent variables to learn high level features .
Our use of CNN as decoder is inspired by recent success of PixelCNN model for images , WaveNet for audios ( van den Oord et al. , 2016 a ) , Video Pixel Network for video modeling and ByteNet for machine translation .
But in contrast to those works showing using a very deep architecture leads to better performance , CNN as decoder is used in our model to control the contextual capacity , leading to better performance .
Our work is closed related the recently proposed variational lossy autoencoder which is used to pre-dict image pixels .
They find that conditioning on a smaller window of a pixels leads to better results with VAE , which is similar to our finding .
Much has been done to come up more powerful prior / posterior distribution representations with techniques such as normalizing flows .
We treat this as one of our future works .
This work is largely orthogonal and could be potentially combined with a more effective choice of decoder to yield additional gains .
There is much previous work exploring unsupervised sentence encodings , for example skip - thought vectors , paragraph vectors , and sequence autoencoders .
applies a pretrained model to semi-supervised classification and find significant gains , we use this as the baseline for our semi-supervised VAE .
Conclusion
We showed that by controlling the decoder 's contextual capacity in VAE , we can improve performance on both language modeling and semi-supervised classification tasks by preventing a degenerate collapse of the training procedure .
These results indicate that more carefully characterizing decoder capacity and understanding how it relates to common variational training procedures may represent important avenues for unlocking future unsupervised problems .
