102	6	12	remove
102	17	34	gated - attention
102	41	51	accuracies
102	52	56	drop
102	57	59	to
102	60	77	72.8 % and 73.6 %
103	13	40	charactercomposition vector
103	47	57	accuracies
103	58	62	drop
103	63	65	to
103	66	83	72.9 % and 73.5 %
104	13	35	word - level embedding
104	42	52	accuracies
104	53	57	drop
104	58	60	to
104	61	78	65.6 % and 66.0 %
82	55	94	https : //github.com/lukecq1231/enc_nli
84	3	6	use
84	11	40	Adam ( Kingma and Ba , 2014 )
84	41	44	for
84	45	57	optimization
87	7	45	pretrained GloVe - 840B - 300D vectors
87	46	48	as
87	49	76	our word - level embeddings
87	81	84	fix
87	102	108	during
87	113	129	training process
85	0	14	Stacked BiLSTM
85	19	27	3 layers
85	34	51	all hidden states
85	52	54	of
85	55	70	BiLSTMs and MLP
85	71	75	have
85	76	90	600 dimensions
86	103	117	100 dimensions
86	4	23	character embedding
86	28	41	15 dimensions
86	48	66	CNN filters length
86	67	69	is
86	70	83	[ 1 , 3 , 5 ]
88	0	35	Out - of - vocabulary ( OOV ) words
88	40	60	initialized randomly
88	61	65	with
88	66	82	Gaussian samples
29	20	28	proposed
29	29	64	natural language inference networks
29	75	86	composed of
29	120	134	word embedding
29	137	153	sequence encoder
29	156	173	composition layer
29	184	203	toplayer classifier
31	0	14	Word Embedding
33	3	14	concatenate
33	15	25	embeddings
33	26	36	learned at
33	37	57	two different levels
33	103	124	character composition
33	129	160	holistic word - level embedding
33	58	70	to represent
33	71	80	each word
33	81	83	in
33	88	96	sentence
40	0	16	Sequence Encoder
41	67	81	sentence pairs
41	86	94	fed into
41	95	112	sentence encoders
41	113	122	to obtain
41	123	153	hidden vectors ( h p and h h )
42	3	6	use
42	7	48	stacked bidirectional LSTMs ( BiL - STM )
42	49	51	as
42	56	64	encoders
53	0	17	Composition Layer
54	118	125	compose
54	130	144	hidden vectors
54	145	156	obtained by
54	161	199	sequence encoder layer ( h p and h h )
54	0	12	To transform
54	13	22	sentences
54	23	27	into
54	28	65	fixed - length vector representations
55	3	10	propose
55	11	43	intra-sentence gated - attention
55	44	53	to obtain
55	56	77	fixed - length vector
67	0	22	Top - layer Classifier
68	0	19	Our inference model
68	20	25	feeds
68	30	47	resulting vectors
68	63	65	to
68	70	86	final classifier
68	87	99	to determine
68	104	135	over all inference relationship
2	75	101	Natural Language Inference
14	85	119	natural language inference ( NLI )
16	15	18	NLI
95	26	52	our implementation of ESIM
95	61	69	achieves
95	73	81	accuracy
95	82	84	of
95	85	91	76.8 %
95	92	94	in
95	99	119	in - domain test set
95	126	132	75.8 %
95	133	135	in
95	140	163	cross - domain test set
95	172	180	presents
95	185	215	state - of - the - art results
96	90	97	achieve
96	98	108	accuracies
96	109	111	of
96	112	129	73.5 % and 73.6 %
96	0	14	After removing
96	19	45	cross - sentence attention
96	50	56	adding
96	57	84	our gated - attention model
96	226	231	among
96	236	249	single models
96	138	143	ranks
96	144	149	first
96	150	152	in
96	157	180	cross - domain test set
96	191	197	second
96	198	200	in
96	205	225	in - domain test set
97	0	4	When
97	5	15	ensembling
97	16	26	our models
97	32	38	obtain
97	39	49	accuracies
97	50	67	74.9 % and 74.9 %
97	76	81	ranks
97	82	90	first in
97	91	105	both test sets
