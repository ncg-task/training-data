title
aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching Model
abstract
As an alternative to question answering methods based on feature engineering, deep learning approaches such as convolutional neural networks (CNNs) and Long Short-Term Memory Models (LSTMs) have recently been proposed for semantic matching of questions and answers. To achieve good results, however, these models have been combined with additional features such as word overlap or BM25 scores. Without this combination, these models perform significantly worse than methods based on linguistic feature engineering. In this paper, we propose an attention based neural matching model for ranking short answer text. We adopt value-shared weighting scheme instead of position-shared weighting scheme for combining different matching signals and incorporate question term importance learning using question attention network. Using the popular benchmark TREC QA data, we show that the relatively simple aNMM model can significantly outperform other neural network models that have been used for the question answering task, and is competitive with models thatare combined with additional features. When aNMM is combined with additional features, it outperforms all baselines.
INTRODUCTION
Question answering (QA), which returns exact answers as either short facts or long passages to natural language questions issued by users, is a challenging task and plays a central role in the next generation of advanced web search. Many of current QA systems use a learning to rank approach that encodes question/answer pairs with complex linguistic features including lexical, syntactic and semantic features. For instance, Surdeanu et al. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. 23] investigated a wide range of feature types including similarity features, translation features, density/frequency features and web correlation features for learning to rank answers and show improvements in accuracy. However, such methods rely on manual feature engineering, which is often time-consuming and requires domain dependent expertise and experience. Moreover, they may need additional NLP parsers or external knowledge sources that may not be available for some languages.
Recently, researchers have been studying deep learning approaches to automatically learn semantic match between questions and answers. Such methods are built on the top of neural network models such as convolutional neural networks (CNNs) and Long Short-Term Memory Models (LSTMs). The proposed models have the benefit of not requiring hand-crafted linguistic features and external resources. Some of them achieve state-ofthe art performance for the answer sentence selection task benchmarked by the TREC QA track. However, the weakness of the existing studies is that the proposed deep models, either based on CNNs or LSTMs, need to be combined with additional features such as word overlap features and BM25 to perform well. Without combining these additional features, their performance is significantly worse than the results obtained by the state-of-the-art methods based on linguistic feature engineering. This led us to propose the following research questions:
RQ1 Without combining additional features, could we build deep learning models that can achieve comparable or even better performance than methods using feature engineering ?
RQ2 By combining additional features, could our model outperform state-of-the-art models for question answering ?
To address these research questions, we analyze the existing current deep learning architectures for answer ranking and make the following two key observations:
1. Architectures not specifically designed for question/answer matching: Some methods employ CNNs for question/answer matching. However, CNNs are originally designed for computer vision (CV), which uses position-shared weights with local perceptive filters, to learn spatial regularities in many CV tasks. However, such spatial regularities may not exist in semantic matching between questions and answers, since important similarity signals between question and answer terms could appear in any position due to the complex linguistic property of natural languages. Meanwhile, models based on LSTMs view the question/answer matching problem in a sequential way. Without direct interactions between question and answer terms, the model may not be able to capture sufficiently detailed matching signals between them.

2.
Lack of modeling question focus: Understanding the focus of questions, e.g., important terms in a question, is helpful for ranking the answers correctly . For example, given a question like "Where was the first burger king restaurant opened ?", it is critical for the answer to talk about "burger", "king", "open", etc. Most existing text matching models do not explicitly model question focus. For example, models based on CNNs treat all the question terms as equally important when matching to answer terms. Models based on LSTMs usually model question terms closer to the end to be more important.
To handle these issues in the existing deep learning architectures for ranking answers, we propose an attention based neural matching model (aNMM). The novel properties of the proposed model and our contributions can be summarized as follows:
1. Deep neural network with value-shared weights: We introduce a novel value-shared weighting scheme in deep neural networks as a counterpart of the position-shared weighting scheme in CNNs, based on the idea that semantic matching between a question and answer is mainly about the (semantic similarity) value regularities rather than spatial regularities.

Incorporate attention scheme over question terms:
We incorporate the attention scheme over the question terms using a gating function, so that we can explicitly discriminate the question term importance.

Extensive experimental evaluation and promising results.
We perform a thorough experimental study based on the TREC QA dataset from TREC QA tracks 8-13, which appears to be one of the most widely used benchmarks for answer reranking. Unlike previous methods using CNNs and LSTMs, which showed inferior results without combining additional features, our model can achieve better performance than a state-of-art method using linguistic feature engineering and comparable performance with previous deep learning models with combined additional features. If we combine our model with a simple additional feature like QL, our method can achieve the state-of-the-art performance among current existing methods for ranking answers under multiple metrics.
Roadmap. The rest of our paper is organized as follows. We will review related work in Section 2. In Section 3, we will present the proposed aNMM model with two components: value-shared weights and question attention network with gating functions. Two different architectures will be presented and analyzed. Section 4 is a systematic experimental analysis using the TREC QA benchmark dataset. Finally, we conclude our paper and discuss future work in Section 5.

RELATED WORK
Our work is related to several research are as, including deep learning models for text matching, factoid question answering, answer ranking in CQA and answer passage / sentence retrieval.
Deep Learning Models for Text Matching. Recently there have been many deep learning models proposed for text matching and ranking. Such deep learning models include DSSM, CDSSM, ARC-I/ARC-II , DCNN, DeepMatch, MultiGranCNN and MatchPyramid. DSSM performs a non-linear projection to map the query and the documents to a common semantic space. The neural network models are trained using clickthrough data such that the conditional likelihood of the clicked document given the query is maximized. DeepMatch uses a topic model to construct the interactions between two texts and then makes different levels of abstractions with a deep architecture to model the relationships between topics. ARC-I and ARC-II are two different architectures proposed by Hu et. al. for matching natural language sentences. ARC-I firstly finds the representation of each sentence and then compares the representations of the two sentences with a multi-layer perceptron (MLP). The drawback of ARC-I is that it defers the interaction between two sentences until their individual representation matures in the convolution model, and therefore has the risk of losing details, which could be important for the matching task. On the other hand, ARC-II is built directly on the interaction space between two sentences. Thus ARC-II makes two sentences meet before their own high-level representations mature, while still retaining the space for individual development of abstraction of each sentence. Our aNMM architecture adopts a similar design with ARC-II in the QA matching matrix where we build neural networks directly on the interaction of sentence term pairs. However, we adopt value-shared weights instead of position-shared weights as in the CNN used by ARC-II. We also add attention scheme to learn question term importance.
Factoid Question Answering. There have been many previous studies on factoid question answering, most of which use the benchmark data from TREC QA track. Yih et. al. formulated answer sentence selection as a semantic matching problem with a latent word-alignment structure and conducted a series of experimental studies on leveraging proposed lexical semantic models. Iyyer et. al. introduced a recursive neural network (RNN) model that can reason over text that contains very few individual words by modeling textual compositionality. Yu et al. proposed an approach for answer sentence selection via distributed representations, and learned to match questions with answers by considering their semantic encoding. They combined the learning results of their model with word overlap features by training a logistic regression classifier. Wang and Nyberg proposed a method which uses a stacked bidirectional Long-Short Term Memory (BLSTM) network to sequentially read words from question and answer sentences, and then output their relevance scores. Their system needs to combine the stacked BLSTM relevance model with a BM25 score to achieve good performance. Severyn and Moschitti presented a convolutional neural network architecture for re-ranking pairs of short texts, where they learned the optimal representation of text pairs and a similarity function to relate them in a supervised way from the available training data. They also need to combine additional features into their model to outperform previous methods. Unlike the previous research, our method can outperform previous methods using feature engineering without combining any additional features. With an additional simple feature like QL, our model is significantly better than the previous state-of-the-art methods including deep learning methods.
Answer Ranking in CQA. There is also previous research on ranking answers from community question answering (CQA) sites. Surdeanu et al. investigated a wide range of feature types such as similarity features, translation features, density / frequency features for ranking answers to non-factoid questions in Yahoo! Answers. Jansen et al. presented an answer re-ranking model for non-factoid questions that integrate lexical semantics with discourse information driven by two representations of discourse. Xue et al. proposed a retrieval model that combines a translationbased language model for the question part with a query likelihood approach for the answer part. Questions from CQA sites are mostly non-factoid questions. Our research is closer to factoid questions such as questions in TREC QA data.  Answer Passage / Sentence Retrieval. Our work is also related to previous research on answer passage / sentence retrieval. Tymoshenko and Moschitti studied the use of syntactic and semantic structures obtained with shallow and deeper syntactic parsers in the answer passage re-ranking task. Keikha et al. developed an annotated data set for non-factoid answer finding using TREC GOV2 collections and topics. They annotated passage-level answers, revisited several passage retrieval models with this data, and came to the conclusion that the current methods are not effective for this task. Yang et al. developed effective methods for answer sentence retrieval using this annotated data by combining semantic features, context features and basic text matching features with a learning to rank approach. Our model is built on attentionbased neural matching model with value-shared weighting schema. Unlike learning to rank approaches with feature engineering, our model can achieve good performance for ranking answers without any additional manual feature engineering, preprocessing of NLP parsers and external resources like knowledge bases.

Word

ATTENTION-BASED NEURAL MATCH-ING MODEL
In this section we present the proposed model referred as aNMM (attention-based Neural Matching Model), which is shown in 1. Before we introduce our model, we firstly define some terminologies.

Terminology
Short Answer Text: We use Short Answer Text to refer to a short fact, answer sentences or answer passages that can address users' information needs in the issued questions. This is the ranking object in this paper and includes answers in various lengths. In the experiments of this paper, we mainly focus on ranking answer sentences that contain correct answer facts as in TREC QA data. QA Matching Matrix: We use QA Matching Matrix to refer to a matrix which represents the semantic matching information of term pairs from a question and answer pair. Given a question q with length M and an answer a with length N , a QA matching matrix is an M by N matrix P, where Pj,i denote the semantic similarity between term qj and term ai measured by the cosine similarity of the corresponding word embeddings of terms. If qj and ai are the same term, we assign Pj,i as 1. QA Matching Vector: We use QA Matching Vector to refer to a row in the QA matching matrix. As presented before, the j-th row of the QA matching matrix P contains the semantic similarity between qj and all terms in answer a .

Model Overview
Our method contains three steps as follows:
1. We construct QA matching matrix for each question and answer pair with pre-trained word embeddings.
2. We then employ a deep neural network with value-shared weighting scheme in the first layer, and fully connected layers in the rest to learn hierarchical abstraction of the semantic matching between question and answer terms.
3. Finally, we employ a question attention network to learn question term importance and produce the final ranking score.
We propose two neural matching model architectures and compare the effectivenesses of them. We firstly describe a basic version of the architecture, which is referred to as aNMM-1.
In the following sections, we will explain in detail the two major designs of aNMM-1, i.e., value-shared weights and question attention network.

Value-shared Weighting
We first train word embeddings with the Word2Vec tool by Mikolov et al. with the English Wikipedia dump to construct QA matching matrices. Given a question sentence and an answer sentence, we compute the dot product of the normalized word embeddings of all term pairs to construct the QA matching matrix P as defined in Section 3.1. A major problem with the QA matching matrix is the variable size due to the different lengths of answers for a given question. To solve this problem, one can use CNN with: The comparison of position-shared weight in CNN and value-shared weight in aNMM. In CNN, the weight associated with anode only depends on its position or relative location as specified by the filters. In aNMM, the weight associated with anode depends on its value. pooling strategy to handle the variable size. However, as we have mentioned before, CNNs basically use position-shared weighting scheme which may not fit semantic matching between questions and answers. Important question terms and semantically similar answer words could appear anywhere in questions/answers due to the complex linguistic property of natural languages. Thus we adopt the following method to handle the various length problem:
Value-shared Weights: For this method, the assumption is that matching signals in different ranges play different roles in deciding the final ranking score. Thus we introduce the value-shared weighting scheme to learn the importance of different levels of matching signals. The comparison between the position-shared weight and value-shared weight is shown in. We can see that for position-shared weights, the weight associated with anode only depends on its position or relative location as specified by the filters in CNN. However in our model, the weight associated with anode depends on its value. The value of anode denotes the strength of the matching signal between term pairs of questions and answers from the QA matching matrix, as explained in Section 3.1. Such a setting enables us to use the learned weights to encode how to combine different levels of matching signals. After this step, the size of the hidden representation becomes fixed and we can use normal fully connected layers to learn higher level representations. We use the term bin to denote a specific range of matching signals. since Pj,i ? [?1, 1], if we set the size of bins as 0.1, then we have 21 bins where there is a separate bin for Pj,i = 1 to denote exact match of terms.
Specifically, value-shared weights are adopted in the forward propagation prediction process from the input layer to the hidden layer over each question term in aNMM-1 as follows:
Input Layer to Hidden Layer. Let w denote a K + 1 dimensional model parameter from input layer to hidden layer. x jk denotes the sum of all matching signals within the k-th value range or bin. For each QA matching vector of a given query q, the combined score after the activation function of the j-th node in hidden layer is defined as
where j is the index of question words in q. We use the sigmoid function as the activation function, which is commonly adopted in many neural network architectures.

Question Attention Network
In addition to value-shared weighting, another model component of aNMM-1 is the question attention network. In a committee of neural networks which consists of multiple networks, we need to combine the output of these networks to output a final decision vector. The question attention network uses the gating function to control the output of each network in this process. Specifically, in aNMM-1 we use the softmax gate function to combine the output of multiple networks where each network corresponds to a question term as shown in. We feed the dot product of query word embedding and model parameter to the softmax function to represent the query term importance. In this setting, we can directly compare the relative term importance of query words within the same query with softmax function. We also tried sigmoid gate function, but this did not perform as well as softmax gate function.
Softmax gate function is used in the forward propagation process from the hidden layer to the output layer as follows:
Hidden Layer to Output Layer. From the hidden layer to the output layer, we add a softmax gate function to learn question attention. Let v denote a P dimensional vector which is a model parameter. We feed the dot product of query word embedding qj and v to the softmax function to represent the query term importance as shown in Equation 2. Note that we normalize the query word embedding before computing the dot product.
Unlike previous models like CNNs and BLSTM, which learn the semantic match score between questions and answers through representation learning from matching matrix or question / answer pair sequences, aNMM achieves this by combining semantic matching signals of term pairs in questions and answers weighted by the output of question attention network, where softmax gate functions help discriminate the term importance or attention on different question terms.

Model Training
For aNMM-1, the model parameters contain two sets: 1) The value-shared weights wk for combining matching signals from the input layer to the hidden layer. 2) The parameters vp in the gating function from the hidden layer to the output layer.
To learn the model parameters from the training data, we adopt a pair-wise learning strategy with a large margin objective. Firstly we construct triples (q, a + , a ? ) from the training data, with q matched with a + better than with a ? . We have the ranking-based loss as the objective function as following:
where S(q, a) denote the predicted matching score for QA pair (q, a). During training stage, we will scan all the triples in training data. Given a triple (q, a + , a ? ), we will compute ?S = 1 ? S(q, a + ) + S(q, a ? ). If ?S ? 0, we will skip this triple. Otherwise, we need to update model parameters with back propagation algorithm to minimize the objective function.
Under softmax gate function setting, the gradients of e w.r.t. v from hidden layer to the output layer is shown in Equation 4
where
The gradient of e w.r.t. w from input layer to hidden layer is shown in.
With the formulas of gradients, we can perform stochastic gradient descent to learn model parameters. We use mini-batch gradient descent to achieve more robust performance on the ranking task. For the learning rate, we adopt adaptive learning rate: ? = ?0(1 ? ), where will approach 1 with more iterations. Such a setting has better guarantee for convergence.

Extension to Deep Neural Networks with Multiple Sets of Value-shared Weights
In aNMM-1, we can only use one set of value-shared weights for each QA matching vector. We further propose a more flexible neural network architecture which could enable us to use multiple sets of value-shared weights for each QA matching vector, leading to multiple intermediate nodes in the first hidden layer, as shown in by the yellow color. We refer to this extended model as aNMM-2. The model architecture shown in is corresponding to aNMM-2.

Forward Propagation Prediction
For aNMM-2, we add a hidden layer in the neural network where we learn multiple combined scores from the input layer. With this hidden layer, we define multiple weight vectors as w. Thus w becomes a two dimensional matrix. The formula for the forward propagation prediction is as follows:
where ? (v ? qj) = exp(v?q j ) L l=1 exp(v?q l ) and ? denote the softmax gate function. T is the number of nodes in hidden layer 1. rt is the model parameter from hidden layer 1 to hidden layer 2, where we feed the linear combination of outputs of nodes in hidden layer 1 to an extra activation function comparing with Equation 2. Then from hidden layer 2 to output layer, we sum over all outputs of nodes in hidden layer 2 weighted by the outputs of softmax gate functions, which also form the question attention network.

Back Propagation for Model Training
For aNMM-2, we have three sets of model parameters: 1) w kt from input layer to hidden layer 1; 2) rt from hidden layer 1 to hidden layer 2; 3) vp from hidden layer 2 to output layer. All three sets of parameters are updated through back propagation. The definition of the objective function is the same as Equation. The back propagation process for model parameter learning is described as follows:
From hidden layer 2 to output layer. The gradients of the objective function w.r.t. v is as following:
Where
) From hidden layer 1 to hidden layer 2. The gradients of the objective function w.r.t. r is as following:
. From input layer to hidden layer 1. The gradients of the objective function w.r.t. w is as following:
Where
Initially we will randomly give the values of model parameters. Then we will use back propagation to update the model parameters. When the learning process converge, we use the learned model parameters for prediction to rank short answer texts.

EXPERIMENTS

Data Set and Experiment Settings
We use the TREC QA data set 1 created by Wang et. al. from TREC QA track 8-13 data, with candidate answers automatically selected from each question's document pool using a combination of overlapping non-stop word counts and pattern matching. This data set is one of the most widely used benchmarks for answer with the English Wikipedia dump. We use the skip-gram model with window size 5 and filter words with frequency less than 5 following the common practice in many neural embedding models. For the word vector dimension, we tune it as a hyper-parameter on the validation data starting from 200 to 1000. Embeddings for words not present are randomly initialized with sampled numbers from uniform distribution U[-0.25,0.25], which follows the same setting as.
Model Hyper-parameters. For the setting of hyper-parameters, we set the number of bins as 600, word embedding dimension as 700 for aNNM-1, the number of bins as 200, word embedding dimension as 700 for aNNM-2 after we tune hyper-parameters on the provided DEV set of TREC QA data.

Evaluation and Metrics
For evaluation, we rank answer sentences with the predicted score of each method and compare the rank list with the ground truth to compute metrics. We choose Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), which are commonly used in information retrieval and question answering, as the metric to evaluate our model.
The definition of MRR is as follows:
where rank(f a) is the position of the first correct answer in the rank list for the question q. Thus MRR is only based on the rank of the first correct answer. It is more suitable for the cases where the rank of the first correct answer is emphasized or each question only have one correct answer. On the other hand, MAP looks at the ranks of all correct answers. It is computed as following:
is the average precision for each query q ? Q. Thus MAP is the average performance on all correct answers. We use the official trec_eval 2 scripts for computing these metrics.

Model Learning Results
In this section, we give some qualitative analysis and visualization of our model learning results. Specifically, we analyze the learned value-shard weights and question term importance by aNMM.

Value-shared Weight
We take the learned value-shared weights of aNMM-1 as the example. shows the learned value-shared weights by aNMM-1. In aNMM-1, for each QA matching vector, there is only one bin node. Thus the learned value-shared weights for aNMM-1 is a one dimension vector. For aNMM-1, we set the number of bins as 600 as presented in Section 4.1. Note that the x-axis is the index of bin range and the y-axis is the value-shared weights corresponding to each bin range. The range of match signals is [-1,1] from the left to the right. We make the following observations: (1) The exact match signal which is corresponding to 1 in the last bin is tied with a very large weight, which shows that exact match information is very important. (2) For positive matching score from (0, 1), which is corresponding to bin index (300, 600), the learned value-shared weights are different for matching score range (0.5, 1) (bin index (450, 600)) and matching score range (0, 0.5) (bin index (300, 450)) . We can observe many positive value-shared weights for matching score range(0.5, 1) and negative value-shared weights for matching score range(0, 0.5). This makes sense since high semantic matching scores are positive indicators on answer correctness, whereas low semantic matching scores indicate that the candidate answer sentences contain irrelevant terms. (3) For negative   matching scores from (?1, 0), we can see there is not a lot of differences between value-shared weights for different ranges. A major reason is that most similarity scores based on word embeddings are positive. Therefore, we can remove bins corresponding to negative matching scores to reduce the dimension of value-shared weight vectors, which can help improve the efficiency of the model training process. We will show more quantitative results on the comparison between value-shared weights and position-shared weights in CNN in Section 4.4.

Question Term Importance
Next we analyze the learned question term importance of our model. Due to the space limit, we also use the learned question term importance of aNMM-1 as an example. shows the examples of learned question term importance by aNMM-1. We also visualize the question term importance in. Based on the results in the table and the figure, we can clearly see that aNMM-1 learns reasonable term importance. For instance, with the question attention network, aNMM-1 discovers important terms like "khmer", "rouge", "power" as for the question "When did the khmer rouge come into power ?". Terms like "age", "rossinin", "stop", "writing","opera" are highlighted for the question "At what age did rossini stop writing opera ? ". For the question "Where was the first burger king restaurant opened ?" mentioned in Section 1, "burger", "king", "opened" are treated as important question terms.
An interesting question is how the learned term importance compare with traditional IR term weighting methods such as IDF. We design an experiment to compare aNMM-1/aNMM-2 with aNMM-IDF, which is a degenerate version of our model where we use IDF to directly replace the output of question attention network. In this case, ? (v ? qj) in Equation 6 is replaced by the IDF of the j-th question term. shows the results. We find that if we replace the output of question attention network of aNMM with IDF, it will decrease the answer ranking performance, especially on TRAIN data. Thus, we can see that with the optimization process in the back propagation training process, aNMM can learn better question term weighting score than heuristic term weighting functions like IDF.

Experimental Results for Ranking Answers

Learning without Combining Additional Features
Our first experimental setting is ranking answer sentences directly by the predicted score from aNMM without combining any additional features. This will enable us to answer RQ1 proposed in Section 1. shows the results of TREC QA on TRAIN and TRAIN-ALL without combining additional features. In this table, we compare the results of aNMM with other previous deep learning methods including CNN and LSTM. We summarize our observations as follows: (1) Both aNMM-1 and aNMM-2 show significant improvements for MAP and MRR on TRAIN and TRAIN-ALL data sets comparing with previous deep learning methods. Specifically, if we compare the results of aNMM-1 with the strongest deep learning baseline method by Severyn et al. based on CNN, we can see aNMM-1 outperform CNN for 14.67% in MAP on TRAIN, 9.15% in MAP on TRAIN-ALL. For MRR, we can also observe similar significant improvements of aNMM-1. These results show that with the value-shared weight scheme instead of the position-shared weight scheme in CNN and term importance learning with question attention network, aNMM can predict ranking scores with much higher accuracy comparing with previous deep learning models for ranking answers. (2) If we compare the results of aNMM-1 and aNMM-2, we can see their results are very close. aNMM-1 has slightly better performance than aNMM-2. This result indicates that adding one more hidden layer to incorporate multiple bin nodes does not necessarily increase the performance for answer ranking in TREC QA data. From the perspective of model efficiency, aNMM-1 could be a better choice since it can 0.6029 0.6852 Heilman and Smith (2010) 0.6091 0.6917 0.5951 0.6951 0.6307 0.7477 0.6781 0.7358 0.7092 0.7700 aNMM-2 0.7407 0.7969 aNMM-1 0.7385 0.7995
be trained much faster with good prediction accuracy. However, for larger training data sets than TREC QA data, aNMM-2 could have better performance since it has more model parameters and is suitable for fitting larger training data set. We leave the study of impact of the number of hidden layers in aNMM to future work. shows the comparison between aNMM with previous methods using feature engineering on TRAIN-ALL without combining additional features. We find that both aNMM-1 and aNMM-2 achieve better performance comparing with other methods using feature engineering. Specifically, comparing the results of aNMM-1 with the strongest baseline by Yih et al. based on enhanced lexical semantic models, aNMM-1 achieves 4.13% gain for MAP and 3.83% gain for MRR. These results show that it is possible to build a uniform deep learning model such that it can achieve better performance than methods using feature engineering. To the best of our knowledge, aNMM is the first deep learning model that can achieve good performance comparing with previous methods either based on deep learning models or feature engineering for ranking answers without any additional features, syntactic parsers and external resources except for pre-trained word embeddings.

Learning with Combining Additional Features
Our second experimental setting is to address RQ2 proposed in Section 1, where we ask whether our model can outperform the state-of-the-art performance achieved by CNN and LSTM for answer ranking when combining additional features. We combine the predicted score from aNMM-1 and aNMM-2 with the Query Likelihood (QL) score using LambdaMART following a similar approach to. We use the implementation of LambdaMART in jforests We compare the results with previous deep learning models with additional features. shows the results on TRAIN and TRAIN-ALL when combining additional features. We can see that with combined features, both aNMM-1 and aNMM-2 have better performance. aNMM-1 also outperforms 3 https://github.com/yasserg/jforests. 0.6029 0.6852 Heilman and Smith (2010) 0.6091 0.6917 Wang and Manning (2010) which is the current state-of-the-art method for ranking answers in terms of both MAP and MRR on TRAIN and TRAIN-ALL. We also tried to combine aNMM score with other additional features such as word overlap features, IDF weighted word overlap features and BM25 as in previous research. The results were either similar or worse than combining aNMM score with QL. For aNMM, the gains after combining additional features are not as large as neural network models like CNN in and LSTM in. We think the reasons for this are two-fold: (1) The QA matching matrix in aNMM model can capture exact match information by assigning 1 to matrix elements if the corresponding answer term and question term are the same. This exact match information includes match between numbers and proper nouns, which are highlighted in previous research work as especially important for factoid questions answering, where most of the questions are of type what, when , who thatare looking for answers containing numbers or proper nouns. Within aNMM architecture, this problem has already been handled with QA matching matrix. Thus incorporating word overlap features will not help much for improving the performance of aNMM. (2) In addition to exact match information, aNMM could also learn question term importance like IDF information through question attention network. Instead of empirically designing heuristic functions like IDF, aNMM can get learning based question term importance score. As analyzed in Section 4.3.2, with the optimization process in the back propagation training process, aNMM can learn similar or even better term weighting score than IDF. Thus combining aNMM score with features like IDF weighted word overlap features and BM25 may not increase the performance of aNMM by a large margin as the casein related research works.

Results Summary
Finally we summarize the results of previously published systems on the QA answer ranking task in. We can see aNMM  trained with TRAIN-ALL set beats all the previous state-of-the art systems including both methods using feature engineering and deep learning models. These results are very promising since aNMM requires no manual feature engineering, no expensive processing by various NLP parsers and no external results like large scale knowledge base except for pre-trained word embeddings. Furthermore, even without combining additional features, aNMM still performs well for answer ranking, showing significant improvements over previous deep learning model with no additional features and linguistic feature engineering methods.

Parameter Sensitivity Analysis
We perform parameter sensitivity analysis of our proposed model aNMM. We focus on aNMM-1 as the example due to the space limitation. For aNMM-1, we fix the number of bins as 600 and change the dimension of word vectors. Similarly, we fix the dimension of word vectors as 700 and vary the number of bins. shows the change of MAP and MRR on the validation data as we vary the hyper-parameters. We summarize our observations as follows: (1) For word vector dimension, the range (300, 700) is a good choice as much lower or higher word vector dimensions will hurt the performance. The choice of word vector dimension also depends on the size of training corpus. Larger corpus requires higher dimension of word vectors to embed terms in vocabulary. (2) For the number of bins, we can see that MAP and MRR will decrease as the bin number increase. Too many bins will increase the model complexity, which leads aNMM to be more likely to overfit the training data. Thus choosing suitable number of bins by optimizing hyperparameter on validation data can help improve the performance of aNMM.

CONCLUSIONS AND FUTURE WORK
In this paper, we propose an attention based neural matching model for ranking short answer text. We adopt value-shared weighting scheme instead of position-shared weighting scheme for combing different matching signals and incorporate question term importance learning using a question attention network. We perform a thorough experimental study with the TREC QA dataset from TREC QA tracks 8-13 and show promising results. Unlike previous methods including CNN as in and LSTM as in, which only show inferior results without combining additional features, our model can achieve better performance than the state-of-art method using linguistic feature engineering without additional features. With a simple additional feature, our method can achieve the new state-of-the-art performance among current existing methods. For further work, we will study other deep learning architectures for answer ranking and extend our work to include nonfactoid question answering data sets.

ACKNOWLEDGMENTS
This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF IIS-1160894, and in part by NSF grant #IIS-1419693. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.