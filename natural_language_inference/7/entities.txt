131	4	45	passage - aligned question representation
131	46	48	is
131	49	56	crucial
157	11	18	observe
157	19	39	general improvements
157	40	50	when using
157	51	57	labels
157	58	62	that
157	63	76	closely align
157	77	81	with
157	86	90	task
162	47	81	interactions between the endpoints
162	82	87	using
162	92	106	spanlevel FFNN
163	0	5	RASOR
163	6	17	outperforms
163	22	47	endpoint prediction model
163	48	50	by
163	51	54	1.1
163	55	57	in
163	58	69	exact match
106	3	12	represent
106	25	30	words
106	31	33	in
106	38	59	question and document
106	60	65	using
106	66	98	300 dimensional GloVe embeddings
106	99	109	trained on
106	112	118	corpus
106	18	20	of
106	122	134	840 bn words
106	119	121	of
107	17	22	cover
107	23	34	200 k words
107	39	74	all out of vocabulary ( OOV ) words
107	79	93	projected onto
107	94	97	one
107	101	141	1 m randomly initialized 300d embeddings
108	3	9	couple
108	14	36	input and forget gates
108	37	39	in
108	40	49	our LSTMs
108	77	80	use
108	83	102	single dropout mask
108	103	111	to apply
108	112	119	dropout
108	120	126	across
108	127	148	all LSTM time - steps
109	0	13	Hidden layers
109	14	16	in
109	21	49	feed forward neural networks
109	50	53	use
109	54	76	rectified linear units
110	0	17	Answer candidates
110	22	32	limited to
110	33	38	spans
110	39	43	with
110	44	60	at most 30 words
112	4	14	best model
112	15	19	uses
112	20	35	50d LSTM states
112	38	57	two - layer BiLSTMs
112	58	61	for
112	66	78	span encoder
112	87	132	passage - independent question representation
112	135	142	dropout
112	143	145	of
112	146	149	0.1
112	169	188	learning rate decay
112	189	191	of
112	192	212	5 % every 10 k steps
113	4	10	models
113	15	32	implemented using
113	33	43	TensorFlow
113	50	60	trained on
113	65	83	SQUAD training set
113	84	89	using
113	94	108	ADAM optimizer
113	109	113	with
113	116	131	mini-batch size
113	132	134	of
113	135	136	4
113	141	154	trained using
113	155	187	10 asynchronous training threads
113	188	190	on
113	193	207	single machine
111	0	9	To choose
111	14	39	final model configuration
111	45	48	ran
111	49	62	grid searches
111	63	67	over
111	74	88	dimensionality
111	89	91	of
111	96	114	LSTM hidden states
111	121	136	width and depth
111	137	139	of
111	144	172	feed forward neural networks
111	175	182	dropout
111	183	186	for
111	191	196	LSTMs
111	203	209	number
111	210	212	of
111	213	232	stacked LSTM layers
111	243	280	decay multiplier [ 0.9 , 0.95 , 1.0 ]
111	295	303	multiply
111	308	338	learning rate every 10 k steps
24	22	29	present
24	32	57	novel neural architecture
24	58	64	called
24	65	70	RASOR
24	76	82	builds
24	83	118	fixed - length span representations
24	121	128	reusing
24	129	151	recurrent computations
24	152	155	for
24	156	176	shared substructures
25	3	14	demonstrate
25	20	40	directly classifying
25	41	48	each of
25	53	68	competing spans
25	75	83	training
25	84	88	with
25	89	109	global normalization
25	110	114	over
25	115	133	all possible spans
25	136	144	leads to
25	147	167	significant increase
25	168	170	in
25	171	182	performance
2	44	73	EXTRACTIVE QUESTION ANSWERING
7	33	50	answer extraction
121	83	88	RASOR
121	89	97	achieves
121	101	116	error reduction
121	57	59	of
121	120	134	more than 50 %
121	161	172	in terms of
121	173	191	exact match and F1
121	194	205	relative to
121	210	239	human performance upper bound
125	51	56	model
125	24	50	efficiently and explicitly
125	61	97	quadratic number of possible answers
125	106	114	leads to
125	117	137	14 % error reduction
125	138	142	over
125	147	181	best performing Match - LSTM model
