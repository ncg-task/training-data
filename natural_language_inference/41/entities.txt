154	3	12	pre-train
154	15	26	large model
154	27	31	with
154	32	41	12 layers
154	42	44	in
154	57	76	encoder and decoder
154	85	96	hidden size
154	50	52	of
154	100	104	1024
155	0	9	Following
155	10	17	RoBERTa
155	23	26	use
155	29	39	batch size
155	40	42	of
155	43	47	8000
155	54	59	train
155	64	69	model
155	70	73	for
155	74	86	500000 steps
156	0	9	Documents
156	14	28	tokenized with
156	33	58	same byte - pair encoding
156	59	61	as
156	62	69	GPT - 2
157	39	42	use
157	45	56	combination
157	57	59	of
157	60	74	text infilling
157	79	99	sentence permutation
158	3	7	mask
158	8	12	30 %
158	13	15	of
158	16	22	tokens
158	23	25	in
158	26	39	each document
158	46	53	permute
158	54	67	all sentences
160	43	52	dis abled
160	53	60	dropout
160	61	64	for
160	69	79	final 10 %
160	80	82	of
160	83	97	training steps
160	0	7	To help
160	12	17	model
160	18	28	better fit
160	33	37	data
163	41	48	RoBERTa
163	61	77	pre-trained with
163	82	96	same resources
163	105	124	different objective
169	8	23	experiment with
169	24	53	several text generation tasks
175	78	85	present
175	86	93	results
175	94	96	on
175	97	123	two summarization datasets
175	126	141	CNN / DailyMail
175	146	150	XSum
178	15	19	BART
178	20	31	outperforms
180	21	39	best previous work
180	48	57	leverages
180	58	62	BERT
180	65	67	by
180	68	86	roughly 6.0 points
180	87	89	on
180	90	107	all ROUGE metrics
180	110	122	representing
180	125	144	significant advance
180	145	147	in
180	148	159	performance
183	3	11	evaluate
183	12	40	dialogue response generation
183	41	43	on
183	44	51	CONVAI2
184	0	4	BART
184	5	16	outperforms
184	17	30	previous work
184	31	33	on
184	34	55	two automated metrics
186	3	6	use
186	11	41	recently proposed ELI5 dataset
187	3	7	find
187	8	12	BART
187	13	24	outperforms
187	29	47	best previous work
187	48	50	by
187	51	64	1.2 ROUGE - L
194	16	29	experiment on
194	34	67	original WMT16 Romanian - English
194	68	82	augmented with
194	83	106	back - translation data
196	4	23	Preliminary results
196	24	33	suggested
196	39	51	our approach
196	52	55	was
196	56	70	less effective
196	71	78	without
196	79	102	back - translation data
196	109	117	prone to
196	118	129	overfitting
17	19	26	present
17	27	31	BART
17	40	50	pre-trains
17	53	58	model
17	59	68	combining
17	69	117	Bidirectional and Auto - Regressive Transformers
18	0	4	BART
18	5	7	is
18	10	31	denoising autoencoder
18	32	42	built with
18	45	75	sequence - to - sequence model
18	84	97	applicable to
18	100	128	very wide range of end tasks
20	5	9	uses
20	12	79	standard Tranformer - based neural machine translation architecture
20	120	127	seen as
20	128	140	generalizing
20	141	145	BERT
20	148	154	due to
20	159	180	bidirectional encoder
20	185	188	GPT
20	191	195	with
20	200	225	left - to - right decoder
19	0	11	Pretraining
19	16	26	two stages
19	33	37	text
19	41	55	corrupted with
19	59	85	arbitrary noising function
19	100	130	sequence - to - sequence model
19	38	40	is
19	134	141	learned
19	142	156	to reconstruct
19	161	174	original text
2	17	54	Sequence - to - Sequence Pre-training
4	46	89	pretraining sequence - to - sequence models
