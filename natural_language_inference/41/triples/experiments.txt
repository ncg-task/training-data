(Contribution||has||Experiments)
(Experiments||has||Experimental setup)
(Experimental setup||pre-train||large model)
(large model||with||12 layers)
(12 layers||in||encoder and decoder)
(large model||with||hidden size)
(hidden size||of||1024)
(Experimental setup||use||combination)
(combination||of||text infilling)
(combination||of||sentence permutation)
(Experimental setup||Following||RoBERTa)
(RoBERTa||use||batch size)
(batch size||of||8000)
(RoBERTa||train||model)
(model||for||500000 steps)
(Experimental setup||has||Documents)
(Documents||tokenized with||same byte - pair encoding)
(same byte - pair encoding||as||GPT - 2)
(Experimental setup||permute||all sentences)
(Experimental setup||dis abled||dropout)
(dropout||for||final 10 %)
(final 10 %||of||training steps)
(final 10 %||To help||model)
(model||better fit||data)
(Experimental setup||mask||30 %)
(30 %||of||tokens)
(tokens||in||each document)
(Experiments||has||Tasks)
(Tasks||experiment with||several text generation tasks)
(several text generation tasks||experiment on||original WMT16 Romanian - English)
(original WMT16 Romanian - English||augmented with||back - translation data)
(original WMT16 Romanian - English||has||Preliminary results)
(Preliminary results||suggested||our approach)
(our approach||prone to||overfitting)
(our approach||was||less effective)
(less effective||without||back - translation data)
(several text generation tasks||use||recently proposed ELI5 dataset)
(recently proposed ELI5 dataset||find||BART)
(BART||has||outperforms)
(outperforms||by||1.2 ROUGE - L)
(outperforms||has||best previous work)
(several text generation tasks||present||results)
(results||on||two summarization datasets)
(two summarization datasets||name||CNN / DailyMail)
(two summarization datasets||name||XSum)
(two summarization datasets||has||BART)
(BART||has||outperforms)
(outperforms||has||all existing work)
(outperforms||has||best previous work)
(best previous work||by||roughly 6.0 points)
(roughly 6.0 points||representing||significant advance)
(significant advance||in||performance)
(roughly 6.0 points||on||all ROUGE metrics)
(best previous work||leverages||BERT)
(several text generation tasks||evaluate||dialogue response generation)
(dialogue response generation||has||BART)
(BART||has||outperforms)
(outperforms||has||previous work)
(outperforms||on||two automated metrics)
(dialogue response generation||on||CONVAI2)
(Experiments||has||Baselines)
(Baselines||has||RoBERTa)
(RoBERTa||pre-trained with||same resources)
(RoBERTa||pre-trained with||different objective)
