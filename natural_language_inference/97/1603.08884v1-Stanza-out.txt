title
A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data
abstract
Understanding unstructured text is a major goal within natural language processing .
Comprehension tests pose questions based on short text passages to evaluate such understanding .
In this work , we investigate machine comprehension on the challenging MCTest benchmark .
Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .
We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .
The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set .
Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text .
When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets a new state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .
Introduction
Humans learn in a variety of ways - by communication with each other , and by study , the reading of text .
Comprehension of unstructured text by machines , at a near- human level , is a major goal for natural language processing .
It has garnered significant attention from the machine learning research community in recent years .
Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .
Such tests are objectively gradable and can be used to assess a range of abilities , from basic understanding to causal reasoning to inference .
Given a text passage and a question about its content , a system is tested on its ability to determine the correct answer .
In this work , we focus on MCTest , a complex but data - limited comprehension benchmark , whose multiple - choice questions require not only extraction but also inference and limited reasoning .
Inference and reasoning are important human skills that apply broadly , beyond language .
We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .
There are many use-cases in which comprehension over limited data would be handy : for example , user manuals , internal documentation , legal contracts , and soon .
Moreover , work towards more efficient learning from any quantity of data is important in its own right , for bringing machines more inline with the way humans learn .
Typically , artificial neural networks require numerous parameters to capture complex patterns , and the more parameters , the more training data is required to tune them .
Likewise , deep models learn to extract their own features , but this is a data - intensive process .
Our model learns to comprehend at a high level even when data is sparse .
The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .
We refer to a question combined with one of its answer candidates as a hypothesis ( to be detailed below ) .
The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .
The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .
As in the semantic perspective , we consider matches over complete sentences .
We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .
Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph .
Words are represented throughout by embedding vectors .
These distinct perspectives naturally form a hierarchy that we depict in .
Language is hierarchical , so it makes sense that comprehension relies on hierarchical levels of understanding .
The perspectives of our model can be considered a type of feature .
However , they are implemented by parametric differentiable functions .
This is in contrast to most previous efforts on MCTest , whose numerous hand - engineered features can not be trained .
Our model , significantly , can be trained end - to - end with backpropagation .
To facilitate learning with limited data , we also develop a unique training scheme .
We initialize the model 's neural networks to perform specific heuristic functions that yield decent ( thought not impressive ) performance on the dataset .
Thus , the training scheme gives the model a safe , reasonable baseline from which to start learning .
We call this technique training wheels .
Computational models that comprehend ( insofar as they perform well on MC datasets ) have developed contemporaneously in several research groups .
Models designed specifically for MCTest include those of , and more recently , , and .
In experiments , our Parallel - Hierarchical model achieves state - of - the - art accuracy on MCTest , outperforming these existing methods .
Below we describe related work , the mathematical details of our model , and our experiments , then analyze our results .
The Problem
In this section we borrow from , who laid out the MC problem nicely .
Machine comprehension requires machines to answer questions based on unstructured text .
This can be viewed as selecting the best answer from a set of candidates .
In the multiple - choice case , candidate answers are predefined , but candidate answers may also be undefined yet restricted ( e.g. , to yes , no , or any noun phrase in the text ) .
For each question q , let T be the unstructured text and A = {a i } the set of candidate answers to q .
The machine comprehension task reduces to selecting the answer that has the highest evidence given T .
As in , we combine an answer and a question into a hypothesis , hi = f ( q , a i ) .
To facilitate comparisons of the text with the hypotheses , we also breakdown the passage into sentences t j , T = {t j }.
In our setting , q , a i , and t j each represent a sequence of embedding vectors , one for each word and punctuation mark in the respective item .
Related Work
Machine comprehension is currently a hot topic within the machine learning community .
In this section we will focus on the best - performing models applied specifically to MCTest , since it is somewhat unique among MC datasets ( see Section 5 ) .
Generally , models can be divided into two categories : those that use fixed , engineered features , and neural models .
The bulk of the work on MCTest falls into the former category .
Manually engineered features often require significant effort on the part of a designer , and / or various auxiliary tools to extract them , and they can not be modified by training .
On the other hand , neural models can be trained end - to - end and typically harness only a single feature : vectorrepresentations of words .
Word embeddings are fed into a complex and possibly deep neural network which processes and compares text to question and answer .
Among deep models , mechanisms of attention and working memory are common , as in and .
3.1 Feature - engineering models treated MCTest as a structured prediction problem , searching for a latent answerentailing structure connecting question , answer , and text .
This structure corresponds to the best latent alignment of a hypothesis with appropriate snippets of the text .
The process of ( latently ) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation .
The model uses event and entity coreference links across sentences along with a host of other features .
These include specifically trained word vectors for synonymy ; antonymy and class - inclusion relations from external data base sources ; dependencies and semantic role labels .
The model is trained using a latent structural SVM extended to a multitask setting , so that questions are first classified using a pretrained top - level classifier .
This enables the system to use different processing strategies for different question categories .
The model also combines question and answer into a well - formed statement using the rules of Cucerzan and Agichtein ( 2005 ) .
Our model is simpler than that of in terms of the features it takes in , the training procedure ( stochastic gradient descent vs. alternating minimization ) , question classification ( we use none ) , and question - answer combination ( simple concatenation or mean vs. a set of rules ) .
augmented the baseline feature set from with features for syntax , frame semantics , coreference chains , and word embeddings .
They combined features using a linear latent - variable classifier trained to minimize a max - margin loss function .
As in , questions and answers are combined using a set of manually written rules .
The method of achieved the previous state of the art , but has significant complexity in terms of the feature set .
Space does not permit a full description of all models in this category , but see also and .
Despite its relative lack of features , the Parallel - Hierarchical model improves upon the featureengineered state of the art for MCTest by a small amount ( about 1 % absolute ) as detailed in Section 5 .
Neural models
Neural models have , to date , performed relatively poorly on MCTest .
This is because the dataset is sparse and complex .
investigated deep - learning approaches concurrently with the present work .
They measured the performance of the Attentive Reader and the Neural Reasoner , both deep , end - to - end recurrent models with attention mechanisms , and also developed an attention - based convolutional network , the HABCNN .
Their network operates on a hierarchy similar to our own , providing further evidence of the promise of hierarchical perspectives .
Specifically , the HABCNN processes text at the sentence level and the snippet level , where the latter combines adjacent sentences ( as we do through an n-gram input ) .
Embedding vectors for the question and the answer candidates are combined and encoded by a convolutional network .
This encoding modulates attention over sentence and snippet encodings , followed by maxpooling to determine the best matches between question , answer , and text .
As in the present work , matching scores are given by cosine similarity .
The HABCNN also makes use of a question classifier .
Despite the shared concepts between the HABCNN and our approach , the Parallel - Hierarchical model performs significantly better on MCTest ( more than 15 % absolute ) as detailed in Section 5 .
Other neural models tested in fare even worse .
The Parallel - Hierarchical Model
Let us now define our machine comprehension model in full .
We first describe each of the perspectives separately , then describe how they are combined .
Below , we use subscripts to index elements of sequences , like word vectors , and superscripts to indicate whether elements come from the text , question , or answer .
In particular , we use the subscripts k , m , n , p to index sequences from the text , question , answer , and hypothesis , respectively , and superscripts t , q , a , h. We depict the model schematically in .
Semantic Perspective
The semantic perspective is similar to the Memory Networks approach for embedding inputs into memory space .
Each sen - tence of the text is a sequence of d-dimensional word vectors :
The semantic vector st is computed by embedding the word vectors into a D-dimensional space using a two - layer network that implements weighted sum followed by an affine tranformation and a nonlinearity ; i.e. ,
The matrix At ? R Dd , the bias vector b t A ? RD , and for f we use the leaky ReLU function .
The scalar ?
k is a trainable weight associated to each word in the vocabulary .
These scalar weights implement a kind of exogenous or bottomup attention that depends only on the input stimulus .
They can , for example , learn to perform the function of stopword lists in a soft , trainable way , to nullify the contribution of unimportant filler words .
The semantic representation of a hypothesis is formed analogously , except that we combine the question word vectors q m and answer word vectors an as a single sequence {h p } = {q m , an }.
For semantic vector sh of the hypothesis , we use a unique transformation matrix A h ?
R Dd and bias vector b h A ?
RD .
These transformations map a text sentence and a hypothesis into a common space where they can be compared .
We compute the semantic match be-tween text sentence and hypothesis using the cosine similarity , M sem = cos ( s t , sh ) .
( 2 )
Word - by - Word Perspective
The first step in building the word - by - word perspective is to transform word vectors from a text sentence , question , and answer through respective neural functions .
For the text ,
?
RD and f is again the leaky ReLU .
We transform the question and the answer toq m and n analogously using distinct matrices and bias vectors .
In contrast with the semantic perspective , we keep the question and answer candidates separate in the wordby - word perspective .
This is because matches to answer words are inherently more important than matches to question words , and we want our model to learn to use this property .
Sentential
Inspired by the work of in paraphrase detection , we compute matches between hypotheses and text sentences at the word level .
This computation uses the cosine similarity as before :
ca kn = cos (t k , n ) .
The word - by - word match between a text sentence and question is determined by taking the maximum over k ( finding the text word that best matches each question word ) and then taking a weighted mean over m ( finding the average match over the full question ) :
Here , ?
m is the word weight for the question word and Z normalizes these weights to sum to one over the question .
We define the match between a sentence and answer candidate , M a , analogously .
Finally , we combine the matches to question and answer according to
Here the ?
are trainable parameters that control the relative importance of the terms .
Sequential Sliding Window
The sequential sliding window is related to the original MCTest baseline by .
Our sliding window decays from its focus word according to a Gaussian distribution , which we extend by assigning a trainable weight to each location in the window .
This modification enables the window to use information about the distance between word matches ; the original baseline used distance information through a predefined function .
The sliding window scans over the words of the text as one continuous sequence , without sentence breaks .
Each window is treated like a sentence in the previous subsection , but we include a location - based weight ?( k ) .
This weight is based on a word 's position in the window , which , given a window , depends on its global position k.
The cosine similarity is adapted as
for the question and analogously for the answer .
We initialize the location weights with a Gaussian and fine - tune them during training .
The final matching score , denoted as M sws , is computed as in and with sq km replacing c q km .
Dependency Sliding Window
The dependency sliding window operates identically to the linear sliding window , but on a different view of the text passage .
The output of this component is M swd and is formed analogously to M sws .
The dependency perspective uses the Stanford Dependency Parser as an auxiliary tool .
Thus , the dependency graph can be considered a fixed feature .
Moreover , linearization of the dependency graph , because it relies on an eigendecomposition , is not differentiable .
However , we handle the linearization in data preprocessing so that the model sees only reordered word - vector inputs .
Specifically , we run the Stanford Dependency Parser on each text sentence to build a dependency graph .
This graph has n w vertices , one for each word in the sentence .
From the dependency graph we form the Laplacian matrix L ?
R nwnw and determine its eigenvectors .
The second eigenvector u 2 of the Laplacian is known as the Fiedler vector .
It is the solution to the minimization
where vi are the vertices of the graph , and ?
ij is the weight of the edge from vertex i to vertex j.
The Fiedler vector maps a weighted graph onto a line such that connected nodes stay close , modulated by the connection weights .
1
This enables us to reorder the words of a sentence based on their proximity in the dependency graph .
The reordering of the words is given by the ordered index set I = arg sort ( u 2 ) .
To give an example of how this works , consider the following sentence from MCTest and its dependency - based reordering :
Jenny , Mrs. Mustard 's helper , called the police .
the police , called Jenny helper , Mrs. 's Mustard .
Sliding - window - based matching on the original sentence will answer the question
Who called the police ? with Mrs. Mustard .
The dependency reordering enables the window to determine the correct answer , Jenny .
Combining Distributed
Evidence
It is important in comprehension to synthesize information found throughout a document .
MCTest was explicitly designed to ensure that it could not be solved by lexical techniques alone , but would instead require some form of inference or limited reasoning .
It therefore includes questions where the evidence for an answer spans several sentences .
To perform synthesis , our model also takes in ngrams of sentences , i.e. , sentence pairs and triples strung together .
The model treats these exactly as it does single sentences , applying all functions detailed above .
A later pooling operation combines scores across all n-grams ( including the singlesentence input ) .
This is described in the next subsection .
With n-grams , the model can combine information distributed across contiguous sentences .
In some cases , however , the required evidence is spread across distant sentences .
To give our model some capacity to deal with this scenario , we take the top N sentences as scored by all the preceding functions , and then repeat the scoring computations viewing these top N as a single sentence .
The reasoning behind these approaches can be explained well in a probabilistic setting .
If we consider our similarity scores to model the likelihood of a text sentence given a hypothesis , p (t j |h i ) , then the n-gram and top N approaches model a joint probability p (t j 1 , t j 2 , . . . , t j k |h i ) .
We can not model the joint probability as a product of individual terms ( score values ) because distributed pieces of evidence are likely not independent .
Combining Perspectives
We use a multilayer perceptron to combine M sem , M word , M swd , and M sws as a final matching score M i for each answer candidate .
This network also pools and combines the separate n-gram scores , and uses a linear activation function .
Our over all training objective is to minimize the ranking loss
where is a constant margin , i * indexes the correct answer , and we take the maximum over i so that we are ranking the correct answer over the best - ranked incorrect answer ( of which there are three ) .
This approach worked better than comparing the correct answer to the incorrect answers individually as in .
Our implementation of the Parallel - Hierarchical model , using the Keras framework , is available on Github .
2
Training Wheels
Before training , we initialized the neural - network components of our model to perform sensible heuristic functions .
Training did not converge on the small MCTest without this vital approach .
Empirically , we found that we could achieve above 50 % accuracy on MCTest using a simple sum of word vectors followed by a dot product between the question sum and the hypothesis sum .
Therefore , we initialized the network for the semantic perspective to perform this sum , by initializing A x as the identity matrix and bx A as the zero vector , x ? {t , h} .
Recall that the activation function is a ReLU so that positive outputs are unchanged .
We also found basic word - matching scores to be helpful , so we initialized the word - by - word networks likewise .
The network for perspectivecombination was initialized to perform a sum of individual scores , using a zero bias - vector and a weight matrix of ones , since we found that each perspective contributed positively to the over all result .
This training wheels approach is related to other techniques from the literature .
For instance , proposed the identity - matrix initialization in the context of recurrent neural networks in order to preserve the error signal through backpropagation .
In residual networks , shortcut connections bypass certain layers in the network so that a simpler function can be trained in conjunction with the full model .
Experiments
The Dataset
MCTest is a collection of 660 elementary - level children 's stories and associated questions , written by human subjects .
The stories are fictional , ensuring that the answer must be found in the text itself , and carefully limited to what a young child can understand .
The more challenging variant consists of 500 stories with four multiple - choice questions each .
Despite the elementary level , stories and questions are more natural and more complex than those found in synthetic MC datasets like bAb I and CNN .
MCTest is challenging because it is both complicated and small .
As per , " it is very difficult to train statistical models only on MCTest . "
It s size limits the number of parameters that can be trained , and prevents learning any complex language modeling simultaneously with the capacity to answer questions .
Training and Model Details
In this section we describe important details of the training procedure and model setup .
For a complete list of hyperparameter settings , our stopword list , and other minutiae , we refer interested readers to our Github repository .
For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .
These vectors are kept fixed throughout training , since we found that training them was not helpful ( likely because of MCTest 's size ) .
The vectors are 300 - dimensional ( d = 300 ) .
We do not use a stopword list for the text passage , instead relying on the trainable word weights to ascribe global importance ratings to words .
These weights are initialized with the inverse document frequency ( IDF ) statistic computed over the MCTest corpus .
3
However , we douse a short stopword list for questions .
This list nullifies query words such as { Who , what , when , where , how} , along with conjugations of the verbs to do and to be .
Following earlier methods , we use a heuristic to improve performance on negation questions .
When a question contains the words which and not , we negate the hypothesis ranking scores so that the minimum becomes the maximum .
The most important technique for training the model was the training wheels approach .
Without this , training was not effective at all .
The identity initialization requires that the network weight matrices are square ( d = D ) .
We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .
Dropout occurs after all neural - network transformations , if those transformations are allowed to change with training .
Our best performing model held networks at the wordby - word level fixed .
For combining distributed evidence , we used up to trigrams over sentences and our bestperforming model reiterated over the top two sentences ( N = 2 ) .
We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .
To determine the best hyperparameters we performed a grid search over 150 settings based on validation - set accuracy .
MCTest 's original validation set is too small for reliable hyperparameter tuning , so , following , we merged the training and validation sets of MCTest - 160 and MCTest - 500 , then split them randomly into a 250 - story training set and a 200 - story validation set .
presents the performance of featureengineered and neural methods on the MCTest test set .
Accuracy scores are divided among questions whose evidence lies in a single sentence ( single ) and across multiple sentences ( multi ) , and among the two variants .
Clearly , MCTest - 160 is easier .
Results
The first three rows represent featureengineered methods .
+
RTE is the best - performing variant of the original baseline published along with MCTest .
It uses a lexical sliding window and distance - based measure , augmented with rules for recognizing textual entailment .
We described the methods of and in Section 3 .
On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and over all ( ?
1 % ) .
The method of achieves the best over all result on MCTest - 160 .
We suspect this is because our neural method suffered from the relative lack of training data .
The last four rows in are neural methods that we discussed in Section 3 .
Performance measures are taken from .
Here we see our model outperforming the alternatives by a large margin across the board ( > 15 % ) .
The Neural Reasoner and the Attentive Reader are large , deep models with hundreds of thousands of parameters , so it is unsurprising that they performed poorly on MCTest .
The specificallydesigned HABCNN fared better , its convolutional architecture cutting down on the parameter count .
Because there are similarities between our model and the HABCNN , we hypothesize that much of the performance difference is attributable to our training wheels methodology .
Analysis and Discussion
We measure the contribution of each component of the model by ablating it .
Results are given in .
Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .
Without this , the model has almost no Method MCTest - 160 accuracy ( % )
MCTest - 500 accuracy ( % ) Single means for synthesizing distributed evidence .
The top N function contributes very little to the over all performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .
Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .
Simple word - by - word matching is obviously useful on MCTest .
The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .
On the other hand , the dependency - based sliding window makes only a minor contribution .
We found this surprising .
It maybe that linearization of the dependency graph removes too much of its information .
Finally , the exogenous word weights make a significant contribution of almost 5 % .
Analysis reveals that most of our system 's test failures occur on questions about quantity ( e.g. , How many ...? ) and temporal order ( e.g. , Who was invited last ? ) .
Quantity questions makeup 9.5 % of our errors on the validation set , while order questions makeup 10.3 % .
This weakness is not unexpected , since our architecture lacks any capacity for counting or tracking temporal order .
Incorporating mechanisms for these forms of reasoning is a priority for future work ( in contrast , the Memory Network model is quite good at temporal reasoning ) .
The Parallel - Hierarchical model is simple .
It does no complex language or sequence modeling .
It s simplicity is a response to the limited data of MCTest .
Nevertheless , the model achieves stateof - the - art results on the multi questions , which ( putatively ) require some limited reasoning .
Our model is able to handle them reasonably well just by stringing important sentences together .
Thus , the model imitates reasoning with a heuristic .
This suggests that , to learn true reasoning abilities , MCTest is too simple a dataset - and it is almost certainly too small for this goal .
However , it maybe that human language processing can be factored into separate processes of comprehension and reasoning .
If so , the Parallel - Hierarchical model is a good start on the former .
Indeed , if we train the method exclusively on single questions then its results become even more impressive : we can achieve a test accuracy of 79.1 % on MCTest - 500 .
Conclusion
We have presented the novel Parallel - Hierarchical model for machine comprehension , and evaluated it on the small but complex MCTest .
Our model achieves state - of - the - art results , outperforming several feature - engineered and neural approaches .
Working with our model has emphasized to us the following ( not necessarily novel ) concepts , which we record hereto promote further empirical validation .
Good comprehension of language is supported by hierarchical levels of understanding ( Cf. ) .
Exogenous attention ( the trainable word weights ) maybe broadly helpful for NLP .
The training wheels approach , that is , initializing neural networks to perform sensible heuristics , appears helpful for small datasets .
Reasoning over language is challenging , but easily simulated in some cases .
