145	0	58	Both the coarse - grain module and the fine - grain module
145	73	86	contribute to
145	87	104	model performance
149	4	29	fine - grain - only model
149	30	44	under-performs
149	49	76	coarse - grain - only model
149	77	89	consistently
149	90	96	across
149	97	123	almost all length measures
151	40	62	matches or outperforms
151	67	94	coarse - grain - only model
151	95	97	on
151	98	106	examples
151	107	111	with
151	114	126	large number
151	127	129	of
151	130	147	support documents
151	156	178	long support documents
146	103	112	result in
146	113	141	less performance degradation
146	0	9	Replacing
146	10	30	selfattention layers
146	31	35	with
146	36	50	mean - pooling
146	59	77	bidirectional GRUs
146	78	82	with
146	83	102	unidirectional GRUs
147	0	9	Replacing
147	14	21	encoder
147	22	26	with
147	29	39	projection
147	40	44	over
147	45	60	word embeddings
147	61	70	result in
147	71	99	significant performance drop
111	0	35	MULTI - EVIDENCE QUESTION ANSWERING
111	36	38	ON
111	39	46	WIKIHOP
121	3	11	tokenize
121	21	26	using
121	27	43	Stanford CoreNLP
122	3	6	use
122	7	30	fixed Glo Ve embeddings
122	31	41	as well as
122	42	68	character ngram embeddings
123	3	8	split
123	9	33	symbolic query relations
123	34	38	into
123	39	44	words
124	15	28	trained using
124	29	33	ADAM
127	4	7	CFC
127	8	16	achieves
127	17	47	state - of - the - art results
127	48	50	on
127	60	88	masked and unmasked versions
127	89	91	of
127	92	99	WikiHop
128	16	18	on
128	23	58	blind , held - out WikiHop test set
128	65	68	CFC
128	69	77	achieves
128	80	97	new best accuracy
128	98	100	of
128	101	107	70.6 %
134	0	39	RERANKING EXTRACTIVE QUESTION ANSWERING
134	40	42	ON
134	43	51	TRIVIAQA
141	0	24	Our experimental results
141	28	32	show
141	38	47	reranking
141	48	53	using
141	58	61	CFC
141	62	70	provides
141	71	99	consistent performance gains
141	100	115	over only using
141	120	160	span extraction question answering model
142	16	25	reranking
142	26	31	using
142	36	39	CFC
142	40	48	improves
142	49	60	performance
143	0	2	On
143	7	30	whole Trivia QA dev set
143	33	42	reranking
143	43	48	using
143	53	56	CFC
143	57	67	results in
143	68	73	again
143	74	76	of
143	77	98	3.1 % EM and 3.0 % F1
46	0	27	Our multi-evidence QA model
46	34	89	Coarse - grain Fine - grain Coattention Network ( CFC )
46	92	105	selects among
46	108	111	set
46	112	114	of
46	115	132	candidate answers
46	133	138	given
46	141	144	set
46	145	147	of
46	148	165	support documents
46	172	177	query
47	4	7	CFC
47	11	22	inspired by
47	23	47	coarse - grain reasoning
47	52	74	fine - grain reasoning
51	0	11	Each module
51	12	19	employs
51	22	50	novel hierarchical attention
51	55	64	hierarchy
51	65	67	of
51	68	79	coattention
51	84	100	self - attention
51	103	113	to combine
51	114	125	information
51	126	130	from
51	135	152	support documents
51	153	167	conditioned on
51	172	177	query
51	182	192	candidates
48	0	2	In
48	3	27	coarse - grain reasoning
48	34	39	model
48	40	46	builds
48	49	63	coarse summary
48	64	66	of
48	67	84	support documents
48	85	99	conditioned on
48	104	109	query
49	3	25	fine - grain reasoning
49	32	37	model
49	38	45	matches
49	46	73	specific finegrain contexts
49	134	142	to gauge
49	147	156	relevance
49	157	159	of
49	87	96	candidate
2	55	90	MULTI - EVIDENCE QUESTION ANSWERING
4	63	81	question answering
42	40	65	question answering ( QA )
43	79	104	neural question answering
