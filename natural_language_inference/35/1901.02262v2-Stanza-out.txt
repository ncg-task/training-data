title
Multi - Style Generative Reading Comprehension
abstract
This study tackles generative reading comprehension ( RC ) , which consists of answering questions based on textual evidence and natural language generation ( NLG ) .
We propose a multi-style abstractive summarization model for question answering , called Masque .
The proposed model has two key characteristics .
First , unlike most studies on RC that have focused on extracting an answer span from the provided passages , our model instead focuses on generating a summary from the question and multiple passages .
This serves to cover various answer styles required for real - world applications .
Second , whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model , our approach learns multi-style answers within a model to improve the NLG capability for all styles involved .
This also enables our model to give an answer in the target style .
Experiments show that our model achieves state - of - the - art performance on the Q&A task and the Q & A + NLG task of MS MARCO 2.1 and the summary task of Nar-rative QA .
We observe that the transfer of the style - independent NLG capability to the target style is the key to its success .
Introduction
Question answering has been a long - standing research problem .
Recently , reading comprehension ( RC ) , a challenge to answer a question given textual evidence provided in a document set , has received much attention .
Current mainstream studies have treated RC as a process of extracting an answer span from one passage or multiple passages , which is usually done by predicting the start and end positions of the answer .
* Work done during an internship at NTT .
The demand for answering questions in natural language is increasing rapidly , and this has led to the development of smart devices such as Alexa .
In comparison with answer span extraction , however , the natural language generation ( NLG ) capability for RC has been less studied .
While datasets such as MS MARCO and Nar-rative QA have been proposed for providing abstractive answers , the stateof - the - art methods for these datasets are based on answer span extraction .
Generative models suffer from a dearth of training data to cover open - domain questions .
Moreover , to satisfy various information needs , intelligent agents should be capable of answering one question in multiple styles , such as wellformed sentences , which make sense even without the context of the question and passages , and concise phrases .
These capabilities complement each other , but previous studies can not use and control different styles within a model .
In this study , we propose Masque , a generative model for multi-passage RC .
It achieves stateof - the - art performance on the Q&A task and the Q & A + NLG task of MS MARCO 2.1 and the summary task of Narrative QA .
The main contri-butions of this study are as follows .
Multi - source abstractive summarization .
We introduce the pointer - generator mechanism for generating an abstractive answer from the question and multiple passages , which covers various answer styles .
We extend the mechanism to a Transformer based one that allows words to be generated from a vocabulary and to be copied from the question and passages .
Multi- style learning for style control and transfer .
We introduce multi-style learning that enables our model to control answer styles and improves RC for all styles involved .
We also extend the pointer - generator to a conditional decoder by introducing an artificial token corresponding to each style , as in .
For each decoding step , it controls the mixture weights over three distributions with the given style ( ) .
Problem Formulation
This paper considers the following task :
, and an answer style label s , an RC model outputs an answer y = {y 1 , . . . , y T } conditioned on the style .
In short , given a 3 - tuple ( x q , {x pk } , s ) , the system predicts P ( y ) .
The training data is a set of 6 - tuples : ( x q , {x pk } , s , y , a , {r pk } ) , where a and {r pk } are optional .
Here , a is 1 if the question is answerable with the provided passages and 0 otherwise , and r pk is 1 if the k - th passage is required to formulate the answer and 0 otherwise .
Proposed Model
We propose a Multi-style Abstractive Summarization model for QUEstion answering , called Masque .
Masque directly models the conditional probability p ( y | x q , {x pk } , s ) .
As shown in 4 .
The answer sentence decoder ( 3.4 ) outputs an answer sentence conditioned on the target style .
Our model is based on multi-source abstractive summarization : the answer that it generates can be viewed as a summary from the question and passages .
The model also learns multi-style answers together .
With these two characteristics , we aim to acquire the style - independent NLG ability and transfer it to the target style .
In addition , to improve natural language understanding in the reader module , our model considers RC , passage ranking , and answer possibility classification together as multi-task learning .
Question - Passages Reader
The reader module is shared among multiple answer styles and the three task - specific modules .
Word Embedding Layer
Let y represent one - hot vectors of the words in the answer .
This layer has the same components as the word embedding layer of the reader module , except that it uses a unidirectional ELMo to ensure that the predictions for position t depend only on the known outputs at positions previous tot .
Artificial tokens .
To be able to use multiple answer styles within a single system , our model introduces an artificial token corresponding to the style at the beginning of the answer ( y 1 ) , as done in .
At test time , the user can specify the first token to control the style .
This modification does not require any changes to the model architecture .
Note that introducing the token at the decoder prevents the reader module from depending on the answer style .
Shared Encoder Layer
This layer uses a stack of Transformer blocks , which are shared by the question and passages , on top of the embeddings provided by the word embedding layer .
The input of the first block is immediately mapped to a d-dimensional vector by a linear transformation .
The outputs of this layer are E pk ?
R d L for each k- th passage , and E q ?
R d J for the question .
Transformer encoder block .
The block consists of two sub - layers : a self - attention layer and a position - wise feed - forward network .
For the self - attention layer , we adopt the multi-head attention mechanism .
Following GPT , the feed - forward network consists of two linear transformations with a GELU activation function in between .
Each sub - layer is placed inside a residual block .
For an input x and a given sub - layer function f , the output is LN ( f ( x ) + x ) , where LN indicates the layer normalization .
To facilitate these residual connections , all sub - layers produce a sequence of d-dimensional vectors .
Note that our model does not use any position embeddings in this block because ELMo gives the positional information of the words in each sequence .
Dual Attention Layer
This layer uses a dual attention mechanism to fuse information from the question to the passages as well as from the passages to the question .
It first computes a similarity matrix U pk ?
R LJ between the question and the k - th passage , as done in , where
indicates the similarity between the l - th word of the k - th passage and the j - th question word .
The w a ?
R 3d are learnable parameters .
The ?
operator denotes the Hadamard product , and the [ ; ] operator denotes vector concatenation across the rows .
Next , the layer obtains the row and column normalized similarity matrices A pk = softmax j ( U pk ? ) and B pk = softmax l ( U pk ) .
It then uses DCN to obtain dual attention representations , G q?p k ?
R 5 d L and G p?q ?
R 5d J :
Here ,
Modeling Encoder Layer
This layer uses a stack of the Transformer encoder blocks for question representations and obtains M q ?
R d J from G p?q .
It also uses another stack for passage representations and obtains M pk ?
R d L from G q?p k for each k- th passage .
The outputs of this layer , M q and { M pk } , are passed onto the answer sentence decoder ; the { M pk } are also passed onto the passage ranker and the answer possibility classifier .
Passage Ranker
The ranker maps the output of the modeling layer , { M pk } , to the relevance score of each passage .
It takes the output for the first word , M pk 1 , which corresponds to the beginning - of - sentence token , to obtain the aggregate representation of each passage sequence .
Given w r ?
Rd as learnable parameters , it calculates the relevance of each k- th passage to the question as
Answer Possibility Classifier
The classifier maps the output of the modeling layer to a probability for the answer possibility .
It also takes the output for the first word , M pk 1 , for all passages and concatenates them .
Given w c ?
R
Kd as learnable parameters , it calculates the answer possibility for the question as
Answer Sentence Decoder
Given the outputs provided by the reader module , the decoder generates a sequence of answer words one element at a time .
It is autoregressive , consuming the previously generated words as additional input at each decoding step .
Attentional Decoder Layer
This layer uses a stack of Transformer decoder blocks on top of the embeddings provided by the word embedding layer .
The input is immediately mapped to a d-dimensional vector by a linear transformation , and the output is a sequence of d-dimensional vectors : {s 1 , . . . , s T }.
Transformer decoder block .
In addition to the encoder block , this block consists of the second and third sub-layers after the self - attention block and before the feed - forward network , as shown in .
As in , the selfattention sub - layer uses a sub-sequent mask to prevent positions from attending to subsequent positions .
The second and third sub-layers perform the multi-head attention over M q and M pall , respectively .
The M pall is the concatenated outputs of the encoder stack for the passages ,
Here , the [ , ] operator denotes vector concatenation across the columns .
This attention for the concatenated passages produces attention weights thatare comparable between passages .
Multi-source Pointer - Generator
Our extended mechanism allows both words to be generated from a vocabulary and words to be copied from both the question and multiple passages ( ) .
We expect that the capability of copying words will be shared among answer styles .
Additive Attention
Additive Attention For each decoding step t , mixture weights ? v , ? q , ?
p for the probability of generating words from the vocabulary and copying words from the question and the passages are calculated .
The three distributions are weighted and summed to obtain the final distribution .
Extended vocabulary distribution .
Let the extended vocabulary , V ext , be the union of the common words ( a small subset of the full vocabulary , V , defined by the input-side word embedding matrix ) and all words appearing in the input question and passages .
P v then denotes the probability distribution of the t- th answer word , y t , over the extended vocabulary .
It is defined as :
where the output embedding W 2 ?
Rd word Vext is tied with the corresponding part of the input embedding , and W 1 ?
Rd word d and b 1 ?
Rd word are learnable parameters .
P v ( y t ) is zero if y t is an out - of - vocabulary word for V .
Copy distributions .
A recent Transformerbased pointer - generator randomly chooses one of the attention - heads to form a copy distribution ; that approach gave no significant improvements in text summarization .
In contrast , our model uses an additional attention layer for each copy distribution on top of the decoder stack .
For the passages , the layer takes st as the query and outputs ? pt ?
R KL as the attention weights and c pt ?
Rd as the context vectors :
where w p , b p ?
Rd and W pm , W ps ?
R dd are learnable parameters .
For the question , our model uses another identical layer and obtains ? qt ?
R J and c qt ?
Rd .
As a result , P q and P pare the copy distributions over the extended vocabulary :
where k ( l ) means the passage index corresponding to the l - th word in the concatenated passages .
Final distribution .
The final distribution of y t is defined as a mixture of the three distributions :
where Wm ? R 33d and b m ?
R 3 are learnable parameters .
Combined Attention
In order not to attend words in irrelevant passages , our model introduces a combined attention .
While the original technique combined word and sentence level attentions , our model combines the word and passage level attentions .
The word attention , Eq. 1 , is re-defined as
Loss Function
We define the training loss as the sum of losses via L ( ? ) = L dec + ?
rank L rank + ?
cls L cls where ?
is the set of all learnable parameters , and ?
rank and ?
cls are balancing parameters .
The loss of the decoder , L dec , is the negative log likelihood of the whole target answer sentence averaged over N able answerable examples :
where Dis the training dataset .
The losses of the passage ranker , L rank , and the answer possibility classifier , L cls , are the binary cross entropy between the true and predicted values averaged over all N examples :
.
Setup
We only describe the settings specific to this experiment .
Datasets .
Following previous studies , we used the summary setting for the comparisons with the reported baselines , where each question refers to one summary ( averaging 659 words ) , and there is no unanswerable questions .
Our model therefore did not use the passage ranker and answer possibility classifier .
Answer styles .
The Narrative QA dataset does not explicitly provide multiple answer styles .
In order to evaluate the effectiveness of multi-style learning , we used the NLG subset of MS MARCO as additional training data .
We associated the Narrative QA and NLG datasets with two answer styles .
The answer style of Narrative QA ( NQA ) is different from that of MS MARCO ( NLG ) in that the answers are short ( averaging 4.73 words ) and contained frequently pronouns .
For instance , for the question " Who is Mark Hunter ? " , a reference is " He is a high school student in Phoenix . "
Evaluation metrics and baselines .
BLEU - 1 and 4 , METEOR , and ROUGE - L were used in accordance with the evaluation in the dataset paper .
We used the reports of top - performing extractive and generative models .
Results
Does our model achieve state - of - the - art performance ?
shows that our single model , trained with two styles and controlled with the NQA style , pushed forward the state - of - the - art by a significant margin .
The evaluation scores of the model controlled with the NLG style were low because the two styles are different .
Also , our model without multi-style learning ( trained with only the NQA style ) outperformed the baselines in terms of ROUGE - L .
This indicates that our model architec - ture itself is powerful for natural language understanding in RC .
Experiments on NarrativeQA
Next , we evaluated our model on Narra - tive QA .
It requires understanding the underlying narrative rather than relying on shallow pattern matching .
Our detailed setup and output examples are in the supplementary material .
Related Work and Discussion
Transfer and multi-task learning in RC .
Recent breakthroughs in transfer learning demonstrate that pre-trained language models perform well on RC with minimal modifications .
In addition , our model also uses ELMo for contextualized embeddings .
Multi - task learning is a transfer mechanism to improve generalization performance , and it is generally applied by sharing the hidden layers between all tasks , while keeping task - specific layers .
and reported that the sharing of the hidden layers between the multi -passage RC and passage ranking tasks was effective .
Our results also showed the effectiveness of the sharing of the question - passages reader module among the RC , passage ranking , and answer possibility classification tasks .
In multi-task learning without task - specific layers , and improved RC performance by learning multiple datasets from the same extractive RC setting .
and investigated multi-task and curriculum learning on many different NLP tasks ; their results were below task - specific RC models .
Our multi-style learning does not use style - specific layers ; instead uses a style - conditional decoder .
Generative RC .
S - Net used an extraction - then - synthesis mechanism for multi-passage RC .
The models proposed by , used an RNN - based pointer - generator mechanism for single - passage RC .
Although these mechanisms can alleviate the lack of training data , large amounts of data are still required .
Our multistyle learning will be a key technique enabling learning from many RC datasets with different styles .
In addition to MS MARCO and Narrative QA , there are other datasets that provide abstractive answers .
DuReader , a Chinese multi-document RC dataset , provides longer documents and answers than those of MS MARCO .
DuoRC and CoQA contain abstractive answers ; most of the answers are short phrases .
Controllable text generation .
Many studies have been carried out in the framework of style transfer , which is the task of rephrasing a text so that it contains specific styles such as sentiment .
Recent studies have used artificial tokens , variational auto - encoders , or adversarial training to separate the content and style on the encoder side .
On the decoder side , conditional language modeling has been used to generate output sentences with the target style .
In addition , output length control with conditional language modeling has been well studied .
Our style - controllable RC relies on conditional language modeling in the decoder .
Multi - passage RC .
The simplest approach is to concatenate the passages and find the answer from the concatenation , as in .
Earlier pipelined models found a small number of relevant passages with a TF - IDF based ranker and passed them to a neural reader , while more recent models have used a neural re-ranker to more accurately select the relevant passages .
Also , non-pipelined models ( including ours ) consider all the provided passages and find the answer by comparing scores between passages .
The most recent models make a proper trade - off between efficiency and accuracy .
RC with unanswerable question identification .
The previous work of ) outputted a no-answer score depending on the probability of all answer spans .
proposed an answer verifier to compare an answer with the question .
jointly learned an RC model and an answer verifier .
Our model introduces a classifier on top of the question - passages reader , which is not dependent on the generated answer .
Abstractive summarization .
Current state - of the - art models use the pointer - generator mechanism .
In particular , content selection approaches , which decide what to summarize , have recently been used with abstractive models .
Most methods select content at the sentence level or the word level .
Our model incorporates content selection at the passage level in the combined attention .
Query - based summarization has rarely been studied because of alack of datasets .
Nema et al .
Conclusion
This study sheds light on multi-style generative RC .
Our proposed model , Masque , is based on multi-source abstractive summarization and learns multi-style answers together .
It achieved stateof - the - art performance on the Q&A task and the Q & A + NLG task of MS MARCO 2.1 and the summary task of Narrative QA .
The key to its success is transferring the style - independent NLG capability to the target style by use of the question - passages reader and the conditional pointer - generator decoder .
In particular , the capability of copying words from the question and passages can be shared among the styles , while the capability of controlling the mixture weights for the generative and copy distributions can be acquired for each style .
Our future work will involve exploring the potential of our multi-style learning towards natural language understanding .
A.2 Experimental Setup for NarrativeQA
Model configurations .
Our best model was jointly trained with the NarrativeQA and MS MARCO NLG datasets for a total of seven epochs with a batch size of 64 , where each batch consisted of multi-style answers that were randomly sampled .
For efficient multi-style learning , each summary in the Narrative QA dataset was divided into ten passages ( size of 130 words ) with sentencelevel overlaps such that each sentence in the summary was entirely contained in a passage .
Each passage from MS MARCO was also truncated to 130 words .
The rest of the configuration was the same as in the MS MARCO experiments .
Evaluation settings .
An official evaluation script is not provided , so we used the evaluation script created by .
The answers were normalized by making words lowercase and removing punctuation marks .
A.3 Output Examples Generated by Masque
Tables 6 and 7 list the generated examples for questions from MS MARCO 2.1 and Narra - tive QA , respectively .
We can see from the examples that our model could control answer styles appropriately for various question and reasoning types .
We did find some important errors : style errors , yes / no classification errors , copy errors with respect to numerical values , grammatical errors , and multi-hop reasoning errors . :
Output examples generated by Masque from MS MARCO .
The model was trained with the Q&A and NLG styles .
The relevant passage is one that an annotator selected to compose the reference answer .
The model could control answer styles appropriately for ( a ) natural language , ( b ) cloze - style , and ( c ) keywords questions .
( d ) The answer style was incorrect .
( e ) The answers were not consistent between the styles .
( f ) Copying from numerical words worked poorly .
There were some grammatical errors in the generative answers , which are underlined . :
Output examples generated by Masque from Narrative QA .
The model was trained with the Narrative QA ( NQA ) and MS MARCO ( NLG ) styles .
It could control answer styles appropriately for questions that required single - sentence reasoning and ( c ) multi-sentence reasoning .
( d ) Example of an error in multi-sentence reasoning .
There were some grammatical errors in the generative answers , which are underlined .
