{
  "has" : {
    "Experimental setup" : {
      "as" : {
        "optimization objective" : {
          "use" : {
            "cross-entropy loss" : {
              "plus" : "L2 regularization penalty",
              "has" : {
                "minimize" : {
                  "by" : "Adadelta",
                  "with" : {
                    "batch size" : {
                      "of" : "64"
                    }
                  }
                }
              }
            }
          },
          "from sentence" : "We use cross-entropy loss plus L2 regularization penalty as optimization objective .
We minimize it by Adadelta ) ( an optimizer of mini - batch SGD ) with batch size of 64 ."

        }
      },
      "has" : {
        "Initial learning rate" : {
          "set to" : "0.5",
          "from sentence" : "Initial learning rate is set to 0.5 ."
        },
        "weight matrices" : {
          "initialized by" : "Glorot Initialization"
        },
        "biases" : {
          "initialized with" : "0",
          "from sentence" : "All weight matrices are initialized by Glorot Initialization , and the biases are initialized with 0 ."
        },
        "Out - of - Vocabulary words" : {
          "in" : "training set",
          "has" : {
            "randomly initialized" : {
              "by" : {
                "uniform distribution" : {
                  "between" : "( ? 0.05 , 0.05 )"
                }
              }
            }
          },
          "from sentence" : "The Out - of - Vocabulary words in training set are randomly initialized by uniform distribution between ( ? 0.05 , 0.05 ) ."
        },
        "L2 regularization decay factors" : {
          "are" : {
            "5 10 ?5 and 10 ? 4" : {
              "for" : "language inference and sentiment analysis"
            }
          },
          "from sentence" : "The L2 regularization decay factors ? are 5 10 ?5 and 10 ? 4 for language inference and sentiment analysis , respectively ."
        },
        "Hidden units number" : {
          "has" : {
            "d h" : {
              "set to" : "300"
            }
          },
          "from sentence" : "Hidden units number d h is set to 300 ."
        },
        "Activation functions" : {
          "are" : "ELU ( exponential linear unit )",
          "from sentence" : "Activation functions ? ( ) are ELU ( exponential linear unit ) ( Clevert , Unterthiner , and Hochreiter 2016 ) if not specified ."
        }
      },
      "initialize" : {
        "word embedding" : {
          "in" : "x",
          "by" : "300D Glo Ve 6B pre-trained vectors",
          "from sentence" : "We initialize the word embedding in x by 300D Glo Ve 6B pre-trained vectors ."
        }
      },
      "use" : {
        "Dropout" : {
          "with" : {
            "keep probability" : {
              "has" : {
                "0.75" : {
                  "for" : "language inference"
                },
                "0.8" : {
                  "for" : "sentiment analysis"
                }
              }
            }
          },
          "from sentence" : "We use Dropout ) with keep probability 0.75 for language inference and 0.8 for sentiment analysis ."
        }
      },
      "implemented with" : "TensorFlow 2",
      "run on" : ["Nvidia GTX 1080 Ti graphic card", {"from sentence" : "All models are implemented with TensorFlow 2 and run on sin - 3.0 m 83.9 80.6 1024D GRU encoders 15 m 98.8 81.4 300D Tree - based CNN encoders 3.5 m 83.3 82.1 300D SPINN - PI encoders 3.7 m 89.2 83.2 600D Bi- LSTM encoders 2.0 m 86.4 83.3 300D NTI - SLSTM - LSTM encoders 4.0 m 82.5 83.4 600D Bi-LSTM encoders+intra-attention 2.8 m 84.5 84.2 300D NSE encoders 3 gle Nvidia GTX 1080 Ti graphic card ."

      }]
    }
  }
}