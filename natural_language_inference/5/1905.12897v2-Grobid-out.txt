title
A Compare-Aggregate Model with Latent Clustering for Answer Selection
abstract
In this paper, we propose a novel method for a sentence-level answer-selection task that is a fundamental problem in natural language processing. First, we explore the effect of additional information by adopting a pretrained language model to compute the vector representation of the input text and by applying transfer learning from a large-scale corpus. Second, we enhance the compare-aggregate model by proposing a novel latent clustering method to compute additional information within the target corpus and by changing the objective function from listwise to pointwise. To evaluate the performance of the proposed approaches, experiments are performed with the WikiQA and TREC-QA datasets. The empirical results demonstrate the superiority of our proposed approach, which achieve state-of-the-art performance for both datasets.
INTRODUCTION
Automatic question answering (QA) is a primary objective of artificial intelligence. Recently, research on this task has taken two major directions based on the answer span considered by the model. The first direction (i.e., the fine-grained approach) finds an exact answer to a question within a given passage. The second direction (i.e., the coarse-level approach) is an information retrieval (IR)-based approach that provides the most relevant sentence from a given document in response to a question. In this study, we are interested in building a model that computes a matching score between two text inputs. In particular, our model is designed to undertake an answer-selection task that chooses the sentence that is most relevant to the question from a list of answer candidates. This task has been extensively investigated by researchers because it is a fundamental task that can be applied to other QA-related tasks.
However, most previous answer-selection studies have employed small datasets compared with the large datasets employed for other natural language processing (NLP) tasks. Therefore, * Work conducted while the author was an intern at Adobe Research. the exploration of sophisticated deep learning models for this task is difficult.
To fill this gap, we conduct an intensive investigation with the following directions to obtain the best performance in the answerselection task. First, we explore the effect of additional information by adopting a pretrained language model (LM) to compute the vector representation of the input text. Recent studies have shown that replacing the word-embedding layer with a pretrained language model helps the model capture the contextual meaning of words in the sentence. Following this study, we select an ELMo language model for this study. We investigate the applicability of transfer learning (TL) using a large-scale corpus that is created for a relevant-sentence-selection task (i.e., question-answering NLI (QNLI) dataset). Second, we further enhance one of the baseline models, Comp-Clip (refer to the discussion in 3.1), for the target QA task by proposing a novel latent clustering (LC) method. The LC method computes latent cluster information for target samples by creating a latent memory space and calculating the similarity between the sample and the memory. By an endto-end learning process with the answer-selection task, the LC method assigns true-label question-answer pairs to similar clusters. In this manner, a model will have further information for matching sentence pairs, which increases the total model performance. Last, we explore the effect of different objective functions (listwise and pointwise learning). In contrast to previous research, we observe that the pointwise learning approach performs better than the listwise learning approach when we apply our proposed methods. Extensive experiments are conducted to investigate the efficacy and properties of the proposed methods and show the superiority of our proposed approaches for achieving state-of-the-art performance with the WikiQA and TREC-QA datasets.

RELATED WORK
Researchers have investigated models based on neural networks for question-answering tasks. One study employs a Siamese architecture that utilizes an encoder (e.g., RNN or CNN) to compute vector representations of the question and the answer. The affinity score is calculated based on these vector representations. To improve the model performance by enabling the use of information from one sentence (e.g., a question or an answer) in computing the representation of another sentence, researchers included the attention mechanism in their models. 1:: The architecture of the model. The dotted box on the right shows the process through which the latent-cluster information is computed and added to the answer. This process is also performed in the question part but is omitted in the figure. The latent memory is shared in both processes.
Another line of research includes the compare-aggregate framework. In this framework, first, vector representations of each sentence are computed. Second, these representations are compared. Last, the results are aggregated to calculate the matching score between the question and the answer.
In this study, unlike the previous research, we employ a pretrained language model and a latent-cluster method to help the model understand the information in the question and the answer.

METHODS

Comp-Clip Model
In this paper, we are interested in estimating the matching score f (y|Q, A), where y, Q = {q 1 , ..., q n } and A = {a 1 , ..., am } represent the label, the question and the answer, respectfully. We select the model from, which is referred to as the Comp-Clip model, as our baseline model. The model consists of the following four parts:
Context representation: The question Q ? Rd ?Q and answer A ? Rd ?A , (where dis a dimensionality of word embedding and Q and A are the length of the sequence in Q and A, respectively), are processed to capture the contextual information and the word as follows:
where ? denotes element-wise multiplication, and ? is the sigmoid function. The W ? R l ?d is the learned model parameter.
Attention: The soft alignment of each element in Q ? R l ?Q and A ? R l ?A are calculated using dynamic-clip attention. We obtain the corresponding vectors HQ ? R l ?A and H A ? R l ?Q .
(2)
Comparison: A comparison function is used to match each word in the question and answer to a corresponding attention-applied vector representation:
where ? denotes element-wise multiplication.
Aggregation: We aggregate the vectors from the comparison layer using CNN with n-types of filters and calculate the matching score between Q and A.

Proposed Approaches
To achieve the best performance in the answer-selection task, we propose four approaches: adding a pretrained LM; adding the LC information of each sentence as auxiliary knowledge; applying TL to benefit from large-scale data; and modifying the objective function from listwise to pointwise learning. depicts the total architecture of the proposed model.

Pretrained Language Model (LM):
Recent studies have shown that replacing the word embedding layer with a pretrained LM helps the model capture the contextual meaning of the words in the sentence. We select an ELMo language model and replace the previous word embedding layer with the ELMo model as follows: L Q = ELMo(Q), LA = ELMo(A). These new representations-L Q and LA -are substituted for Q and A, respectively, in equation.
Latent Clustering (LC) Method: We assume that extracting the LC information of the text and using it as auxiliary information will help the neural network model analyze the corpus. The dotted box in shows the proposed LC method. We create n-many latent memory vectors M 1:n and calculate the similarity between the sentence representation and each latent memory vector. The latentcluster information of the sentence representation will be obtained using a weighted sum of the latent memory vectors according to the calculated similarity as follows:
where s ? Rd is a sentence representation, M 1:n ? Rd ? ?n indicates the latent memory, and W? Rd ?d ? is the learned model parameter. We apply the LC method and extract cluster information from each question and answer. This additional information is added to each of the final representations in the comparison part (see 3.1) as follows:
where f is the LC method (in equation 5) and [;] denotes the concatenation of each vector. These new representations-C Q new and CA new -are substituted for C Q and CA in equation. Note that we average word-embedding to obtain sentence representation in the previous equation.
Transfer Learning (TL): To observe the efficacy in a large dataset, we apply transfer learning using the question-answering NLI (QNLI) corpus. We train the CompClip model with the QNLI corpus and then fine-tune the model with target corpora, such as the Wik-iQA and TREC-QA datasets.
Pointwise Learning to Rank: Previous research adopts a listwise learning approach. With a dataset that consists of a question, Q, a related answer set, A = {A 1 , ..., A N }, and a target label, y = {y 1 , ..., y N }, a matching score is computed using equation. This approach applies KL-divergence loss to train the model as follows: score i = model(Q, A i ), S = softmax([score 1 , ..., score i ]), loss = N n=1 KL(S n ||y n ),
where i is the number of answer candidates for the given question and N is the total number of samples employed during training. In contrast, we pair each answer candidate to the question and compute the cross-entropy loss to train the model as follows: loss = ? N n=1 y n log (score n ),

EMPIRICAL RESULTS
We regard all tasks as relevant answer selections for the given questions. Following the previous study, we report the model performance as the mean average precision (MAP) and the mean reciprocal rank (MRR) . To test the performance of the model, we utilize the TREC-QA, WikiQA and QNLI datasets].

Dataset
WikiQA is an answer selection QA dataset constructed from real queries of Bing and Wikipedia. Following the literature, we use only questions that contain at least one correct answer among the list of answer candidates.

Implementation Details
To implement the Comp-Clip model, we apply a context projection weight matrix with 100 dimensions thatare shared between the question part and the answer part (eq. 1). In the aggregation part, we use 1-D CNN with a total of 500 filters, which involves five types of filters K ? R {1,2,3,4,5}?100 , 100 per type. This CNN is independently applied to the question part and answer part. For the LC method, we perform additional hyper-parameter searching 0.743* 0.699* 0.754* 0.708* ----Comp-Clip (2017) 0.732* 0.718* 0.738* 0.732* -0.821 -0.899 IWAN (2017) 0.738* 0.692* 0.749* 0.705* -0.822 -0.899 IWAN + sCARNN (2018) 0.719* 0.716* 0.729* 0.722* -0.829 -0.875 MCAN (2018) - experiments to select the best parameters. We select k (for the kmax-pool in equation 5) as 6 and 4 for the WikiQA and TREC-QA case, respectively. In both datasets, we apply 8 latent clusters. The vocabulary size in the WiKiQA, TREC-QA and QNLI dataset are 30,104, 56,908 and 154,442, respectively. When applying the TL, the vocabulary size is set to 154,442, and the dimension of the context projection weight matrix is set to 300. We use the Adam optimizer, including gradient clipping, by the norm at a threshold of 5. For the purpose of regularization, we applied a dropout with a ratio of 0.5.. Unlike previous studies, we report our results for both the dev dataset and the test dataset because we note a performance gap between these datasets. While training the model, we apply an early stop that is based on the performance of the dev dataset and measure the performance on the test dataset. Because Comp-Clip [1] is our baseline model, we implement it from scratch and achieve a performance that is similar to that of the original paper.

Comparison with Other Methods
WikiQA: For the WikiQA dataset, the pointwise learning approach shows a better performance than the listwise learning approach. We combine LM with the base model (Comp-Clip +LM) and observe a significant improvement in performance in terms of MAP (0.714 to 0.746 absolute). When we add the LC method (Comp-Clip +LM +LC), the best previous results are surpassed in terms of MAP (0.718 to 0.764 absolute). We achieve avast improvement in performance

TREC-QA:
The pointwise learning approach also shows excellent performance with the TREC-QA dataset. As shown in table 1, the TREC-QA dataset has a larger number of answer candidates per question. We assume that this characteristic prevents the model from handling the dataset with a listwise learning approach. As in the WikiQA case, we achieve additional performance gains in terms of the MAP as we apply LM, LC, and TL (0.850, 0.868 and 0.875, respectively). In particular, our model outperforms the best previous result when we add LC method, (Comp-Clip +LM +LC) in terms of MAP (0.865 to 0.868).

Impact of Latent Clustering
To evaluate the impact of latent clustering method (Comp-Clip +LM +LC) in a larger dataset environment, we perform QNLI evaluation. shows the performance of the model (Comp-Clip +LM +LC) for the QNLI dataset with a variant number of clusters.
Note that the QNLI dataset is created from the SQuAD dataset, which only provides train and dev subsets. Consequently, we report the model performances for the dev dataset. As shown in the table, we achieve the best results with 8 clusters in listwise learning and 16 clusters in pointwise learning. In both cases, we achieve no additional performance gain after 16 clusters.

CONCLUSION
In this study, our proposed method achieves state-of-the-art performance for both the WikiQA dataset and TREC-QA dataset. We show that leveraging a large amount of data is crucial for capturing the contextual representation of input text. In addition, we show that the proposed latent clustering method with a pointwise objective function significantly improves the model performance in the sentence-level QA task.