204	0	11	In terms of
204	12	30	prediction quality
204	63	85	unselected head tokens
204	89	102	contribute to
204	107	117	prediction
204	120	128	bringing
204	129	146	0.2 % improvement
204	153	158	using
204	159	179	separate RSS modules
204	180	189	to select
204	194	219	head and dependent tokens
204	220	228	improves
204	229	237	accuracy
204	238	240	by
204	241	246	0.5 %
204	257	305	hard attention and soft self - attention modules
204	306	313	improve
204	318	326	accuracy
204	327	329	by
204	330	345	0.3 % and 2.9 %
44	42	96	https://github.com/ taoshen58/DiSAN /tree/master/ReSAN
177	4	15	experiments
177	20	32	conducted in
177	33	39	Python
177	40	44	with
177	45	55	Tensorflow
177	60	66	run on
177	69	87	Nvidia GTX 1080 Ti
178	3	6	use
178	7	15	Adadelta
178	16	18	as
178	19	28	optimizer
178	37	45	performs
178	46	57	more stable
178	58	62	than
178	63	67	Adam
178	68	70	on
178	71	76	ReSAN
180	7	41	300D Glo Ve 6B pre-trained vectors
185	0	26	Natural Language Inference
193	0	11	Compared to
193	16	49	methods from official leaderboard
193	52	57	ReSAN
193	114	122	achieves
193	127	145	best test accuracy
193	58	69	outperforms
193	70	109	all the sentence encoding based methods
196	14	19	ReSAN
196	25	36	outperforms
196	41	65	300D SPINN - PI encoders
196	66	68	by
196	69	74	3.1 %
194	15	26	compared to
194	31	47	last best models
194	50	54	i.e.
194	57	86	600D Gumbel TreeLSTM encoders
194	91	121	600D Residual stacked encoders
194	124	129	ReSAN
194	130	134	uses
194	135	155	far fewer parameters
194	156	160	with
194	161	179	better performance
198	0	11	Compared to
198	16	32	recurrent models
198	35	39	e.g.
198	42	51	Bi - LSTM
198	56	64	Bi - GRU
198	138	144	due to
198	145	172	parallelizable computations
198	69	74	ReSAN
198	75	80	shows
198	81	106	better prediction quality
198	111	137	more compelling efficiency
199	16	36	convolutional models
199	39	43	i.e.
199	46	61	Multiwindow CNN
199	66	82	Hierarchical CNN
199	87	92	ReSAN
199	93	118	significantly outperforms
199	124	126	by
199	127	142	3.1 % and 2.4 %
200	16	40	attention - based models
200	43	63	multi-head attention
200	68	73	DiSAN
200	76	81	ReSAN
200	82	86	uses
200	89	117	similar number of parameters
200	118	122	with
200	123	146	better test performance
200	151	165	less time cost
36	25	32	propose
36	35	65	novel hard attention mechanism
36	66	72	called
36	75	111	reinforced sequence sampling ( RSS )
36	122	129	selects
36	130	136	tokens
36	137	141	from
36	145	159	input sequence
36	160	162	in
36	163	171	parallel
36	216	218	is
36	219	240	highly parallelizable
36	241	248	without
36	249	272	any recurrent structure
37	8	15	develop
37	28	64	reinforced self - attention ( ReSA )
37	85	93	combines
37	98	101	RSS
37	102	106	with
37	109	130	soft self - attention
38	0	2	In
38	3	7	ReSA
38	10	36	two parameter - untied RSS
38	54	64	applied to
38	65	75	two copies
38	76	78	of
38	83	97	input sequence
39	0	5	Re SA
39	11	17	models
39	22	41	sparse dependencies
39	42	49	between
39	54	79	head and dependent tokens
39	80	91	selected by
39	96	111	two RSS modules
40	13	18	build
40	22	47	sentence - encoding model
40	52	97	reinforced self - attention network ( ReSAN )
40	102	110	based on
40	111	115	ReSA
40	116	123	without
40	124	147	any CNN / RNN structure
14	44	64	attention mechanisms
