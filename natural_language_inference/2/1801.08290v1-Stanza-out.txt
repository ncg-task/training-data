title
A Question - Focused Multi- Factor Attention Network for Question Answering
abstract
Neural network models recently proposed for question answering ( QA ) primarily focus on capturing the passagequestion relation .
However , they have minimal capability to link relevant facts distributed across multiple sentences which is crucial in achieving deeper understanding , such as performing multi-sentence reasoning , co-reference resolution , etc .
They also do not explicitly focus on the question and answer type which often plays a critical role in QA .
In this paper , we propose a novel end - to - end question - focused multi-factor attention network for answer extraction .
Multi-factor attentive encoding using tensor - based transformation aggregates meaningful facts even when they are located in multiple sentences .
To implicitly infer the answer type , we also propose a max-attentional question aggregation mechanism to encode a question vector based on the important words in a question .
During prediction , we incorporate sequence - level encoding of the first wh-word and its immediately following word as an additional source of question type information .
Our proposed model achieves significant improvements over the best prior state - of - the - art results on three large - scale challenging QA datasets , namely NewsQA , TriviaQA , and Search QA .
Introduction
In machine comprehension - based ( MC ) question answering ( QA ) , a machine is expected to provide an answer for a given question by understanding texts .
In recent years , several MC datasets have been released .
released a multiple - choice question answering dataset .
created a large cloze - style dataset using CNN and Daily Mail news articles .
Several models ) based on neural attentional and pointer networks ( Vinyals , Fortunato , and Jaitly 2015 ) have been proposed since then .
released the SQuAD dataset where the answers are freeform unlike in the previous MC datasets .
Most of the previously released datasets are closed - world , i.e. , the questions and answers are formulated given the text passages .
As such , the answer spans can often be extracted by simple word and context matching .
Copyright c 2018 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .
All rights reserved .
attempted to alleviate this issue by proposing the News QA dataset where the questions are formed only using the CNN article summaries without accessing the full text .
As a result , a significant proportion of questions require reasoning beyond simple word matching .
Two even more challenging open - world QA datasets , TriviaQA and SearchQA , have recently been released .
Trivia
QA consists of question - answer pairs authored by trivia enthusiasts and independently gathered evidence documents from Wikipedia as well as Bing Web search .
In SearchQA , the question - answer pairs are crawled from the Jeopardy archive and are augmented with text snippets retrieved from Google search .
Recently , many neural models have been proposed , which mostly focus on passage - question interaction to capture the context similarity for extracting a text span as the answer .
However , most of the models do not focus on synthesizing evidence from multiple sentences and fail to perform well on challenging open - world QA tasks such as New s QA and Triv - ia QA .
Moreover , none of the models explicitly focus on question / answer type information for predicting the answer .
In practice , fine - grained understanding of question / answer type plays an important role in QA .
In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .
Intuitively , AMANDA extracts the answer not only by synthesizing relevant facts from the passage but also by implicitly determining the suitable answer type during prediction .
The key contributions of this paper are :
We propose a multi-factor attentive encoding approach based on tensor transformation to synthesize meaningful evidence across multiple sentences .
It is particularly effective when answering a question requires deeper understanding such as multi-sentence reasoning , co-reference resolution , etc .
To subsume fine - grained answer type information , we propose a max- attentional question aggregation mecha - nism which learns to identify the meaningful portions of a question .
We also incorporate sequence - level representations of the first wh-word and its immediately following word in a question as an additional source of question type information .
Problem Definition
Given a pair of passage and question , an MC system needs to extract a text span from the passage as the answer .
We formulate the answer as two pointers in the passage , which represent the beginning and ending tokens of the answer .
Let P be a passage with tokens ( P 1 , P 2 , . . . , PT ) and Q be a question with tokens ( Q 1 , Q 2 , . . . , Q U ) , where T and U are the length of the passage and question respectively .
To answer the question , a system needs to determine two pointers in the passage , band e , such that 1 ? b ? e ? T .
The resulting answer tokens will be ( P b , P b+1 , . . . , P e ) .
Network Architecture
The architecture of the proposed question - focused multifactor attention network 1 is given in .
Word - level Embedding
Word - level embeddings are formed by two components : pre-trained word embedding vectors from GloVe ( Pennington , Socher , and Manning 2014 ) and convolutional neural network - based ( CNN ) character embeddings .
Character embeddings have proven to be very useful for outof - vocabulary ( OOV ) words .
We use a character - level CNN followed by max - pooling over an entire word to get the embedding vector for each word .
Prior to that , a character - based lookup table is used to generate the embedding for every character and the lookup table weights are learned during training .
We concatenate these two embedding vectors for every word to generate word - level embeddings .
Sequence - level Encoding
We apply sequence - level encoding to incorporate contextual information .
Let e pt and e qt be the tth embedding vectors of the passage and the question respectively .
The embedding vectors are fed to a bi-directional LSTM ( BiLSTM ) .
Considering that the outputs of the BiLSTMs are unfolded across time , we represent the outputs as P ? R T H and Q ?
R U H for passage and question respectively .
H is the number of hidden units for the BiLSTMs .
At every time step , the hidden unit representation of the BiLSTMs is obtained by concatenating the hidden unit representations of the corresponding forward and backward LSTMs .
For the passage , at time step t , the forward and backward LSTM hidden unit representations can be written as :
The tth row of P is represented as
where || represents the concatenation of two vectors .
Similarly , the sequence level encoding for a question is
where qt is the tth row of Q .
Cartesian Similarity - based Attention Layer
The attention matrix is calculated by taking dot products between all possible combinations of sequence - level encoding vectors for a passage and a question .
Note that for calculating the attention matrix , we do not introduce any additional learnable parameters .
The attention matrix A ? R T U can be expressed as :
Intuitively , A i , j is a measure of the similarity between the sequence - level encoding vectors of the ith passage word and the jth question word .
Question - dependent
Passage Encoding
In this step , we jointly encode the passage and question .
We apply a row - wise softmax function on the attention matrix :
If rt ? R U is the tth row of R ? R T U , then U j=1 r t , j = 1 . Each row of R measures how relevant every question word is with respect to a given passage word .
Next , an aggregated question vector is computed corresponding to each sequence - level passage word encoding vector .
The aggregated question vector gt ?
R H corresponding to the tth passage word is computed as gt = rt Q .
The aggregated question vectors corresponding to all the passage words can be computed as
The aggregated question vectors corresponding to the passage words are then concatenated with the sequence - level passage word encoding vectors .
If the question - dependent passage encoding is denoted as S ?
R T 2 H and st is the tth row of S , then st = ct | | gt , where ct is the sequencelevel encoding vector of the tth passage word ( tth row of P ) .
Then a BiLSTM is applied on S to obtain V ?
R T H .
Multi- factor Attentive Encoding
Tensor - based neural network approaches have been used in a variety of natural language processing tasks .
We propose a multi-factor attentive encoding approach using tensor - based transformation .
In practice , recurrent neural networks fail to remember information when the context is long .
Our proposed multi-factor attentive encoding approach helps to aggregate meaningful information from along context with fine - grained inference due to the use of multiple factors while calculating attention .
Let vi ?
R H and v j ?
R H represent the questiondependent passage vectors of the ith and jth word , i.e. , the ith and jth row of V. Tensor - based transformation for multifactor attention is formulated as follows :
where W
?
R HmH is a 3 - way tensor and m is the number of factors .
The output of the tensor product f m i , j ?
R m is a vector where each element f m i , j ,k is a result of the bilinear form defined by each tensor slice W
? i , j ? [ 1 , T ] , the multi-factor attention tensor can be given as F [ 1 :m ]
?
R mT T .
For every vector f m i , j of F [ 1 : m ] , we perform a max pooling operation over all the elements to obtain the resulting attention value :
where F i , j represents the element in the ith row and jth column of F ? R T T .
Each row of F measures how relevant every passage word is with respect to a given questiondependent passage encoding of a word .
We apply a row - wise softmax function on F to normalize the attention weights , obtaining F ? R T T .
Next , an aggregated multi-factor attentive encoding vector is computed corresponding to each question - dependent passage word encoding vector .
The aggregated vectors corresponding to all the passage words , M ? R T H , can be given as M =F
V .
The aggregated multi-factor attentive encoding vectors are concatenated with the question - dependent passage word encoding vectors to obtain M ? R T 2 H .
To control the impact of M , we apply a feed - forward neural network - based gating method to obtain Y ? R T 2 H .
If the tth row of M ism t , then the tth row of Y is :
where represents element - wise multiplication .
W g ? R 2 H2H and b g ?
R 2H are the transformation matrix and bias vector respectively .
We use another pair of stacked BiLSTMs on top of Y to determine the beginning and ending pointers .
Let the hidden unit representations of these two BiLSTMs be B ? R T H and E ? R T H .
To incorporate the dependency of the ending pointer on the beginning pointer , the hidden unit representation of B is used as input to E.
Question - focused
Attentional Pointing
Unlike previous approaches , our proposed model does not predict the answer pointers directly from contextual passage encoding or use another decoder for generating the pointers .
We formulate a question representation based on two parts :
max - attentional question aggregation ( q ma )
question type representation ( q f ) q ma is formulated by using the attention matrix A and the sequence - level question encoding Q .
We apply a maxcol operation on A which forms a row vector whose elements are the maximum of the corresponding columns of A .
We define k ?
R U as the normalized max - attentional weights :
where softmax is used for normalization .
The maxattentional question representation q ma ?
R H is :
Intuitively , q ma aggregates the most relevant parts of the question with respect to all the words in the passage .
q f is the vector concatenation of the representations of the first wh-word and its following word from the sequencelevel question encoding Q .
The set of wh-words we used is { what , who , how , when , which , where , why } .
If qt wh and qt wh + 1 represent the first wh-word and its following word ( i.e. , the t wh th and ( t wh + 1 ) th rows of Q ) , q f ?
R 2H is expressed as :
The final question representationq ?
R H is expressed as :
where W q ?
R 3 HH and b q ? R H are the weight matrix and bias vector respectively .
If no wh-word is present in a question , we use the first two sequence - level question word representations for calculatingq .
We measure the similarity betweenq and the contextual encoding vectors in B and E to determine the beginning and ending answer pointers .
Corresponding similarity vectors s b ?
R T and s e ?
R T are computed as : The probability distributions for the beginning pointer band the ending pointer e for a given passage P and a question Q can be given as :
The joint probability distribution for obtaining the answer a is given as :
To train our model , we minimize the cross entropy loss :
summing over all training instances .
During prediction , we select the locations in the passage for which the product of Pr ( b ) and Pr ( e ) is maximum keeping the constraint 1 ? b ? e ? T .
Visualization
To understand how the proposed model works , for the example given in , we visualize the normalized multifactor attention weights
F and the attention weights k which are used for max-attentional question aggregation .
In , a small portion of F has been shown , in which the answer words Robert and Park are both assigned higher weights when paired with the context word Korean - American .
Due to the use of multi-factor attention , the answer segment pays more attention to the important keyword although it is quite far in the context passage and thus effectively infers the correct answer by deeper understanding .
In , it is clear that the important question word name is getting a higher weight than the other question words .
This helps to infer the fine - grained answer type during prediction , i.e. , a person 's name in this example .
Experiments
We evaluated AMANDA on three challenging QA datasets : NewsQA , TriviaQA , and Search QA .
Using the News QA development set as a benchmark , we perform rigorous analysis for better understanding of how our proposed model works .
Datasets
The News QA dataset consists of around 100K answerable questions in total .
Similar to ; Weissenborn , Wiese , and Seiffe 2017 ) , we do not consider the unanswerable questions in our experiments .
News
QA is more challenging compared to the previously released datasets as a significant proportion of questions requires reasoning beyond simple word - and contextmatching .
This is due to the fact that the questions in News QA were formulated only based on summaries without accessing the main text of the articles .
Moreover , News
QA passages are significantly longer ( average length of 616 words ) and cover a wider range of topics .
Trivia
QA consists of question - answer pairs authored by trivia enthusiasts and independently gath - SearchQA is also constructed to more closely reflect IR - style QA .
They first collected existing question - answer pairs from a Jeopardy archive and augmented them with text snippets retrieved by Google .
One difference with TriviaQA is that the evidence passages in Search QA are Google snippets instead of Wikipedia or Web search documents .
This makes reasoning more challenging as the snippets are often very noisy .
Search
QA consists of 140,461 question - answer pairs , where each pair has 49.6 snippets on average and each snippet has 37.3 tokens on average .
Experimental Settings
We tokenize the corpora with NLTK 2 .
We use the 300 dimension pre-trained word vectors from GloVe ( Pennington , Socher , and Manning 2014 ) and we do not update them during training .
The out - of - vocabulary words are initialized with zero vectors .
We use 50 - dimension character - level embedding vectors .
The number of hidden units in all the LSTMs is 150 .
We use dropout ) with probability 0.3 for every learnable layer .
For multi-factor attentive encoding , we choose 4 factors ( m ) based on our experimental findings ( refer to ) .
During training , the minibatch size is fixed at 60 .
We use the Adam optimizer with learning rate of 0.001 and clipnorm of 5 .
During testing , we enforce the constraint that the ending pointer will always be equal to or greater than the beginning pointer .
We use exact match ( EM ) and F 1 scores as the evaluation metrics .
shows that AMANDA outperforms all the stateof - the - art models by a significant margin on the New s QA dataset .
shows the results on the TriviaQA dataset .
In , the model named Classifier based on feature engineering was proposed by .
They also reported the performance of BiDAF .
A memory network - based approach , MEMEN , was recently proposed by .
Note that in the Wikipedia domain , we choose the answer which provides the highest maximum joint probability ( according to Eq. ( 14 ) ) for any document .
shows that AMANDA achieves state - of the - art results in both Wikipedia and Web domain on distantly supervised and verified data .
Results
Results on the Search QA dataset are shown in .
In addition to a TF - IDF approach , modified and reported the performance of attention sum reader ( ASR ) which was originally proposed by .
We consider a maximum of 150 words surrounding the answer from the concatenated ranked list of snippets as a passage to more quickly train the model and to reduce the amount of noisy information .
During prediction , we choose the first 200 words ( about 5 snippets ) from the concatenated ranked list of snippets as an evidence passage .
These are chosen based on performance on the development set .
Based on question patterns , question types are always represented by the first two sequence - level representations of question words .
To make the results comparable , we also report accuracy for single - word - answer ( unigram ) questions and F 1 score for multi-word - answer ( n- gram ) questions .
AMANDA outperforms both systems , especially for multi-word - answer questions by a huge margin .
This indicates that AMANDA can learn to make inference reasonably well even if the evidence passages are noisy .
shows that AMANDA performs better than any of the ablated models which include the ablation of multifactor attentive encoding , max - attentional question aggregation ( q ma ) , and question type representation ( q f ) .
We also perform statistical significance test using paired t- test and bootstrap resampling .
Performance of AMANDA ( both in terms of EM and F1 ) is significantly better ( p < 0.01 ) than the ablated models .
One of the key contributions of this paper is multi-factor attentive encoding which aggregates information from the relevant passage words by using a tensor - based attention mechanism .
The use of multiple factors helps to fine - tune answer inference by synthesizing information distributed across multiple sentences .
The number of factors is the granularity to which the model is allowed to refine the evidence .
The effect of multi-factor attentive encoding is illustrated by the following example taken from the News QA development set :
What will allow storage on remote servers ?
... The iCloud service will now be integrated into the iOS 5 operating system .
It will work with apps and allow content to be stored on remote servers instead of the users ' i Pod , i Phone or other device ...
When multi-factor attentive encoding is ablated , the model could not figure out the cross - sentence co-reference and wrongly predicted the answer as apps .
On the contrary , with multi-factor attentive encoding , AMANDA could correctly infer the answer as iCloud service .
Effectiveness of the Model Components
Another contribution of this work is to include the question focus during prediction .
It is performed by adding two components : q ma ( max - attentional question aggregation ) and q f ( question type representation ) .
q ma and q f implicitly infer the answer type during prediction by focusing on the important question words .
Impact of the question focus components is illustrated by the following example taken from the New s QA development set : who speaks on Holocaust remembrance day ?
... Israel 's vice prime minister Silvan Shalom said Tuesday " Israel can never ... people just 65 years ago " ...
He was speaking as Israel observes its Holocaust memorial day , remembering the roughly ...
Without the q ma and q f components , the answer was wrongly predicted as Israel , whereas with q ma and q f , AMANDA could correctly infer the answer type ( i.e. , a person 's name ) and predict Silvan Shalom as the answer .
Ablation studies of other components such as character embedding , question - dependent passage encoding , and the second LSTM during prediction are given in .
When the second LSTM ( E ) is ablated , a feed - forward layer is used instead .
shows that question - dependent passage encoding has the highest impact on performance .
Variation on the number of factors ( m ) and q ma shows the performance of AMANDA for different values of m .
We use 4 factors for all the experiments as it gives the highest F1 score .
Note that m = 1 is equivalent to standard bilinear attention .
shows the variation of question aggregation formulation .
For mean aggregation , the attentional weight vector k is formulated by applying column - wise averaging on the attention matrix A .
Intuitively , it is giving equal priority to all the passage words to determine a particular question word attention .
Similarly , in the case of sum aggregation , we apply a column - wise sum operation .
shows that the best performance is obtained when q ma is obtained with a column - wise maximum operation on A .
Effectively , it is helping to give higher weights to the more important question words based on the most relevant passage words . :
Examples of different error types and their percentages .
Ground truth answers are bold - faced and predicted answers are underlined .
Quantitative Error Analysis
We analyzed the performance of AMANDA across different question types and different predicted answer lengths . ( a ) shows that it performs poorly on why and other questions whose answers are usually longer .
supports this fact as well .
When the predicted answer length increases , both F1 and EM start to degrade .
The gap between F1 and EM also increases for longer answers .
This is because for longer answers , the model is notable to decide the exact boundaries ( low EM score ) but manages to predict some correct words which partially overlap with the reference answer ( relatively higher F1 score ) .
Qualitative Error Analysis
On the News QA development set , AMANDA predicted completely wrong answers on 25.1 % of the questions .
We randomly picked 50 such questions for analysis .
The observed types of errors are given in with examples .
42 % of the errors are due to answer ambiguities , i.e. , no unique answer is present .
22 % of the errors are due to mis-match between question and context words .
10 % of the errors are due to the need for highly complex inference .
6 % of the errors occur due to paraphrasing , i.e. , the question is posed with different words which do not appear in the passage context .
The remaining 20 % of the errors are due to insufficient evidence , incorrect tokenization , wrong coreference resolution , etc .
Related Work
Recently , several neural network - based models have been proposed for QA .
Models based on the idea of chunking and ranking include and .
used a fine - grained gating mechanism to capture the correlation between a passage and a question .
used a Match - LSTM to encode the question and passage together and a boundary model determined the beginning and ending boundary of an answer .
reimplemented Match - LSTM for the News QA dataset and proposed a faster version of it .
used a co-attentive encoder followed by a dynamic decoder for iteratively estimating the boundary pointers .
proposed a bi-directional attention flow approach to capture the interactions between passages and questions .
proposed a simple context matching - based neural encoder and incorporated word overlap and term frequency features to estimate the start and end pointers .
proposed a gated self - matching approach which encodes the passage and question together using a self - matching attention mechanism .
proposed a memory network - based multi - layer embedding model and reported results on the TriviaQA dataset .
Different from all prior approaches , our proposed multifactor attentive encoding helps to aggregate relevant evidence by using a tensor - based multi -factor attention mechanism .
This in turn helps to infer the answer by synthesizing information from multiple sentences .
AMANDA also learns to focus on the important question words to encode the aggregated question vector for predicting the answer with suitable answer type .
Conclusion
In this paper , we have proposed a question - focused multifactor attention network ( AMANDA ) , which learns to aggregate meaningful evidence from multiple sentences with deeper understanding and to focus on the important words in a question for extracting an answer span from the passage with suitable answer type .
AMANDA achieves state - of the - art performance on NewsQA , TriviaQA , and Search QA datasets , outperforming all prior published models by significant margins .
Ablation results show the importance of the proposed components .
