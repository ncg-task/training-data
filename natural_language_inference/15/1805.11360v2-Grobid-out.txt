title
Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information
abstract
Sentence matching is widely used in various natural language tasks such as natural language inference, paraphrase identification, and question answering. For these tasks, understanding logical and semantic relationship between two sentences is required but it is yet challenging. Although attention mechanism is useful to capture the semantic relationship and to properly align the elements of two sentences, previous methods of attention mechanism simply use a summation operation which does not retain original features enough. Inspired by DenseNet, a densely connected convolutional network, we propose a densely-connected co-attentive recurrent neural network, each layer of which uses concatenated information of attentive features as well as hidden features of all the preceding recurrent layers. It enables preserving the original and the co-attentive feature information from the bottommost word embedding layer to the uppermost recurrent layer. To alleviate the problem of an ever-increasing size of feature vectors due to dense concatenation operations, we also propose to use an autoencoder after dense concatenation. We evaluate our proposed architecture on highly competitive benchmark datasets related to sentence matching. Experimental results show that our architecture, which retains recurrent and attentive features, achieves state-of-the-art performances for most of the tasks.
Introduction
Semantic sentence matching, a fundamental technology in natural language processing, requires lexical and compositional semantics. In paraphrase identification, sentence matching is utilized to identify whether two sentences have identical meaning or not. In natural language inference also known as recognizing textual entailment, it determines whether a hypothesis sentence can reasonably be inferred from a given premise sentence. In question answering, sentence matching is required to determine the degree of matching 1) between a query and a question for question retrieval, and 2) between a question and an answer for answer selection. However identifying logical and semantic relationship between two sentences is not trivial due to the problem of the semantic gap.
Recent advances of deep neural network enable to learn textual semantics for sentence matching. Large amount of annotated data such as, SNLI (Bowman Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved., and MultiNLI have contributed significantly to learning semantics as well. In the conventional methods, a matching model can be trained in two different ways. The first methods are sentence-encoding-based ones where each sentence is encoded to a fixed-sized vector in a complete isolated manner and the two vectors for the corresponding sentences are used in predicting the degree of matching. The others are joint methods that allow to utilize interactive features like attentive information between the sentences.
In the former paradigm, because two sentences have no interaction, they cannot utilize interactive information during the encoding procedure. In our work, we adopted a joint method which enables capturing interactive information for performance improvements. Furthermore, we employ a substantially deeper recurrent network for sentence matching like deep neural machine translator (NMT). Deep recurrent models are more advantageous for learning long sequences and outperform the shallower architectures. However, the attention mechanism is unstable in deeper models with the well-known vanishing gradient problem. Though) uses residual connection between recurrent layers to allow better information and gradient flow, there are some limitations. The recurrent hidden or attentive features are not preserved intact through residual connection because the summation operation may impede the information flow in deep networks.
Inspired by Densenet ), we propose a densely-connected recurrent network where the recurrent hidden features are retained to the uppermost layer. In addition, instead of the conventional summation operation, the concatenation operation is used in combination with the attention mechanism to preserve co-attentive information better. The proposed architecture shown in is called DRCN which is an abbreviation for Densely-connected Recurrent and Co-attentive neural Network. The proposed DRCN can utilize the increased representational power of deeper recurrent networks and attentive information. Furthermore, to alleviate the problem of an ever-increasing feature vector size due to concatenation operations, we adopted an autoencoder and forwarded a fixed length vector to the higher layer recurrent module as shown in the figure. DRCN is, to our best knowledge, the first generalized version of DenseRNN which is expandable to deeper layers with the property of Dashed arrows indicate that a group of RNN-layer, concatenation and AE can be repeated multiple (N ) times (like a repeat mark in a music score). The bottleneck component denoted as AE, inserted to prevent the ever-growing size of a feature vector, is optional for each repetition. The upper right diagram is our specific architecture for experiments with 5 RNN layers (N = 4).
controllable feature sizes by the use of an autoencoder. We evaluate our model on three sentence matching tasks: natural language inference, paraphrase identification and answer sentence selection. Experimental results on five highly competitive benchmark datasets (SNLI, MultiNLI, QUORA, TrecQA and SelQA) show that our model significantly outperforms the current state-of-the-art results on most of the tasks.

Related Work
Earlier approaches of sentence matching mainly relied on conventional methods such as syntactic features, transformations or relation extraction. These are restrictive in that they work only on very specific tasks.
The developments of large-scale annotated datasets and deep learning algorithms have led a big progress on matching natural language sentences. Furthermore, the wellestablished attention mechanisms endowed richer information for sentence matching by providing alignment and dependency relationship between two sentences. The release of the large-scale datasets also has encouraged the developments of the learning-centered approaches to semantic representation. The first type of these approaches is sentence-encoding-based methods where sentences are encoded into their own sentence representation without any cross-interaction. Then, a classifier such as a neural network is applied to decide the relationship based on these independent sentence representations. These sentence-encoding-based methods are simple to extract sentence representation and are able to be used for transfer learning to other natural language tasks). On the other hand, the joint methods, which makeup for the lack of interaction in the former methods, use cross-features as an attention mechanism to express the word-or phrase-level alignments for performance improvements.
Recently, the architectural developments using deeper layers have led more progress in performance. The residual connection is widely and commonly used to increase the depth of a network stably). More recently, Huang et al. ) enable the features to be connected from lower to upper layers using the concatenation operation without any loss of information on lower-layer features.
External resources are also used for sentence matching. Chen et al.) used syntactic parse trees or lexical data bases like WordNet to measure the semantic relationship among the words and Pavlick et al. added interpretable semantics to the paraphrase data base.
Unlike these, in this paper, we do not use any such external resources. Our work belongs to the joint approaches which uses densely-connected recurrent and co-attentive information to enhance representation power for semantic sentence matching.

Methods
In this section, we describe our sentence matching architecture DRCN which is composed of the following three components: (1) word representation layer, (2) attentively connected RNN and (3) interaction and prediction layer. We denote two input sentences as P = {p 1 , p 2 , ? ? ? , p I } and Q = {q 1 , q 2 , ? ? ? , q J } where pi /q j is the i th /j th word of the sentence P /Q and I/J is the word length of P /Q. The over all architecture of the proposed DRCN is shown in.

Word Representation Layer
To construct the word representation layer, we concatenate word embedding, character representation and the exact matched flag which was used in.
In word embedding, each word is represented as a ddimensional vector by using a pre-trained word embedding method such as GloVe or Word2vec). In our model, a word embedding vector can be updated or fixed during training. The strategy whether to make the pre-trained word embedding be trainable or not is heavily task-dependent. Trainable word embeddings capture the characteristics of the training data well but can result in overfitting. On the other hand, fixed (non-trainable) word embeddings lack flexibility on task-specific data, while it can be robust for overfitting, especially for less frequent words. We use both the trainable embedding e tr pi and the fixed (non-trainable) embedding e fix pi to let them play complementary roles in enhancing the performance of our model. This technique of mixing trainable and non-trainable word embeddings is simple but yet effective. The character representation c pi is calculated by feeding randomly initialized character embeddings into a convolutional neural network with the max-pooling operation. The character embeddings and convolutional weights are jointly learned during training.
Like (Gong, Luo, and Zhang 2018), the exact match flag f pi is activated if the same word is found in the other sentence.
Our final word representational feature p w i for the word pi is composed of four components as follows:
(1)
Here, E tr and E fix are the trainable and non-trainable (fixed) word embeddings respectively. Char-Conv is the characterlevel convolutional operation and [? ; ?] is the concatenation operator. For each word in both sentences, the same above procedure is used to extract word features.

Densely connected Recurrent Networks
The ordinal stacked RNNs (Recurrent Neural Networks) are composed of multiple RNN layers on top of each other, with the output sequence of previous layer forming the input sequence for the next. More concretely, let H l be the l th RNN layer in a stacked RNN. Note that in our implementation, we employ the bidirectional LSTM (BiLSTM) as a base block of H l . At the time step t, an ordinal stacked RNN is expressed as follows:
While this architecture enables us to buildup higher level representation, deeper networks have difficulties in training due to the exploding or vanishing gradient problem.
To encourage gradient to flow in the backward pass, residual connection ) is introduced which bypasses the non-linear transformations with an identity mapping. Incorporating this into (2), it becomes
(3)
However, the summation operation in the residual connection may impede the information flow in the network . Motivated by Densenet ), we employ direct connections using the concatenation operation from any layer to all the subsequent layers so that the features of previous layers are not to be modified but to be retained as they are as depicted in. The densely connected recurrent neural networks can be described ash l

Bottleneck component
Our network uses all layers' outputs as a community of semantic knowledge. However, this network is a structure with increasing input features as layers get deeper, and has a large number of parameters especially in the fully-connected layer. To address this issue, we employ an autoencoder as a bottleneck component. Autoencoder is a compression technique that reduces the number of features while retaining the original information, which can be used as a distilled semantic knowledge in our model. Furthermore, this component increased the test performance by working as a regularizer in our experiments.

Interaction and Prediction Layer
To extract a proper representation for each sentence, we apply the step-wise max-pooling operation over densely connected recurrent and co-attentive features (pooling in). More specifically, if the output of the final RNN layer is a 100d vector for a sentence with 30 words, a 30 ? 100 matrix is obtained which is max-pooled column-wise such that the size of the resultant vector p or q is 100. Then, we aggregate these representations p and q for the two sentences P and Q in various ways in the interaction layer and the final feature vector v for semantic sentence matching is obtained as follows:
Here, the operations +, ? and | ? | are performed elementwise to infer the relationship between two sentences. The element-wise subtraction p ? q is an asymmetric operator for one-way type tasks such as natural language inference or answer sentence selection.
Finally, based on previously aggregated features v, we use two fully-connected layers with ReLU activation followed by one fully-connected output layer. Then, the softmax function is applied to obtain a probability distribution of each class. The model is trained end-to-end by minimizing the multi-class cross entropy loss and the reconstruction loss of autoencoders.

Experiments
We evaluate our matching model on five popular and wellstudied benchmark datasets for three challenging sentence matching tasks: (i) SNLI and MultiNLI for natural language inference; (ii) Quora Question Pair for paraphrase identification; and (iii) TrecQA and SelQA for answer sentence selection in question answering. Additional details about the above datasets can be found in the supplementary materials.

Implementation Details
We initialized word embedding with 300d GloVe vectors pre-trained from the 840B Common Crawl corpus (Pennington, Socher, and Manning 2014), while the word embeddings for the out-of-vocabulary words were initialized randomly. We also randomly initialized character embedding with a 16d vector and extracted 32d character representation with a convolutional network. For the densely-connected recurrent layers, we stacked 5 layers each of which have 100 hidden units. We set 1000 hidden units with respect to the fullyconnected layers. The dropout was applied after the word and character embedding layers with a keep rate of 0.5. It was also applied before the fully-connected layers with a keep rate of 0.8. For the bottleneck component, we set 200 hidden units as encoded features of the autoencoder with a dropout rate of 0.2. The batch normalization was applied on the fully-connected layers, only for the one-way type datasets. The RMSProp optimizer with an initial learning rate of 0.001 was applied. The learning rate was decreased by a factor of 0.85 when the dev accuracy does not improve. All weights except embedding matrices are constrained by L2 regularization with a regularization constant ? = 10 ?6 . The sequence lengths of the sentence are all different for each dataset: 35 for SNLI, 55 for MultiNLI, 25 for Quora question pair and 50 for TrecQA. The learning parameters were selected based on the best performance on the dev set. We employed 8 different randomly initialized sets of parameters with the same model for our ensemble approach.

Experimental Results

SNLI and MultiNLI
We evaluated our model on the natural language inference task over SNLI and MultiNLI datasets. shows the results on SNLI dataset of our model with other published models. Among them, ESIM+ELMo and LM-Transformer are the current state-of-the-art models. However, they use additional contextualized word representations from language models as an externel knowledge. The proposed DRCN obtains an accuracy of 88.9% which is a competitive score although we do not use any external knowledge like ESIM+ELMo and LM-Transformer. The ensemble model achieves an accuracy of 90.1%, which sets the new state-ofthe-art performance. Our ensemble model with 53m parameters (6.7m?8) outperforms the LM-Transformer whose the number of parameters is 85m. Furthermore, in case of the encoding-based method, we obtain the best performance of 86.5% without the co-attention and exact match flag. shows the results on MATCHED and MISMATCHED problems of MultiNLI dataset. Our plain DRCN has a competitive performance without any contextualized knowledge. And, by combining DRCN with the ELMo, one of the contextualized embeddings from language models, our model outperforms the LM-Transformer which has 85m parameters with fewer parameters of 61m. From this point of view, the combination of our model with a contextualized knowledge Models Acc.
|?| Sentence encoding-based method BiLSTM-Max 84.5 40m Gumbel TreeLSTM 85.6 2.9m CAFE 85.9 3.7m Gumbel TreeLSTM 86.0 10m Residual stacked 86.0 29m Reinforced SAN 86.3 3.1m Distance SAN 86   is a good option to enhance the performance.
Quora Question Pair shows our results on the Quora question pair dataset. BiMPM using the multiperspective matching technique between two sentences reports baseline performance of a L.D.C. network and basic multi-perspective models. We obtained accuracies of 90.15% and 91.30% in single and ensemble methods, respectively, surpassing the previous state-of-the-art model of DIIN.
TrecQA and SelQA shows the performance of different models on TrecQA and SelQA datasets for answer sentence selection task that aims to select a set of candidate answer sentences given a question. Most competitive models) also use attention methods for words alignment between question and candidate answer sentences. However, the proposed DRCN using collective attentions over multiple layers, achieves the new state-ofthe-art performance, exceeding the current state-of-the-art performance significantly on both datasets.

Analysis
Ablation study We conducted an ablation study on the SNLI dev set as shown in, where we aim to exam-Models Accuracy (%) Siamese-LSTM 82.58 MP LSTM 83.21 L.D.C. 85.55 BiMPM 88.17 pt-DecAttchar.c 88.40 DIIN 89.06 DRCN 90.15 DIIN* 89.84 DRCN* 91.30 0.750 0.811 PWIM  0.758 0.822 MP CNN 0.762 0.830 HyperQA 0.770 0.825 PR+CNN 0.780 0.834 DRCN 0.804 0.862 clean version HyperQA 0.801 0.877 PR+CNN 0.801 0.877 BiMPM: Performance for answer sentence selection on TrecQA and selQA test set.
ine the effectiveness of our word embedding technique as well as the proposed densely-connected recurrent and coattentive features. Firstly, we verified the effectiveness of the autoencoder as a bottleneck component in. Although the number of parameters in the DRCN significantly decreased as shown in, we could see that the performance was rather higher because of the regularization effect. Secondly, we study how the technique of mixing trainable and fixed word embeddings contributes to the performance in models (3-4). After removing E tr or E fix in eq. (1), the performance degraded, slightly. The trainable embedding E tr seems more effective than the fixed embedding E fix . Next, the effectiveness of dense connections was tested in models (5-9). In (5-6), we removed dense connections only over co-attentive or recurrent features, respectively. The result shows that the dense connections over attentive features are more effective. In, we removed dense connections over both co-attentive and recurrent features, and the performance degraded to 88.5%. In (8), we replace dense connection with residual connection  only over recurrent and co-attentive features. It means that only the word embedding features are densely connected to the uppermost layer while recurrent and attentive features are connected to the upper layer using the residual connection. In (9), we removed additional dense connection over word embedding features from (8). The results of (8-9) demonstrate that the dense connection using concatenation operation over deeper layers, has more powerful capability retaining collective knowledge to learn textual semantics. The model is the basic 5-layer RNN with attention and is the one without attention. The result of (10) shows that the connections among the layers are important to help gradient flow. And, the result of (11) shows that the attentive information functioning as a soft-alignment is significantly effective in semantic sentence matching. The performances of models having different number of recurrent layers are also reported in. The models (5-9) which have connections between layers, are more robust to the increased depth of network, however, the performances of (10-11) tend to degrade as layers get deeper. In addition, the models with dense connections rather than residual connections, have higher performance in general. shows that the connection between layers is essential, especially in deep models, endowing more representational power, and the dense connection is more effective than the residual connection. Word Alignment and Importance Our denselyconnected recurrent and co-attentive features are connected to the classification layer through the max pooling operation such that all max-valued features of every layer affect the loss function and perform a kind of deep supervision . Thus, we could cautiously interpret the classification results using our attentive weights and max-pooled positions. The attentive weights contain information on how two sentences are aligned and the numbers of max-pooled positions in each dimension play an important role in classification. shows the attention map (? i,j in eq. (5)) on each layer of the samples in. The Avg(Layers) is the average of attentive weights over 5 layers and the gray heatmap right above the Avg(Layers) is the rate of max-pooled positions. The darker indicates the higher importance in classification. In the figure, we can see that tight, competing and bicycle are more important words than others in classifying the label. The word tight clothing in the hypothesis can be inferred from spandex in the premise. And competing is also inferred from race. Other than that, the riding is matched with pedaling, and pair is matched with two. Judging by the matched terms, the model is undoubtedly able to classify the label as an entailment, correctly.
In, most of words in both the premise and the hypothesis coexist except white and gray. In attention map of layer 1, the same or similar words in each sentence have a high correspondence (gray and white are not exactly matched but have a linguistic relevance). However, as the layers get deeper, the relevance between white building and gray building is only maintained as a clue of classification (See layer 5). Because white is clearly different from gray, our model determines the label as a contradiction.
The densely connected recurrent and co-attentive features are well-semanticized over multiple layers as collective knowledge. And the max pooling operation selects the softpositions that may extract the clues on inference correctly.

Linguistic Error Analysis
We conducted a linguistic error analysis on MultiNLI, and compared DRCN with the ESIM, DIIN and CAFE. We used annotated subset provided by the MultiNLI dataset, and each sample belongs to one of the 13 linguistic categories. The results in table 7 show that our model generally has a good performance than others on most categories. Especially, we can see that ours outperforms much better on the Quantity/Time category which is one of the most difficult problems. Furthermore, our DRCN shows the highest mean and the lowest stddev for both MATCHED and MISMATCHED problems, which indicates that it not only results in a competitive performance but also has a consistent performance.

Conclusion
In this paper, we introduce a densely-connected recurrent and co-attentive network (DRCN) for semantic sentence matching. We connect the recurrent and co-attentive features from the bottom to the top layer without any deformation. These intact features over multiple layers compose a community of semantic knowledge and outperform the previous deep RNN models using residual connections. In doing so, bottleneck components are inserted to reduce the size of the network. Our proposed model is the first generalized version of DenseRNN which can be expanded to deeper layers with the property of controllable feature sizes by the use of an autoencoder. We additionally show the interpretability of our model using the attentive weights and the rate of maxpooled positions. Our model achieves the state-of-the-art performance on most of the datasets of three highly challenging natural language tasks. Our proposed method using the collective semantic knowledge is expected to be applied to the various other natural language tasks.

Visualization on the comparable models
We study how the attentive weights flow as layers get deeper in each model using the dense or residual connection. We used the samples of the SNLI dev set in. and 5 show the attention map on each layer of the models of DRCN, Table 6 (8), and Table 6 (9). In the model of (8), we replaced the dense connection with the residual connection only over recurrent and co-attentive features. And, in the model of (9), we removed additional dense connection over word embedding features from Table 6 (8). We denote the model of Table 6 (9) as Res1 and the model of Table 6 (8) as Res2 for convenience.
In, DRCN does not try to find the right alignments at the upper layer if it already finds the rationale for the prediction at the relatively lower layer. This is expected that the DRCN use the features of all the preceding layers as a collective knowledge. While Res1 and Res2 have to find correct alignments at the top layer, however, there are some mis alignments such as competing and bicyclists rather than competing and race in Res2 model.
In the second example in, although the DRCN couldn't find the clues at the lower layer, it gradually finds the alignments, which can be a rationale for the prediction. At the 5th layer of DRCN, the attentive weights of gray building and white building are significantly higher than others. On the other hand, the attentive weights are spread in several positions in both Res1 and Res2 which use residual connection.: Visualization of attentive weights on the entailment example. The premise is "two bicyclists in spandex and helmets in a race pedaling uphill." and the hypothesis is "A pair of humans are riding their bicycle with tight clothing, competing with each other.". The attentive weights of DRCN, Res1, and Res2 are presented from left to right.: Visualization of attentive weights on the contradiction example. The premise is "Several men in front of a white building." and the hypothesis is "Several people in front of a gray building.". The attentive weights of DRCN, Res1, and Res2 are presented from left to right.