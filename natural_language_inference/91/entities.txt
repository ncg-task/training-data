143	95	134	https://github.com / stanfordnlp/spinn.
143	67	77	full SPINN
143	0	31	Our optimized C ++/ CUDA models
143	40	58	Theano source code
144	3	6	fix
144	62	64	at
144	65	68	300
144	11	28	model dimension D
144	37	61	word embedding dimension
145	11	31	CPU performance test
145	32	34	on
145	37	82	2.20 GHz 16 core Intel Xeon E5-2660 processor
145	83	87	with
145	88	102	hyperthreading
146	58	60	on
146	64	82	NVIDIA Titan X GPU
146	3	7	test
146	12	39	thin - stack implementation
146	48	57	RNN model
21	11	21	introduces
21	69	122	Stack - augmented Parser - Interpreter Neural Network
21	128	133	SPINN
22	0	5	SPINN
22	6	14	executes
22	19	31	computations
22	32	34	of
22	37	60	tree - structured model
22	61	63	in
22	66	85	linearized sequence
22	96	107	incorporate
22	110	131	neural network parser
22	132	145	that produces
22	150	174	required parse structure
23	12	25	improves upon
23	30	50	TreeRNN architecture
24	0	2	At
24	3	12	test time
24	18	36	can simultaneously
24	37	42	parse
24	47	56	interpret
24	57	75	unparsed sentences
24	78	86	removing
24	91	101	dependence
24	102	104	on
24	108	123	external parser
25	14	22	supports
25	23	42	batched computation
25	43	46	for
25	52	81	parsed and unparsed sentences
25	84	92	yielding
25	93	110	dramatic speedups
25	111	115	over
25	116	133	standard TreeRNNs
26	24	65	novel tree - sequence hybrid architecture
26	66	78	for handling
26	79	99	local linear context
26	100	102	in
26	103	126	sentence interpretation
2	25	32	Parsing
2	37	59	Sentence Understanding
189	3	12	find that
189	17	43	bare SPINN - PI - NT model
189	44	52	performs
189	53	66	little better
189	67	71	than
189	76	88	RNN baseline
189	100	110	SPINN - PI
189	111	115	with
189	120	139	added tracking LSTM
189	140	148	performs
189	149	153	well
191	4	20	full SPINN model
191	21	25	with
191	30	61	relatively weak internal parser
191	62	70	performs
191	71	89	slightly less well
191	108	124	robustly exceeds
191	129	140	performance
191	141	143	of
191	148	160	RNN baseline
192	5	34	SPINN - PI and the full SPINN
192	35	59	significantly outperform
192	64	99	previous sentence - encoding models
193	28	38	outperform
193	43	59	tree - based CNN
193	76	80	uses
193	81	110	tree - structured composition
193	111	114	for
193	115	139	local feature extraction
193	151	177	simpler pooling techniques
193	178	186	to build
193	187	204	sentence features
195	4	14	full SPINN
195	15	24	performed
195	25	40	moderately well
195	41	55	at reproducing
195	60	85	Stanford Parser 's parses
195	86	88	of
195	93	102	SNLI data
195	103	105	at
195	108	142	transition - by - transition level
195	145	149	with
195	150	165	92.4 % accuracy
195	166	168	at
195	169	178	test time
194	12	21	show that
194	24	29	model
194	30	39	that uses
194	40	85	tree - structured composition fully ( SPINN )
194	86	100	outper - forms
194	119	154	only partially ( tree - based CNN )
194	171	182	outperforms
194	101	104	one
194	105	110	which
194	198	223	not use it at all ( RNN )
