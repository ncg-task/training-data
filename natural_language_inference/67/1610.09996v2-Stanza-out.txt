title
End - to - End Answer Chunk Extraction and Ranking for Reading Comprehension
abstract
This paper proposes dynamic chunk reader ( DCR ) , an end - toend neural reading comprehension ( RC ) model that is able to extract and rank a set of answer candidates from a given document to answer questions .
DCR is able to predict answers of variable lengths , whereas previous neural RC models primarily focused on predicting single tokens or entities .
DCR encodes a document and an input question with recurrent neural networks , and then applies a word - by - word attention mechanism to acquire question - aware representations for the document , followed by the generation of chunk representations and a ranking module to propose the top - ranked chunk as the answer .
Experimental results show that DCR achieves stateof - the - art exact match and F 1 scores on the SQuAD dataset ( Rajpurkar et al. 2016 ) .
Introduction
Reading comprehension - based question answering ( RCQA ) is the task of answering a question with a chunk of text taken from related document ( s ) .
A variety of neural models have been proposed recently either for extracting a single entity or a single token as an answer from a given text ( Hermann et al .
In both cases , an answer boundary is either easy to determine or already given .
Different from the above two assumptions for RCQA , in the real - world QA scenario , people may ask questions about both entities ( factoid ) and non-entities such as explanations and reasons ( non -factoid ) ( see for examples ) .
In this regard , RCQA has the potential to complement other QA approaches that leverage structured data ( e.g. , knowledge bases ) for both the above question types .
This is because RCQA can exploit the textual evidences to ensure increased answer coverage , which is particularly helpful for non-factoid answers .
However , it is also challenging for RCQA to identify answer in arbitrary position in the passage with arbitrary length , especially for nonfactoid answers which might be clauses or sentences .
As a result , apart from a few exceptions , this research direction has not been fully explored yet .
Compared to the relatively easier RC task of predicting single tokens / entities 1 , predicting answers of arbitrary lengths and positions significantly increase the search space complexity : the number of possible candidates to consider is in the order of O ( n 2 ) , where n is the number of passage words .
In contrast , for previous works in which answers are single tokens / entities or from candidate lists , the complexity is in O ( n ) or the size of candidate lists l ( usually l ? 5 ) , respectively .
To address the above complexity , Rajpurkar et al .
used a two - step chunk - and - rank approach that employs a rule - based algorithm to extract answer candidates from a passage , followed by a ranking approach with hand - crafted features to select the best answer .
The rule - based chunking approach suffered from low coverage ( ?
70 % recall of answer chunks ) that can not be improved during training ; and candidate ranking performance depends greatly on the quality of the hand - crafted features .
More recently , proposed two endto - end neural network models , one of which chunks a candidate answer by predicting the answer 's two boundary indices and the other classifies each passage word into answer / notanswer .
Both models improved significantly over the method proposed by Rajpurkar et al ..
Our proposed model , called dynamic chunk reader ( DCR ) , not only significantly differs from both the above systems in the way that answer candidates are generated and ranked , but also shares merits with both works .
First , our model uses deep networks to learn better representations for candidate answer chunks , instead of using fixed feature representations as in .
Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .
The contributions of this paper are three - fold .
( 1 ) We pro - We also propose several simple but effective features to strengthen the attention mechanism , which fundamentally improves candidate ranking , with the by -product of higher exact boundary match accuracy .
The experiments on the Stanford Question Answering Dataset ( SQuAD ) , which contains a variety of human - generated factoid and non-factoid questions , have shown the effectiveness of above three contributions .
Our paper is organized as follows .
We formally define the RCQA problem first .
Next , we describe our baseline with a neural network component .
We present the end - to - end dynamic chunk reader model next .
Finally , we analyze our experimental results and discuss the related work .
shows an example of our RC setting where the goal is to answer a question Q i , factoid ( Q1 ) or non-factoid ( Q2 and Q3 ) , based on a supporting passage P i , by selecting a continuous sequence of text A i ?
P i as answer .
Q i , P i , and A i are all word sequences , where each word is drawn from a vocabulary , V .
The i - th instance in the training set is a triple in the form of ( P i , Q i , A i ) , where P i = ( p i 1 , . . . , p i| Pi| ) , Q i = ( q i 1 , . . . , q i| Qi| ) , and A i = ( a i 1 , . . . , a i | Ai | ) ( p i , q i , a i ? V ) .
Owing to the dis agreement among annotators , there could be more than one correct answer for the same question ; and the k - th answer to Q i is denoted by
Problem Definition
An answer candidate for the i - th training example is defined as c m , n i , a sub-sequence in P i , that spans from position m ton ( 1 ? m ? n ? | P i | ) .
The ground truth answer A i could be included in the set of all candidates Ci = { c m ,n i |? m , n ? N + , subj ( m , n , P i ) and 1 ? m ? n ? | P i |} , where subj ( m , n , P i ) is the constraint put on the candidate chunk for P i , such as , " c m , n i can have at most 10 tokens " , or " c m , n i must have a pre-defined POS pattern " .
To evaluate a system 's performance , it s top answer to a question is matched against the corresponding gold standard answer ( s ) .
Remark : Categories of RC Tasks
Other simpler variants of the aforementioned RC task were explored in the past .
For example , quiz - style datasets ( e.g. , MCTest ( Richardson , Burges , and Renshaw 2013 ) , Movie QA ) have multiple - choice questions with answer options .
Cloze - style datesets , usually automatically generated , have factoid " question " s created by replacing the answer in a sentence from the text with blank .
For the answer selection task this paper focuses on , several datasets exist , e.g.
TREC - QA for factoid answer extraction from multiple given passages , bAbI ( Weston , Chopra , and Bordes 2014 ) designed for inference purpose , and the SQuAD dataset used in this paper .
To the best of our knowledge , the SQuAD dataset is the only one for both factoid and nonfactoid answer extraction with a question distribution more close to real - world applications .
Baseline : Chunk - and - Rank Pipeline with
Neural RC
In this section we modified a state - of - the - art RC system for cloze - style tasks for our answer extraction purpose , to see how much gap we have for the two type of tasks , and to inspire our end - to - end system in the next section .
In order to make the cloze - style RC system to make chunk - level decision , we use the RC model to generate features for chunks , which are further used in a feature - based ranker like in ( Rajpurkar et al . ) .
As a result , this baseline can be viewed as a deep learning based counterpart of the system in ( Rajpurkar et al . ) .
It has two main components :
1 ) a standalone answer chunker , which is trained to produce overlapping candidate chunks , and 2 ) a neural RC model , which is used to score each word in a given passage to be used thereafter for generating chunk scores .
Answer Chunking
To reduce the errors generated by the rule - based chunker in , first , we capture the part - of - speech ( POS ) pattern of all answer subsequences in the training dataset to form a POS pattern trie tree , and then apply the answer POS patterns to passage P i to acquire a collection of all subsequences ( chunk candidates )
Ci whose POS patterns can be matched to the POS pattern trie .
This is equivalent to putting an constraint subj ( m , n , P i ) to candidate answer chunk generation process that only choose the chunk with a POS pattern seen for answers in the training data .
Then the sub-sequences
Ci are used as answer candidates for P i .
Note that overlapping chunks could be generated for a passage , and we rely on the ranker to choose the best candidate based on features from the cloze - style RC system .
Experiments showed that for > 90 % of the questions on the development set , the ground truth answer is included in the candidate set constructed in such manner .
Dynamic Chunk Reader
The dynamic chunk reader ( DCR ) model is presented in .
Inspired by the baseline we built , DCR is deemed to be superior to the baseline for 3 reasons .
First , each chunk has a representation constructed dynamically , instead of having a set of pre-defined feature values .
Second , each passage word 's representation is enhanced by word - by - word attention that evaluates the relevance of the passage word to the question .
Third , these components are all within a single , end - to - end model that can be trained in a joint manner .
DCR works in four steps .
First , the encoder layer encodes passage and question separately , by using bidirectional recurrent neural networks ( RNN ) .
Second , the attention layer calculates the relevance of each passage word to the question .
Third , the chunk representation layer dynamically extracts the candidate chunks from the given passage , and create chunk representation that encodes the contextual information of each chunk .
Fourth , the ranker layer scores the relevance between the representations of a chunk and the given question , and ranks all candidate chunks using a softmax layer .
We describe each step in details below .
Encoder Layer
We use bi-directional RNN encoder to encode P i and Q i of example i , and get hidden state for each word position p ij and q ik .
3 As RNN input , a word is represented by a row vector x ?
Rn . x can be the concatenation of word embedding and word features ( see ) .
The word vector for the t - th word is x t .
A word sequence is processed using an RNN encoder with gated recurrent units ( GRU ) , which was proved to be effective in RC and neural machine translation tasks ( Bahdanau , Cho , and .
For each position t , GRU computes ht with input x t and previous state h t?1 , as :
( 4 ) where ht , rt , and u t ?
Rd are d-dimensional hidden state , reset gate , and update gate , respectively ; W {r , u} , W ? R nd and U {r , u} , U ?
R dd are the parameters of the GRU ; ?
is the sigmoid function , and denotes elementwise production .
For a word at t , we use the hidden state ? ?
ht from the forward RNN as a representation of the preceding context , and the ? ?
ht from a backward RNN that encodes text reversely , to incorporate the context after t.
Next ,
where hp j and h q k are hidden states from the bi-directional RNN encoders ( see ) .
An inner product , ? jk , is calculated between hp j and every question word h q k .
It indicates how well the passage word p j matches with every question word q k . ?
j is a weighted pooling of | Q | question hidden states , which serves as a p j - aware question representation .
The concatenation of hp j and ?
j leads to a passage - question joint representation , v j ?
R 4 d .
4 Next , we apply a second bi - GRU layer taking the v j s as inputs , and obtain forward and backward representations ? ? ? j and ? ? ? j ?
Rd , and in turn their
Chunk Representation
Layer
A candidate answer chunk representation is dynamically created given attention layer output .
We first decide the text boundary for the candidate chunk , and then form a chunk representation using all or part of those ?
j outputs inside the chunk .
To decide a candidate chunk ( boundary ) : we tried two ways : ( 1 ) adopt the POS trie - based approach used in our baseline , and ( 2 ) enumerate all possible chunks up to a maximum number of tokens .
For ( 2 ) , we create up to N ( max chunk length ) chunks starting from any position j in P j .
Approach ( 1 ) can generate candidates with arbitrary lengths , but fails to recall candidates whose POS pattern is unseen in training set ; whereas approach ( 2 ) considers all possible candidates within a window and is more flexible , but over- generates invalid candidates .
For a candidate answer chunk c m , n spanning from position m ton inclusively , we construct chunk representation ?
m , n ?
R 2 d using every ?
j within range [ m , n] , with a function g ( ) .
Formally , ? m , n = g (? m , . . . , ? n )
We experimented with several pooling functions ( e.g. , max , average ) for g ( ) , and found out that , instead of pooling , the best function is to concatenate the hidden state of the first word in a chunk in forward RNN and that of the last word in backward RNN .
Formally , ? m , n = g (? m , . . . , ? n ) = [ ? ? ? m ; ? ? ? n ]
( 8 ) We hypothesize that the hidden states at that two ends can better represent the chunk 's contexts , which is critical for this task , than the states within the chunk .
This observation also agrees with .
Ranker Layer Each chunk c m,n is evaluated on its context similarity to the question , by taking the cosine similarity between the chunk context representation ?
m ,n acquired from chunk representation layer , and the question representation which is the concatenation of the last hidden state in forward RNN and the first hidden state in backward RNN .
Thus , for training example i , we have the probability of the chunk c m ,n i as P ( c m , n
, ? ? ? h
Qi k or ? ? ?
h
Qi k is the k - th hidden state output from question Q i 's forward and backward RNN encoder , respectively .
In runtime , the chunk with the highest probability is taken as the answer .
In training , the following negative log likelihood is minimized :
Note that the i - th training instance is only used when A i is included in the corresponding candidate chunk set Ci , i.e. ?
m , n A i = c m , n i .
The softmax in the final layer serves as the list - wise ranking module similar in spirit to ) . and ( 4 ) are designed to help the model align the passage text with question .
Note that some types of questions ( e.g. , " who " , " when " questions ) have answers that have a specific POS / NE tag pattern .
For instance , " who " questions mostly have proper nouns / persons as answers and " when " questions may frequently have numbers / dates ( e.g. , a year ) as answers .
Thus , we believe that the model could exploit the co-relation between question types and answer POS / NE patterns easier with POS and NE tag features .
Implementation Details
We pre-processed the SQuAD dataset using Stanford CoreNLP tool 5 with its default setting to tokenize the text and obtain the POS and NE annotations .
To train our model , we used stochastic gradient descent with the ADAM optimizer , with an initial learning rate of 0.001 .
All GRU weights were initialized from a uniform distribution between ( - 0.01 , 0.01 ) .
The hidden state size , d , was set to 300 for all GRUs .
The question bi - GRU shared parameters with the passage bi - GRU , while the attention - based passage bi - GRU had its own parameters .
We shuffled all training examples at the beginning of each epoch and adopted a curriculum learning approach ) , by sorting training instances by length in every 10 batches , to enable the model start learning from relatively easier instances and to harder ones .
We also applied dropout of rate 0.2 to the embedding layer of input bi - GRU encoder , and gradient clipping when the norm of gradients exceeded 10 .
We trained in mini-batch style ( mini - batch size is 180 ) and applied zero - padding to the passage and question inputs in each batch .
We also set the maximum passage length to be 300 tokens , and pruned all the tokens after the 300 - th token in the training set to save memory and speedup the training process .
This step reduced the training set size by about 1.6 % .
During test , we test on the full length of passage , so that we do n't prune out the potential candidates .
We trained the model for at most 30 epochs , and in case the accuracy did not improve for 10 epochs , we stopped training .
For the feature ranking - based system , we used jforest ranker ( Ganjis affar , Caruana , and Lopes 2011 ) with Lambda MART - Regression Tree algorithm and the ranking metric was NDCG @ 10 .
For the Gated Attention Reader in baseline system , we replicated the method and use the same configurations as in .
Results shows our main results on the SQuAD dataset .
Compared to the scores reported in , our exact match ( EM ) and F1 on the development set and EM score on the test set are better , and F1 on the test set is comparable .
We also studied how each component in our model contributes to the over all performance .
shows the details as well as the results of the baseline ranker .
As the first row of shows , our baseline system improves 10 % ( EM ) over , row 1 ) , the feature - based ranking system .
However when compared to our DCR model , row 2 ) , the baseline ( row 1 ) is more than 12 % ( EM ) behind even though it is based on the state - of - the - art model for cloze - style RC tasks .
This can be attributed to the advanced model structure and end - to - end manner of DCR .
Experiments
We also did ablation tests on our DCR model .
First , replacing the word - by - word attention with Attentive Reader style attention decreases the EM score by about 4.5 % , showing the strength of our proposed attention mechanism .
Second , we remove the features in input to see the contribution of each feature .
The result shows that POS feature ( 1 ) and question - word feature ( 3 ) are the two most important features .
Finally , combining the DCR model with the proposed POS - trie constraints yields a score similar to the one obtained using the DCR model with all possible n-gram chunks .
The result shows that ( 1 ) our chunk representations are powerful enough to differentiate even a huge amount of chunks when no constraints are applied ; and ( 2 ) the proposed POS - trie reduces the search space at the cost of a small drop in performance .
Analysis
To better understand our system , we calculated the accuracy of the attention mechanism of the gated attention reader used in our deep learning - based baseline .
We found that it is 72 % accurate i.e. , 72 % of the times a word with the highest attention score is inside the correct answer span .
This means that , if we could accurately detect the boundary around the word with the highest attention score to form the answer span , we could achieve an accuracy close to 72 % .
In addition , we checked the answer recall of our candidate chunking approach .
When we use a window size of 10 , 92 % of the time , the ground truth answer will be included in the extracted Candidate chunk set .
Thus the upper bound of the exact match score of our baseline system is around 66 % ( 92 % ( the answer recall ) 72 % ) .
From the results , we see our DCR system 's exact match score is at 62 % .
This shows that DCR is proficient at differentiating answer spans dynamically .
To further analyze the system 's performance while predicting answers of different lengths , we show the exact match ( EM ) and F 1 scores for answers with lengths up to 10 tokens in .
From the graph , we can see that , with the increase of answer length , both EM and F1 drops , but in different speed .
The gap between F1 and exact match also widens as answer length increases .
However , the model still yields a decent accuracy when the answer is longer than a single word .
Additionally , shows that the system is better at " when " and " who " questions , but performs poorly on " why " questions .
The large gap between exact match and F1 on " why " questions means that perfectly identifying the span is harder than locating the core of the answer span .
Since " what " , " which " , and " how " questions contain a broad range of question types , we split them further based on the bigram a question starts with , and shows the breakdown for " what " questions .
We can see that " what " questions asking for explanations such as " what happens " and " what happened " have lower EM and F1 scores .
In contrast , " what " questions asking for year and numbers have much higher scores and , for these questions , exact match scores are close to F 1 scores , which means chunking for these questions are easier for DCR .
Related Work
Attentive Reader was the first neural model for factoid RCQA were also potential candidates for the task , and Gulcehre et al.
reported results on the bAbI task , which is worse than memory networks .
Similarly , sequence - to - sequence models were also used , but they did not yield better results either .
Recently , several models have been proposed to enable more complex inference for RC task .
For instance , gated attention model ) employs a multi - layer architecture , where each layer encodes the same document , but the attention is updated from layer to layer .
EpiReader adopted a joint training model for answer extractor and reasoner , where the extractor proposes top candidates , and the reasoner weighs each candidate by examining entailment relationship between question - answer representation and the document .
An iterative alternating attention mechanism and gating strategies were proposed in ( Sordoni , Bachman , and to optimize the attention through several hops .
In contrast , introduced fine - grained document attention from each question word and then aggregated those attentions from each question token by summation with or without weights .
This system achieved the state - of - the - art score on the CNN dataset .
Those different variations all result in roughly 3 - 5 % improvement over attention sum reader , but none of those could achieve higher than that .
Other methods include using dynamic entity representation with maxpooling ) that aims to change entity representation with context , and Weissenborn 's ( 2016 ) system , which tries to separate entity from the context and then matches the question to context , scoring an accuracy around 70 % on the CNN dataset .
However , all of those models assume that the answers are single tokens .
This limits the type of questions the models can answer .
proposed a matchlstm and achieved good results on SQ uAD .
However , this approach predicts a chunk boundary or whether a word is part of a chunk or not .
In contrast , our approach explicitly constructs the chunk representations and similar chunks are compared directly to determine correct answer boundaries .
Conclusion
In this paper we proposed a novel neural reading comprehension model for question answering .
Different from the previously proposed models for factoid RCQA , the proposed model , dynamic chunk reader , is not restricted to predicting a single named entity as an answer or selecting an answer from a small , pre-defined candidate list .
Instead , it is capable of answering both factoid and non-factoid questions as it learns to select answer chunks thatare suitable for an input question .
DCR achieves this goal with a joint deep learning model enhanced with a novel attention mechanism and five simple yet effective features .
Error analysis shows that the DCR model achieves good performance , but still needs to improve on predicting longer answers , which are usually non-factoid in nature .
