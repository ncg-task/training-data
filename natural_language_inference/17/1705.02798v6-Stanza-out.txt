title
Reinforced Mnemonic Reader for Machine Reading Comprehension
abstract
In this paper , we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks , which enhances previous attentive readers in two aspects .
First , a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions thatare temporally memorized in a multi-round alignment architecture , so as to avoid the problems of attention redundancy and attention deficiency .
Second , a new optimization approach , called dynamic - critical reinforcement learning , is introduced to extend the standard supervised method .
It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms .
Extensive experiments on the Stanford Question Answering Dataset ( SQuAD ) show that our model achieves state - of - the - art results .
Meanwhile , our model outperforms previous systems by over 6 % in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets .
Introduction
Teaching machines to comprehend a given context paragraph and answer corresponding questions is one of the long - term goals of natural language processing and artificial intelligence .
gives an example of the machine reading comprehension ( MRC ) task .
Benefiting from the rapid development of deep learning techniques and large - scale benchmark datasets , end - to - end neural networks have achieved promising results on this task .
Despite of the advancements , we argue that there still exists two limitations :
1 .
To capture complex interactions between the context and the question , a variety of neural attention , such as bi-attention , * Contribution during internship at Fudan University and Microsoft Research .
coattention , are proposed in a single - round alignment architecture .
In order to fully compose complete information of the inputs , multiround alignment architectures that compute attentions repeatedly have been proposed .
However , in these approaches , the current attention is unaware of which parts of the context and question have been focused in earlier attentions , which results in two distinct but related issues , where multiple attentions 1 ) focuses on same texts , leading to attention redundancy and 2 ) fail to focus on some salient parts of the input , causing attention deficiency .
2 .
To train the model , standard maximum - likelihood method is used for predicting exactly - matched ( EM ) answer spans .
Recently , reinforcement learning algorithm , which measures the reward as word overlap between the predicted answer and the groung truth , is introduced to optimize towards the F1 metric instead of EM metric .
Specifically , an estimated baseline is utilized to normalize the reward and reduce variances .
However , the convergence can be suppressed when the baseline is better than the reward .
This is harmful if the inferior reward is partially overlapped with the ground truth , as the normalized objective will discourage the prediction of ground truth positions .
We refer to this case as the convergence suppression problem .
To address the first problem , we present a reattention mechanism that temporally memorizes past attentions and uses them to refine current attentions in a multi-round alignment architecture .
The computation is based on the fact that two words should share similar semantics if their attentions about same texts are highly overlapped , and be less similar vice versa .
Therefore , the reattention can be more concentrated if past attentions focus on same parts of the input , or be relatively more distracted so as to focus on new regions if past attentions are not overlapped at all .
As for the second problem , we extend the traditional training method with a novel approach called dynamic - critical reinforcement learning .
Unlike the traditional reinforcement learning algorithm where the reward and baseline are statically sampled , our approach dynamically decides the reward and the baseline according to two sampling strategies , Context : The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 - 10 to earn their third Super Bowl title .
Question : Which NFL team represented the AFC at Super Bowl 50 ?
Answer : Denver Broncos namely random inference and greedy inference .
The result with higher score is always set to be the reward while the other is the baseline .
In this way , the normalized reward is ensured to be always positive so that no convergence suppression will be made .
All of the above innovations are integrated into a new end - to - end neural architecture called Reinforced Mnemonic Reader in .
We conducted extensive experiments on both the SQuAD dataset and two adversarial SQuAD datasets to evaluate the proposed model .
On SQuAD , our single model obtains an exact match ( EM ) score of 79.5 % and F1 score of 86.6 % , while our ensemble model further boosts the result to 82.3 % and 88.5 % respectively .
On adversarial SQuAD , our model surpasses existing approahces by more than 6 % on both AddSent and AddOneSent datasets .
MRC with Reattention
Task Description
For the MRC tasks , a question Q and a context C are given , our goal is to predict an answer A , which has different forms according to the specific task .
In the SQuAD dataset , the answer A is constrained as a segment of text in the context C , nerual networks are designed to model the probability distribution p ( A |C , Q ) .
Alignment Architecture for MRC
Among all state - of - the - art works for MRC , one of the key factors is the alignment architecture .
That is , given the hidden representations of question and context , we align each context word with the entire question using attention mechanisms , and enhance the context representation with the attentive question information .
A detailed comparison of different alignment architectures is shown in .
Early work for MRC , such as Match - LSTM , utilizes the attention mechanism stemmed from neural machine translation [ Dzmitry Bahdanau , 2015 ] serially , where the attention is computed inside the cell of recurrent neural networks .
A more popular approach is to compute attentions in parallel , resulting in a similarity matrix .
Concretely , given two sets of hidden vectors ,
Model
Aligning Rounds Attention Interactive Self Type and U = {u j } m j=1 , representing question and context respectively , a similarity matrix E ?
R nm is computed as
where E ij indicates the similarity between i - th question word and j- th context word , and f is a scalar function .
Different methods are proposed to normalize the matrix , resulting in variants of attention such as bi-attention and coattention .
The attention is then used to attend the question and form a question - aware context representation H = {h j } m j=1 .
Later , propose a serial self aligning method to align the context aginst itself for capturing longterm dependencies among context words .
apply the self alignment in a similar way of Eq. 1 , yielding another similarity matrix B ?
R mm as
where 1 {} is an indicator function ensuring that the context word is not aligned with itself .
Finally , the attentive information can be integrated to form a self - aware context representation Z = {z j } m j=1 , which is used to predict the answer .
We refer to the above process as a single - round alignment architecture .
Such architecture , however , is limited in its capability to capture complex interactions among question and context .
Therefore , recent works build multi-round alignment architectures by stacking several identical aligning layers .
More specifically , let Vt = {v ti } n i =1 and Ut = {u t j } m j=1 denote the hidden representations of question and context in t-th layer , and
is the corresponding question - aware context representation .
Then the two similarity matrices can be computed as
However , one problem is that each alignment is not directly aware of previous alignments in such architecture .
The attentive information can only flow to the subsequent layer through the hidden representation .
This can cause two problems :
1 ) the attention redundancy , where multiple attention distributions are highly similar .
Let softmax ( x ) denote the softmax function over a vector x .
Then this problem can be formulized as D ( softmax ( E t :j ) softmax ( E k :j ) ) < ? (t = k ) , where ?
is a small bound and Dis a function measuring the distribution distance .
2 ) the attention deficiency , which means that the attention fails to focus on salient parts of the input : D ( softmax ( E t :j * ) softmax ( E t :j ) ) > ? , where ?
is another bound and softmax ( E t :j * ) is the " ground truth " attention distribution .
Reattention Mechanism
To address these problems , we propose to temporally memorize past attentions and explicitly use them to refine current attentions .
The intuition is that two words should be correlated if their attentions about same texts are highly overlapped , and be less related vice versa .
For example , in , suppose that we have access to previous attentions , and then we can compute their dot product to obtain a " similarity of attention " .
In this case , the similarity of word pair ( team , Broncos ) is higher than ( team , Panthers ) .
Therefore , we define the computation of reattention as follows .
Let E t?1 and B t?1 denote the past similarity matrices thatare temporally memorized .
The refined similarity matrix E t ( t > 1 ) is computed as
where ?
is a trainable parameter .
Here , softmax ( E t?1 i : ) is the past context attention distribution for the i - th question word , and softmax ( B t?1 :j ) is the self attention distribution for the jth context word .
In the extreme case , when there is no overlap between two distributions , the dot product will be 0 .
On the other hand , if the two distributions are identical and focus on one single word , it will have a maximum value of 1 .
Therefore , the similarity of two words can be explicitly measured using their past attentions .
Since the dot product is relatively small than the original similarity , we initialize the ? with a tunable hyper - parameter and keep it trainable .
The refined similarity matrix can then be normalized for attending the question .
Similarly , we can compute the refined matrix B t to get the unnormalized self reattention as
3 Dynamic - critical Reinforcement Learning
In the extractive MRC task , the model distribution p ( A|C , Q ; ? ) can be divided into two steps : first predicting the start position i and then the end position j as
where ?
represents all trainable parameters .
The standard maximum - likelihood ( ML ) training method is to maximize the log probabilities of the ground truth answer positions where y 1 k and y 2 k are the answer span for the k - th example , and we denote p 1 ( i |C , Q ; ? ) and p 2 ( j|i , C , Q ; ? ) asp 1 ( i ) and p 2 ( j|i ) respectively for abbreviation .
Recently , reinforcement learning ( RL ) , with the task reward measured as word overlap between predicted answer and groung truth , is introduced to MRC [ Xiong et al. , 2017 a ] .
A baseline b , which is obtained by running greedy inference with the current model , is used to normalize the reward and reduce variances .
Such approach is known as the self - critical sequence training ( SCST ) , which is first used in image caption .
More specifically , let R ( A s , A * ) denote the F 1 score between a sampled answer As and the ground truth A * .
The training objective is to minimize the negative expected reward by
where we abbreviate the model distribution p ( A |C , Q ; ? ) asp ? ( A ) , and the reward function R ( A s , A * ) as R ( A s ) .
is obtained by greedily maximizing the model distribution :
The expected gradient ? ?
L SCST ( ? ) can be computed according to the REINFORCE algorithm [ Sutton and as
where the gradient can be approxiamated using a single Monte - Carlo sample
As derived from p ? .
However , a sampled answer is discouraged by the objective when it is worse than the baseline .
This is harmful if the answer is partially overlapped with ground truth , since the normalized objective would discourage the prediction of ground truth positions .
For example , in , suppose that As is champion Denver Broncos and is Denver Broncos .
Although the former is an acceptable answer , the normalized reward would be negative and the prediction for end position would be suppressed , thus hindering the convergence .
We refer to this case as the convergence suppression problem .
Here , we consider both random inference and greedy inference as two different sampling strategies : the first one encourages exploration while the latter one is for exploitation 1 .
Therefore , we approximate the expected gradient by dynamically set the reward and baseline based on the F 1 scores of both As and .
The one with higher score is set as reward , while the other is baseline .
We call this approach as dynamiccritical reinforcement learning ( DCRL )
Notice that the normalized reward is constantly positive so that superior answers are always encouraged .
Besides , when the score of random inference is higher than the greedy one , DCRL is equivalent to SCST .
Thus , Eq. 9 is a special case of Eq. 10 .
Following and , we combine ML and DCRL objectives using homoscedastic uncertainty as task - dependent weightings so as to stabilize the RL training as
where ?
a and ?
bare trainable parameters .
4 End - to - end Architecture
Based on previous innovations , we introduce an end - to - end architecture called Reinforced Mnemonic Reader , which is shown in .
It consists of three main components :
1 ) an encoder builds contextual representations for question and context jointly ; 2 ) an iterative aligner performs multi-round alignments between question and context with the reattention mechanism ;
3 ) an answer pointer predicts the answer span sequentially .
Beblow we give more details of each component .
1
In practice we found that a better approximation can be made by considering a top - K answer list , where is the best result and As is sampled from the rest of the list .
hj t ...
Self Reattention
X C X Q :
The architecture overview of Reinforced Mnemonic Reader .
The subfigures to the right show detailed demonstrations of the reattention mechanism :
1 ) refined E t to attend the query ; 2 ) refined B t to attend the context .
Besides , a character - level embedding is obtained by encoding the character sequence with a bi-directional long short - term memory network ( BiLSTM ) , where two last hidden states are concatenated to form the embedding .
In addition , we use binary feature of exact match , POS embedding and NER embedding for both question and context , as suggested in .
Together the inputs X Q = {x q i } n i =1 and X C = {x c j } m j=1 are obtained .
To model each word with its contextual information , a weight - shared BiLSTM is utilized to perform the encoding
Thus , the contextual representations for both question and context words can be obtained , denoted as two matrices :
Iterative Aligner .
The iterative aligner contains a stack of three aligning blocks .
Each block consists of three modules :
1 ) an interactive alignment to attend the question into the context ; 2 ) a self alignment to attend the context against itself ;
3 ) an evidence collection to model the context representation with a BiLSTM .
The reattention mechanism is utilized between two blocks , where past attentions are temporally memorizes to help modulating current attentions .
Below we first describe a single block in details , which is shown in , and then introduce the entire architecture .
Single Aligning Block .
First , the similarity matrix E ?
R nm is computed using Eq. 1 , where the multiplicative product with nonlinearity is applied as attention function :
The question attention for the j - th context word is then : softmax ( E :j ) , which is used to compute an attended question vector ? j = V softmax ( E :j ) .
To efficiently fuse the attentive information into the context , an heuristic fusion function , denoted as o = fusion ( x , y ) , is proposed as
where ?
denotes the sigmoid activation function , denotes element - wise multiplication , and the bias term is omitted .
The computation is similar to the highway networks , where the output vector o is a linear interpolation of the input x and the intermediate vectorx .
A gate g is used to control the composition degree to which the intermediate vector is exposed .
With this function , the questionaware context vectors H = [ h 1 , ... , h m ] can be obtained as : h j = fusion ( u j ,? j ) . Similar to the above computation , a self alignment is applied to capture the long - term dependencies among context words .
Again , we compute a similarity matrix B ?
R mm using Eq .
2 .
The attended context vector is then computed a s:h j = H softmax ( B :j ) , where softmax ( B :j ) is the self attention for the j - th context word .
Using the same fusion function as z j = fusion ( h j , h j ) , we can obtain self - aware context vectors Z = [ z 1 , ... , z m ].
Finally , a BiLSTM is used to perform the evidence collection , which outputs the fully - aware context vectors R = [ r 1 , ... , rm ] with Z as its inputs .
Multi-round Alignments with Reattention .
To enhance the ability of capturing complex interactions among inputs , we stack two more aligning blocks with the reattention mechanism as follows
where align t denote the t - th block .
In the t - th block ( t > 1 ) , we fix the hidden representation of question as V , and set the hidden representation of context as previous fully - aware context vectors R t?1 .
Then we compute the unnormalized reattention E t and B t with Eq. 4 and Eq. 5 respectively .
In addition , we utilize a residual connection in the last BiLSTM to form the final fully - aware context vectors
.
Answer Pointer .
We apply a variant of pointer networks as the answer pointer to make the predictions .
First , the question representation V is summarized into a fixed - size summary vector s as :
Then we compute the start probability p 1 ( i ) by heuristically attending the context representation R 3 with the question summary s as
Next , a new question summarys is updated by fusing context information of the start position , which is computed as l = R 3 p 1 , into the old question summary : s = fusion ( s , l ) .
Finally the end probability p 2 ( j| i ) is computed as
5 Experiments
Implementation Details
We mainly focus on the SQuAD dataset to train and evaluate our model .
SQuAD is a machine comprehension dataset , totally containing more than 100 , 000 questions manually annotated by crowdsourcing workers on a set of 536 Wikipedia articles .
In addition , we also test our model on two adversarial SQuAD datasets , namely AddSent and Add OneSent .
In both adversarial datasets , a confusing sentence with a wrong answer is appended at the end of the context in order to fool the model .
We evaluate the Reinforced Mnemonic Reader ( R.M - Reader ) by running the following setting .
We first train the model until convergence by optimizing Eq. 7 .
We then finetune this model with Eq. 11 , until the F 1 score on the development set no longer improves .
We use the Adam optimizer [ Kingma and Ba , 2014 ] for both ML and DCRL training .
The initial learning rates are 0.0008 and 0.0001 respectively , and are halved whenever meeting a bad iteration .
The batch size is 48 and a dropout rate of 0.3 is used to prevent overfitting .
Word embeddings remain fixed during training .
For out of vocabulary words , we set the embeddings from Gaussian distributions and keep them trainable .
The size of character embedding and corresponding LSTMs is 50 , the main hidden size is 100 , and the hyperparameter ? is 3 .
Single Model
Overall Results
We submitted our model on the hidden test set of SQuAD for evaluation .
Two evaluation metrics are used : Exact Match ( EM ) , which measures whether the predicted answer are exactly matched with the ground truth , and F1 score , which measures the degree of word overlap at token level .
As shown in , R.M - Reader achieves an EM score of 79.5 % and F1 score of 86.6 % .
Since SQuAD is a competitve MRC benchmark , we also build an ensemble model that consists of 12 single models with the same architecture but initialized with different parameters .
Our ensemble model improves the metrics to 82.3 % and 88.5 % respectively 2 . shows the performance comparison on two adversarial datasets , Add Sent and Add OneSent .
All models are trained on the original train set of SQuAD , and are tested on the two datasets .
As we can see , R.M - Reader comfortably outperforms all previous models by more than 6 % in both EM and F 1 scores , indicating that our model is more robust against adversarial attacks .
Ablation Study
The contributions of each component of our model are shown in .
Firstly , ablation ( 1 - 4 ) explores the utility of reattention mechanism and DCRL training method .
We notice that reattention has more influences on EM score while DCRL contributes more to F1 metric , and removing both of them results in huge drops on both metrics .
Replacing DCRL with SCST also causes a marginal decline of performance on both metrics .
Next , we relace the default attention function with the dot product : f ( u , v ) = u v ( 5 ) , and both metrics suffer from degradations .
( 6 - 7 ) shows the effectiveness of heuristics used in the fusion function .
Removing any of the two heuristics leads to some performance declines , and heuristic subtraction is more effective than multiplication .
Ablation ( 8 - 9 ) further explores different forms of fusion , where gate refers too = g x and MLP denotes o =x in Eq. 4 , respectively .
In both cases the highway - like function has outperformed its simpler variants .
Finally , we study the effect of different numbers of aligning blocks in ( 10 - 12 ) .
We notice that using 2 blocks causes a slight performance drop , while increasing to 4 blocks barely affects the SoTA result .
Interestingly , a very deep alignment with 5 blocks results in a significant performance decline .
We argue that this is because the model encounters the degradation problem existed in deep networks .
Effectiveness of Reattention
We further present experiments to demonstrate the effectiveness of reattention mechanism .
For the attention redun - KL divergence - Reattention + Reattention Redundancy E 1 to E 2 0.695 0.086 0.866 0.074 E 2 to E 3 0.404 0.067 0.450 0.052 B 1 to B 2 0.976 0.092 1.207 0.121 B 2 to B 3 1.179 0.118 1.193 0.097 Deficiency E 2 to E 2 * 0.650 0.044 0.568 0.059 E 3 to E 3 * 0.536 0.047 0.482 0.035 dancy problem , we measure the distance of attention distributions in two adjacent aligning blocks , e.g. , softmax ( E 1 :j ) and softmax ( E 2 :j ) . Higher distance means less attention redundancy .
For the attention deficiency problem , we take the arithmetic mean of multiple attention distributions from the ensemble model as the " ground truth " attention distribution softmax ( E t :j * ) , and compute the distance of individual attention softmax ( E t :j ) with it .
Lower distance refers to less attention deficiency .
We use Kullback - Leibler divergence as the distance function D , and we report the averaged value over all examples .
shows the results .
We first see that the reattention indeed help in alleviating the attention redundancy : the divergence between any two adjacent blocks has been successfully enlarged with reattention .
However , we find that the improvement between the first two blocks is larger than the one of last two blocks .
We conjecture that the first reattention is more accurate at measuring the similarity of word pairs by using the original encoded word representation , while the latter reattention is distracted by highly nonlinear word representations .
In addition , we notice that the attention deficiency has also been moderated : the divergence betwen normalized E t and E t * is reduced .
Prediction Analysis
Figure 5 compares predictions made either with dynamiccritical reinforcement learning or with self - critical sequence training .
We first find that both approaches are able to obtain answers that match the query - sensitive category .
For example , the first example shows that both four and two are retrieved when the questions asks for how many .
Nevertheless , we observe that DCRL constantly makes more accurate prediction on answer spans , especially when SCST already points a rough boundary .
In the second example , SCST takes the whole phrase after Dyrrachium as its location .
The third example shows a similar phenomenon , where the SCST retrieves the phrase constantly servicing and replacing mechanical brushes as its answer .
We demonstrates that this is because SCST encounters the convergence suppression problem , which impedes the prediction of ground truth answer boundaries .
DCRL , however , successfully avoids such problem and thus finds the exactly correct entity .
Context : Carolina 's secondary featured Pro Bowl safety Kurt Coleman , who led the team with a career high seven interceptions , while also racking up 88 tackles and Pro Bowl cornerback Josh Norman , who developed into a shutdown corner during the season and had four interceptions , two of which were returned for touchdowns .
Question :
How many interceptions did Josh Norman score touchdowns within 2015 ?
Answer : two Context :
The motor used polyphase current which generated a rotating magnetic field to turn the motor ( a principle Tesla claimed to have conceived in 1882 ) .
This innovative electric motor , patented in May 1888 , was a simple self - starting design that did not need a commutator , thus avoiding sparking and the high maintenance of constantly servicing and replacing mechanical brushes .
Question :
What high maintenance part did Tesla 's AC motor not require ?
Answer : mechanical brushes Context :
The further decline of Byzantine state - of - affairs paved the road to a third attack in 1185 , when a large Norman army invaded Dyrrachium , owing to the betrayal of high Byzantine officials .
Some time later , Dyrrachium - one of the most important naval bases of the Adriatic - fell again to Byzantine hands .
Question :
Where was Dyrrachium located ?
Answer : the Adriatic : Predictions with DCRL ( red ) and with SCST ( blue ) on SQu AD dev set .
Conclusion
We propose the Reinforced Mnemonic Reader , an enhanced attention reader with two main contributions .
First , a reattention mechanism is introduced to alleviate the problems of attention redundancy and deficiency in multi-round alignment architectures .
Second , a dynamic - critical reinforcement learning approach is presented to address the convergence suppression problem existed in traditional reinforcement learning methods .
Our model achieves the state - of the - art results on the SQuAD dataset , outperforming several strong competing systems .
Besides , our model outperforms existing approaches by more than 6 % on two adversarial SQuAD datasets .
We believe that both reattention and DCRL are general approaches , and can be applied to other NLP task such as natural language inference .
Our future work is to study the compatibility of our proposed methods .
