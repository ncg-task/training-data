title
Read + Verify : Machine Reading Comprehension with Unanswerable Questions
abstract
Machine reading comprehension with unanswerable questions aims to abstain from answering when no answer can be inferred .
In addition to extract answers , previous works usually predict an additional " no - answer " probability to detect unanswerable cases .
However , they fail to validate the answerability of the question by verifying the legitimacy of the predicted answer .
To address this problem , we propose a novel read - then - verify system , which not only utilizes a neural reader to extract candidate answers and produce noanswer probabilities , but also leverages an answer verifier to decide whether the predicted answer is entailed by the input snippets .
Moreover , we introduce two auxiliary losses to help the reader better handle answer extraction as well as noanswer detection , and investigate three different architectures for the answer verifier .
Our experiments on the SQuAD 2.0 dataset show that our system obtains a score of 74.2 F1 on test set , achieving state - of - the - art results at the time of submission ( Aug. 28th , 2018 ) .
Introduction
The ability to comprehend text and answer questions is crucial for natural language processing .
Due to the creation of various large - scale datasets ) , remarkable advancements have been made in the task of machine reading comprehension .
Nevertheless , one important hypothesis behind current approaches is that there always exists a correct answer in the context passage .
Therefore , the models only need to choose a most plausible text span based on the question , instead of checking if there exists an answer in the first place .
Recently , a new version of Stanford Question Answering Dataset ( SQuAD ) , namely SQuAD 2.0 ( Rajpurkar , Jia , and Liang 2018 ) , has been proposed to test the ability of answering answerable questions as well as detecting unanswerable cases .
To deal with unanswerable cases , systems must learn to identify a wide range of linguistic phenomena such as negation , antonymy and entity changes between the passage and the question .
Previous works all apply a shared - normalization :
An overview of our approach .
The reader first extracts a candidate answer and produces a no-answer probability ( NA Prob ) .
The answer verifier then checks whether the extracted answer is legitimate or not .
Finally , the system aggregates previous results and outputs the final prediction .
operation between a " no-answer " score and answer span scores , so as to produce a probability that a question is unanswerable as well as output a candidate answer .
However , they have not considered further validating the answerability of the question by verifying the legitimacy of the predicted answer .
Here , answerability denotes whether the question has an answer , and legitimacy means whether the extracted text can be supported by the passage and the question .
Human , on the contrary , tends to first find a plausible answer given a question , and then checks if there exists any contradictory semantics .
To address the above issue , we propose a read - then - verify system that aims to be robust to unanswerable questions in this paper .
As shown in , our system consists of two components : ( 1 ) a no-answer reader for extracting candidate answers and detecting unanswerable questions , and ( 2 ) an answer verifier for deciding whether or not the extracted candidate is legitimate .
The key contributions of our work are three - fold .
First , we augment existing readers with two auxiliary losses , to better handle answer extraction and no - answer detection respectively .
Since the downstream verifying stage always requires a candidate answer , the reader must be able to extract plausible answers for all questions .
However , previous approaches are not trained to find potential candidates for unanswerable questions .
We solve this problem by introducing an independent span loss that aims to concentrate on the answer extraction task regardless of the answerability of the question .
In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the no -answer score and the other is used for our auxiliary loss .
Besides , we present another independent noanswer loss to further alleviate the confliction , by focusing on the no-answer detection task without considering the shared normalization of answer extraction .
Second , in addition to the standard reading phase , we introduce an additional answer verifying phase , which aims at finding local entailment that supports the answer by comparing the answer sentence with the question .
This is based on the observation that the core phenomenon of unanswerable questions usually occurs between a few passage words and question words .
Take for example , after comparing the passage snippet " Normandy , a region in France " with the question , we can easily determine that no answer exists since the question asks for an impossible condition 1 .
This observation is even more obvious when antonym or mutual exclusion occurs , such as the question asks for " the decline of rainforests " but the passage mentions that " the rainforests spread out " .
Inspired by recent advances in natural language inference ( NLI ) , we investigate three different architectures for the answer verifying task .
The first one is a sequential model that takes two sentences as along sequence , while the second one attempts to capture interactions between two sentences .
The last one is a hybrid model that combines the above two models to test if the performance can be further improved .
Lastly , we evaluate our system on the SQuAD 2.0 dataset , a reading comprehension benchmark augmented with unanswerable questions .
Our best reader achieves a F 1 score of 73.7 and 69.1 on the development set , with or without ELMo embeddings .
When combined with the answer verifier , the whole system improves to 74.8 F1 and 71.5 F1 respectively .
Moreover , the best system obtains a score of 74.2 F1 on test set , achieving state - of - the - art results at the time of submission ( Aug. 28th , 2018 ) .
Background
Existing reading comprehension models focus on answering questions where a correct answer is guaranteed to exist .
However , they are notable to identify unanswerable questions but tend to return an unreliable text span .
Consequently , we first give a brief introduction on the unanswer - 1 Impossible condition means that the question asks for something that is not satisfied by anything in the given passage .
able reading comprehension task , and then investigate current solutions .
Task Description
Given a context passage and a question , the machine needs to not only find answers to answerable questions but also detect unanswerable cases .
The passage and the question are described as sequences of word tokens , denoted as P = {x pi } lp i=1 and Q = {x q j } lq j=1 respectively , where l p is the passage length and l q is the question length .
Our goal is to predict an answer A , which is constrained as a segment of text in the passage : A = {x pi } lb i=la , or return an empty string if there is no answer , where la and lb indicate the answer boundary .
No - Answer Reader
To predict an answer span , current approaches first embed and encode both of passage and question into two series of fix - sized vectors .
Then they leverage various attention mechanisms , such as bi-attention or reattention , to build interdependent representations for passage and question , which are denoted as
respectively .
Finally , they summarize the question representation into a dense vector t , and utilize the pointer network to produce two scores over passage words that indicate the answer boundary :
where ? and ?
are the span scores for answer start and end bounds .
In order to additionally detect if the question is unanswerable , previous approaches attempt to predict a special no - answer score z in addition to the distribution over answer spans .
Concretely , a shared softmax function can be applied to normalize both of no -answer score and span scores , yielding a joint no - answer objective defined as :
lp j= 1 e ?i? j where a and bare the ground - truth start and end positions , and ?
is 1 if the question is answerable and 0 otherwise .
At test time , a question is detected as being unanswerable once the normalized no - answer score exceeds some threshold .
Approach
In this section we describe our proposed read - then - verify system .
The system first leverages a neural reader to extract a candidate answer and detect if the question is unanswerable .
It then utilizes an answer verifier to further check the legitimacy of the predicted answer .
We enhance the reader with two novel auxiliary losses , and investigate three different architectures for the answer verifier .
Here , " Masked Multi Self Attention " refers to multi-head self - attention function ) that only attends to previous tokens .
" Add & Norm " indicates residual connection and layer normalization .
( c ) Our proposed token - wise interaction model , which is designed to compare two sentences and aggregate the results for verifying the answer .
Reader with Auxiliary Losses
Although previous no - answer readers are capable of jointly learning answer extraction and no - answer detection , there exists two problems for each individual task .
For the answer extraction , previous readers are not trained to find candidate answers for unanswerable questions .
In our system , however , the reader is required to extract a plausible answer that is fed to the downstream verifying stage for all questions .
As for no - answer detection , a confliction could be triggered due to the shared normalization between span scores and noanswer score .
Since the sum of these normalized scores is always 1 , an over- confident span probability would cause an unconfident no - answer probability , and vice versa .
Therefore , inaccurate confidence on answer span , which has been observed by , could lead to imprecise prediction on no - answer score .
To address the above issues , we propose two auxiliary losses to optimize and enhance each task independently without interfering with each other .
Independent Span Loss
This loss is designed to concentrate on answer extraction .
In this task , the model is asked to extract candidate answers for all possible questions .
Therefore , besides answerable questions , we also include unanswerable cases as positive examples , and consider the plausible answer as gold answer 2 .
In order to not conflict with no - answer detection , we propose to use a multi-head pointer network to additionally produce another pair of span scores
2 In SQuAD 2.0 , the plausible answer is annotated by human for every unanswerable question .
A pre-trained reader can also be used to extract plausible answers if no annotation is provided .?
and ? :
where multiple heads share the same network architecture but with different parameters .
Then , we define an independent span loss as :
lp j= 1 e? i? j where andb are the augmented ground - truth answer boundaries .
The final span probability is obtained using a simple mean pooling over the two pairs of softmaxnormalized span scores .
Independent No - Answer Loss Despite a multi-head pointer network being used to prevent the confliction problem , no - answer detection can still be weakened since the no -answer score z is normalized with span scores .
Therefore , we consider exclusively encouraging the prediction on no -answer detection .
This is achieved by introducing an independent no - answer loss as :
where ?
is the sigmoid activation function .
Through this loss , we expect the model to produce a more confident prediction on no -answer score z without considering the shared - normalization operation .
Finally , we combine the above losses as follows :
where ? and ?
are two hyper - parameters that control the weight of two auxiliary losses .
Answer Verifier
After the answer is extracted , an answer verifier is used to compare the answer sentence with the question , so as to recognize local textual entailment that supports the answer .
Here , we define the answer sentence as the context sentence that contains either gold answers or plausible answers .
We explore three different architectures , as shown in :
( 1 ) a sequential model that takes the inputs as along sequence , ( 2 ) an interactive model that encodes two sentences interdependently , and ( 3 ) a hybrid model that takes both of the two approaches into account .
Model - I : Sequential Architecture In Model - I , we convert the answer sentence and the question along with the extracted answer into an ordered input sequence .
Then we adapt the recently proposed Generative Pre-trained Transformer ( OpenAI GPT ) to perform the task .
The model is a multi-layer Transformer decoder , which is first trained with a language modeling objective on a large unlabeled text corpus and then finetuned on the specific target task .
Specifically , given an answer sentence S , a question Q and an extracted answer A , we concatenate the two sentences with the answer while adding a delimiter token in between to get [ S ; Q ; $ ; A ] .
We then embed the sequence with its word embedding as well as position embedding .
Multiple transformer blocks are used to encode the sequence embeddings as follows :
where X denotes the sequence 's indexes in the vocab , W e is the token embedding matrix , W p is the position embedding matrix , and n is the number of transformer blocks .
Each block consists of a masked multi-head self - attention layer ) and a position - wise feed - forward layer .
Residual connection and layer normalization are used after each layer .
The last token 's activation h lm n is then fed into a linear projection layer followed by a softmax function to output the no-answer probability y: p ( y | X ) = softmax ( h lm n W y )
A standard cross - entropy objective is used to minimize the negative log-likelihood :
Model - II : Interactive Architecture In Model - II , we consider an interactive architecture that aims to capture the interactions between two sentences , so as to recognize their local entailment relationships for verifying the answer .
This model consists of the following layers :
Encoding :
We embed words using the Glo Ve embedding , and also embed characters of each word with trainable vectors .
We run a bidirectional LSTM ( BiLSTM ) to encode the characters and concatenate two last hidden states to get character - level embeddings .
In addition , we use a binary feature to indicate if a word is part of the answer .
All embeddings along with the feature are then concatenated and encoded by a weight - shared BiLSTM , yielding two series of contextual representations :
where l sis the length of answer sentence , and [ ; ] denotes concatenation .
Inference Modeling :
An inference modeling layer is used to capture the interactions between two sentences and produce two inference - aware sentence representations .
We first compute the dot products of all tuples < s i , q j > as attention weights , and then normalize these weights so as to obtain attended vectors as follows :
e aij ls k =1 ea kj s i
Here , bi refers to the attended vector from question Q for the i - th word in answer sentence S , and vice versa for c j .
Next , in order to separately compare the aligned pairs
for finding local inference information , we use a weight - shared function F to model these aligned pairs as :
F can have various forms , such as BiLSTM , multilayer perceptron , and soon .
Here we use a heuristic function o = F ( x , y) proposed by , which demonstrates good performances compared to other options :
where gelu is the Gaussian Error Linear Unit , is element - wise multiplication , and the bias term is omitted .
Intra - Sentence Modeling :
Next we apply an intra-sentence modeling layer to capture self correlations inside each sentence .
The input are inference - aware vectorss i andq j , which are first passed through another BiLSTM layer for encoding .
We then use the same attention mechanism described above , only now between each sentence and itself , and we set a ij = ? i nf if i = j to ensure that the word is not aligned with itself .
Another function F is used to produce self - aware vectors ?
i andq j respectively .
Prediction :
Before the final prediction , we apply a concatenated residual connection and model the sentences with a BiLSTM as :
A mean - max pooling operation is then applied to summarize the final representation of two sentences , namelys i andq j .
All summarized vectors are then concatenated and fed into a feed - forward classifier that consists of a projection sublayer with gelu activation and a softmax output sublayer , yielding the no-answer probability .
As before , we optimize the negative log-likelihood objective function .
Model - III : Hybrid Architecture
To explore how the features extracted by Model - I and Model - II can be integrated to obtain better representation capacities , we investigate the combination of the above two models , namely Model - III .
We merge the output vectors of two models into a single joint representation .
An unified feed - forward classifier is then applied to output the no-answer probability .
Such design allows us to test whether the performance can benefit from the integration of two different architectures .
In practice we use a simple concatenation to merge the two sources of information .
Experimental Setup Dataset
We evaluate our approach on the SQuAD 2.0 dataset ( Rajpurkar , Jia , and Liang 2018 ) .
SQuAD 2.0 is a new machine reading comprehension benchmark that aims to test the models whether they have truely understood the questions by knowing what they do n't know .
It combines answerable questions from the previous SQuAD 1.1 dataset with 53,775 unanswerable questions about the same passages .
Crowdsourcing workers craft these questions with a plausible answer in mind , and make sure that they are relevant to the corresponding passages .
Training and Inference
Our no -answer reader is trained on context passages , while the answer verifier is trained on oracle answer sentences .
Model - I follows a procedure of unsupervised pre-training and supervised fine - tuning .
That is , the model is first optimized with a language modeling objective on a large unlabeled text corpus to initialize its parameters .
Then it adapts the parameters to the answer verifying task with our supervised objective .
For Model - II , we directly train it with the supervised loss .
Model - III , however , consists of two different architectures that require different training procedures .
Therefore , we initialize Model - III with the pre-trained parameters from both of Model - I and Model - II , and then finetune the whole model until convergence .
At test time , the reader first predicts a candidate answer as well as a passage - level no - answer probability .
The answer verifier then validates the extracted answer along with its sentence and outputs a sentence - level probability .
Following the official evaluation setting , a question is detected to be unanswerable once the joint no - answer probability , which is computed as the mean of the above two probabilities , exceeds some threshold .
We tune this threshold to maximize F 1 score on the development set , and report both of EM ( Exact Match ) and F1 metrics .
We also evaluate the performance on no-answer detection with an accuracy metric ( ACC ) , where it s threshold is set as 0.5 by default .
Implementation
We use the Reinforced Mnemonic Reader ( RMR ) , one of the state - of - the - art reading comprehension models on the SQuAD 1.1 dataset , as our base reader .
The reader is configurated with its default setting , and trained with the no -answer objective with our auxiliary losses .
ELMo ( Embeddings from Language Models ) ) is exclusively listed in our experimental configuration .
We run a grid search on ? and ? among [ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ] .
Based on the performance on development set , we set ? as 0.3 and ? to be 1 .
As for answer verifiers , we use the original configuration from for Model - I. For Model - II , the Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 0.0008 is used , the hidden size is set as 300 , and a dropout ) of 0.3 is applied for preventing overfitting .
The batch size is 48 for the reader , 64 for Model - II , and 32 for Model - I as well as Model - III .
We use the Glo Ve 100D embeddings for the reader , and 300D embeddings for Model - II and Model - III .
We utilize the nltk tokenizer 3 to preprocess passages and questions , as well as split sentences .
The passages and the sentences are truncated to not exceed 300 words and 150 words respectively .
Evaluation Main Results
We first submit our approach on the hidden test set of SQuAD 2.0 for evaluation , which is shown in .
We use Model - III as the default answer verifier , and only report the best result .
As we can see , our system obtains state - of the - art results by achieving an EM score of 71.7 and a F 1 score of 74.2 on the test set .
Notice that SLQA + has reached a comparable result compared to our approach .
We argue that it s promising result is largely due to its superior perfor -
Ablation Study
Next , we do an ablation study on the SQuAD 2.0 development set to show the effects of our proposed methods for each individual component .
first shows the ablation results of different auxiliary losses on the reader .
Removing the independent span loss ( indep - I ) results in a performance drop for all answerable questions ( HasAns ) , indicating that this loss helps the model in better identifying the answer boundary .
Ablating independent no - answer loss ( indep - II ) , on the other hand , causes little influence on HasAns , but leads to a severe decline on no - answer accuracy ( NoAns ACC ) .
This suggests that a confliction between answer extraction and no -answer detection indeed happens .
Finally , deleting both of two losses causes a degradation of more than 1.5 points on the over all performance in terms of F1 , with or without ELMo embeddings .
details the results of various architectures for the answer verifier .
Model - III outperforms all of other competitors , achieving a no-answer accuracy of 76.2 .
This illustrates that the combination of two different architectures can bring in further improvement .
Adding ELMo embeddings , however , does not boost the performance .
We hythosize that the bytepair encoding from Model - I and the word / character embeddings from Model - II have provided enough representation capacities .
After doing separate ablations on each component , we then compare the performance of the whole system , as shown in .
The combination of base reader with any answer verifier can always result in considerable performance gains , and combining the reader with Model - III obtains the best result .
We find that the improvement on noanswer accuracy is significant .
This metric raises from 73.1 to 77.1 after adding Model - III to RMR , increasing by 4 absolute points .
Similar observation can be found when ELMo embeddings are used , demonstrating that the gains are consistent and stable .
In order to investigate how the readers affect the over all performance , we fix the answer verifier as Model - III and use DocQA as the base reader instead of RMR , as shown in .
We find that the absolute improvements are even larger : the no-answer accuracy roughly increases by 6 points when adding Model - III to DocQA ( from 69.1 to 75.2 ) , and 5.5 points when adding Model - III to DocQA + ELMo ( from 70.6 to 76.1 ) .
Finally , we plot the precision - recall curves of F 1 score on the development set in .
We observe that RMR + ELMo + Verifier achieves the best precision when the recall is less than 80 .
After the recall exceeds 80 , the precision of RMR + ELMo becomes slightly better .
Ablating two auxiliary losses , however , leads to an over all degradation on the curve , but it still outperforms the baseline by a large margin .
Error Analysis
To perform error analysis , we first categorize all examples on the development set into 5 classes :
Case1 : the question is answerable , the no-answer proba - bility is less than the threshold , and the answer is correct .
Case2 : the question is unanswerable , and the no-answer probability is larger than the threshold .
Case3 : almost the same as case1 , except that the predicted answer is wrong .
Case4 : the question is unanswerable , but the no-answer probability is less than the threshold .
Case5 : the question is answerable , but the no-answer probability is larger than the threshold .
We then show the percentage of each category in .
As we can see , the base reader trained with auxiliary losses is notably better at case2 and case4 compared to the baseline , implying that our proposed losses help the model mainly improve upon unanswerable cases .
After adding the answer verifier , we observe that although the system 's performance on unanswerable cases slightly decreases , the results on case1 and case5 have been improved .
This demonstrates that the answer verifier does well on detecting answerable question rather than unanswerable one .
Besides , we find that the error of answer extraction is relatively small ( 6.5 % for Case3 in RMR + ELMo + Verifier ) .
However , the classification error on no -answer detection is much larger .
More than 20 % of examples are misclassified even with our best system ( 10.3 % for Case4 and 10.9 % for Case5 in RMR + ELMo + Verifier ) .
Therefore , we argue that the main performance bottleneck lies in no-answer detection instead of answer extraction .
Next , to understand the challenges our approach faces , we manually investigate 50 incorrectly predicted unanswerable .
As we can see , our system is good at recognize negation and antonym .
The frequency of negation decreases from 9 % to 0 % and only 4 antonym examples are predicted wrongly .
We think that this is because the two types are relatively easier to identify .
Both of negation and antonym only require to detect one single word in the question , such as " never " or " not " for negation and " increase " to " decrease " for antonym .
However , impossible condition and other neutral types roughly acount for 46 % of the error set , indicating that our system performs less effectively on these more difficult cases .
Related Work
Reading Comprehension Datasets .
Various large - scale reading comprehension datasets , such as cloze - style test , answer extraction benchmark ) and answer generation benchmark , have been proposed .
However , these datasets still guarantee that the given context must contain an answer .
Recently , some works construct negative examples by retrieving passages for existing questions based on Lucene ) and TF - IDF , or using crowdworkers to craft unanswerable questions .
Compared to automatically retrieved negative examples , human - annotated examples are more difficult to detect for two reasons : ( 1 ) the questions are relevant to the passage and ( 2 ) the passage contains a plausible answer to the question .
Therefore , we choose to work on the SQuAD 2.0 dataset in this paper .
Neural Networks for Reading Comprehension .
Neural reading models typically leverage various attention mechanisms to build interdependent representations of passage and question , and sequentially predict the answer boundary ) .
However , these approaches are not designed to handle no - answer cases .
To address this problem , previous works predict a no-answer probability in addition to the distribution over answer spans , so as to jointly learn no - answer detection as well as answer extraction .
Our no - answer reader extends existing approaches by introducing two auxiliary losses that enhance these two tasks independently .
Recognizing Textual Entailment .
Recognizing textual entailment ( RTE ) , or known as natural language inference ( NLI ) , requires systems to understand entailment , contradiction or semantic neutrality between two sentences .
This task is strongly related to no -answer detection , where the machine needs to understand if the passage and the question supports the answer .
To recognize entailment , various branches of works have been proposed , including encodingbased approach , interaction - based approach and sequence - based approach .
In this paper we investigate the last two branches and further propose a hybrid architecture that combines both of them properly .
Answer Validation .
Early answer validation task aims at ranking multiple candidate answers to return a most reliable one .
Later , the answer validation exercise has been proposed to decide whether an answer is corrector not according to a given supporting text and a question , but the dataset is too small for neural network - based approaches .
Recently , propose to validate the candidate answer for detecting unanswerable questions , by comparing the question with the passage .
Our answer verifier , on the contrary , denoises the passage by comparing questions with answer sentences , so as to focus on finding local entailment that supports the answer .
Conclusion
We proposed a read - then - verify system that is able to abstain from answering when a question has no answer given the passage .
We first introduce two auxiliary losses to help the reader concentrate on answer extraction and no -answer detection respectively , and then utilize an answer verifier to validate the legitimacy of the predicted answer , in which three different architectures are investigated .
Our system has achieved state - of - the - art results on the SQuAD 2.0 dataset at the time of submission ( Aug. 28th , 2018 ) .
Looking forward , we plan to design new structures for answer verifiers to handle questions with more complicated inferences .
