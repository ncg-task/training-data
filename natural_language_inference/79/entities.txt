163	3	8	learn
163	13	47	word vectors and paragraph vectors
163	48	53	using
163	54	79	75,000 training documents
163	82	96	25,000 labeled
163	101	127	50,000 unlabeled instances
164	4	21	paragraph vectors
164	22	25	for
164	30	54	25,000 labeled instances
164	64	75	fed through
164	78	92	neural network
164	93	97	with
164	98	114	one hidden layer
164	115	119	with
164	120	128	50 units
164	135	154	logistic classifier
164	164	174	to predict
164	179	188	sentiment
169	60	79	optimal window size
169	80	82	is
169	83	91	10 words
170	4	10	vector
170	11	23	presented to
170	28	38	classifier
170	44	60	concatenation of
170	84	93	PV - DBOW
170	107	114	PV - DM
171	19	49	learned vector representations
171	50	54	have
171	55	69	400 dimensions
172	17	47	learned vector representations
172	48	52	have
172	53	67	400 dimensions
172	68	71	for
172	77	96	words and documents
174	0	18	Special characters
174	19	26	such as
174	27	32	, .!?
175	4	14	treated as
175	17	28	normal word
173	0	10	To predict
173	15	27	10 - th word
173	33	44	concatenate
173	49	66	paragraph vectors
173	71	83	word vectors
23	19	26	propose
23	27	43	Paragraph Vector
23	49	71	unsupervised framework
23	72	83	that learns
23	84	129	continuous distributed vector representations
23	130	133	for
23	134	149	pieces of texts
25	9	25	Paragraph Vector
25	67	84	can be applied to
25	85	118	variable - length pieces of texts
26	19	40	vector representation
26	44	57	trained to be
26	58	64	useful
26	65	79	for predicting
26	80	85	words
26	86	88	in
26	91	100	paragraph
28	0	39	Both word vectors and paragraph vectors
28	44	54	trained by
28	59	86	stochastic gradient descent
28	91	106	backpropagation
29	6	23	paragraph vectors
29	24	27	are
29	28	34	unique
29	35	40	among
29	41	51	paragraphs
29	58	70	word vectors
29	71	74	are
29	75	81	shared
27	20	31	concatenate
27	36	52	paragraph vector
27	53	57	with
27	58	78	several word vectors
27	79	83	from
27	86	95	paragraph
27	100	107	predict
27	112	126	following word
27	127	129	in
27	134	147	given context
30	0	2	At
30	3	18	prediction time
30	25	42	paragraph vectors
30	47	58	inferred by
30	59	65	fixing
30	70	82	word vectors
30	87	95	training
30	100	120	new paragraph vector
30	121	126	until
30	127	138	convergence
2	0	54	Distributed Representations of Sentences and Documents
179	24	27	for
179	28	42	long documents
179	45	68	bag - of - words models
179	69	76	perform
179	77	87	quite well
179	98	118	difficult to improve
179	129	134	using
179	135	147	word vectors
181	4	29	combination of two models
181	30	36	yields
181	40	51	improvement
181	52	71	approximately 1.5 %
181	72	83	in terms of
181	84	95	error rates
184	4	20	method described
184	35	37	is
184	42	55	only approach
184	61	65	goes
184	66	98	significantly beyond the barrier
184	99	101	of
184	102	117	10 % error rate
185	3	11	achieves
185	12	18	7.42 %
185	19	27	which is
185	28	62	another 1.3 % absolute improvement
185	68	93	15 % relative improvement
185	96	100	over
185	105	125	best previous result
