72	39	75	https://github.com/Helsinki-NLP/HBMP
71	4	16	architecture
71	21	38	implemented using
71	39	46	PyTorch
80	4	23	sentence embeddings
80	24	28	have
80	29	40	hidden size
80	41	43	of
80	44	47	600
80	48	51	for
80	52	66	both direction
80	161	200	3 - layer multilayer perceptron ( MLP )
80	201	205	have
80	210	214	size
80	215	217	of
80	218	232	600 dimensions
73	25	29	used
73	32	71	gradient descent optimization algorithm
73	72	80	based on
73	85	101	Adam update rule
73	113	131	pre-implemented in
73	132	139	PyTorch
74	10	23	learning rate
74	24	26	of
74	27	33	5e - 4
75	22	34	decreased by
75	39	45	factor
75	46	48	of
75	49	52	0.2
75	53	58	after
75	59	69	each epoch
75	70	72	if
75	77	82	model
75	87	90	not
75	91	98	improve
76	10	20	batch size
76	21	23	of
76	24	26	64
79	3	6	use
79	7	41	pre-trained Glo Ve word embeddings
79	42	44	of
79	45	49	size
79	50	64	300 dimensions
81	9	16	dropout
81	17	19	of
81	20	23	0.1
81	24	31	between
81	36	46	MLP layers
82	16	29	trained using
82	30	55	one NVIDIA Tesla P100 GPU
20	83	90	opt for
20	95	121	sentence encoding approach
21	58	64	extend
21	32	54	InferSent architecture
21	84	88	with
21	91	114	hierarchylike structure
21	25	27	of
21	118	154	bidirectional LSTM ( BiLSTM ) layers
21	155	159	with
21	160	171	max pooling
2	0	19	Sentence Embeddings
4	0	32	Sentence - level representations
124	3	22	clearly outperforms
124	39	69	non-hierarchical BiLSTM models
124	101	106	fares
124	107	111	well
124	112	128	in comparison to
124	129	165	other state of the art architectures
124	79	81	in
124	173	199	sentence encoding category
125	32	40	close to
125	45	69	current state of the art
125	70	72	on
125	73	77	SNLI
125	99	108	strong on
125	120	152	matched and mismatched test sets
125	153	155	of
125	156	164	MultiNLI
126	10	12	on
126	13	20	SciTail
126	26	33	achieve
126	38	58	new state of the art
126	59	63	with
126	67	75	accuracy
126	76	78	of
126	79	85	86.0 %
130	0	3	For
130	8	20	SNLI dataset
130	23	32	our model
130	33	41	provides
130	46	59	test accuracy
130	60	62	of
130	63	69	86.6 %
130	70	75	after
130	76	84	4 epochs
130	85	87	of
130	88	96	training
133	8	50	MultiNLI matched test set ( MultiNLI - m )
133	51	60	our model
133	61	69	achieves
133	72	85	test accuracy
133	86	88	of
133	89	95	73.7 %
133	96	101	after
133	102	110	3 epochs
133	111	113	of
133	114	122	training
133	125	133	which is
133	134	152	0.8 % points lower
133	153	157	than
133	162	185	state of the art 74.5 %
134	8	45	mismatched test set ( MultiNLI - mm )
134	46	55	our model
134	56	64	achieves
134	67	80	test accuracy
134	81	83	of
134	84	90	73.0 %
134	91	96	after
134	97	105	3 epochs
134	106	108	of
134	109	117	training
134	120	128	which is
134	129	147	0.6 % points lower
134	148	152	than
134	157	180	state of the art 73.6 %
138	0	2	On
138	7	22	SciTail dataset
138	26	34	compared
138	35	44	our model
138	50	57	against
138	58	95	non-sentence embedding - based models
139	3	9	obtain
139	12	17	score
139	18	20	of
139	21	27	86.0 %
139	28	33	after
139	34	42	4 epochs
139	43	45	of
139	46	54	training
139	63	65	is
139	68	101	2.7 % points absolute improvement
139	102	104	on
139	109	144	previous published state of the art
140	0	9	Our model
140	15	26	outperforms
140	27	40	In - fer Sent
140	41	46	which
140	47	55	achieves
140	59	67	accuracy
140	71	77	85.1 %
142	4	11	results
142	12	23	achieved by
142	28	42	proposed model
142	43	46	are
142	47	67	significantly higher
142	68	72	than
142	77	105	previously published results
