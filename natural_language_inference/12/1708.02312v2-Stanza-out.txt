title
Shortcut - Stacked Sentence Encoders for Multi- Domain Inference
abstract
We present a simple sequential sentence encoder for multi-domain natural language inference .
Our encoder is based on stacked bidirectional LSTM - RNNs with shortcut connections and fine - tuning of word embeddings .
The over all supervised model uses the above encoder to encode two input sentences into two vectors , and then uses a classifier over the vector combination to label the relationship between these two sentences as that of entailment , contradiction , or neural .
Our Shortcut - Stacked sentence encoders achieve strong improvements over existing encoders on matched and mismatched multi-domain natural language inference ( top non-ensemble single - model result in the EMNLP RepEval 2017 Shared Task ( Nangia et al. , 2017 ) ) .
Moreover , they achieve the new state - of - theart encoding result on the original SNLI dataset ( Bowman et al. , 2015 ) .
Introduction and Background
Natural language inference ( NLI ) or recognizing textual entailment ( RTE ) is a fundamental semantic task in the field of natural language processing .
The problem is to determine whether a given hypothesis sentence can be logically inferred from a given premise sentence .
Recently released datasets such as the Stanford Natural Language Inference Corpus ( SNLI ) and the Multi - Genre Natural Language Inference Corpus ) ( Multi - NLI ) have not only encouraged several end - to - end neural network approaches to NLI , but have also served as an evaluation resource for general representation learning of natural language .
Depending on whether a model will first encode a sentence into a fixed - length vector without any incorporating information from the other sentence , the several proposed models can be categorized into two groups : ( 1 ) encoding - based models ( or sentence encoders ) , such as Tree - based CNN encoders ( TBCNN ) in or in , and ( 2 ) joint , pairwise models that use cross -features between the two sentences to encode them , such as the Enhanced Sequential Inference Model ( ESIM ) in or the bilateral multiperspective matching ( BiMPM ) model .
Moreover , common sentence encoders can again be classified into tree - based encoders such as which we mentioned before , or sequential encoders such as the biLSTM model by .
In this paper , we follow the former approach of encoding - based models , and propose a novel yet simple sequential sentence encoder for the Multi - NLI problem .
Our encoder does not require any syntactic information of the sentence .
It also does not contain any attention or memory structure .
It is basically a stacked ( multi-layered ) bidirectional LSTM - RNN with shortcut connections ( feeding all previous layers ' outputs and word embeddings to each layer ) and word embedding fine - tuning .
The over all supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors , and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment , contradiction , or neural ( similar to the classifier setup of and ) .
Our simple shortcut - stacked encoders achieve strong improvements over existing encoders due to its multi-layered and shortcutconnected properties , on both matched and mis- matched evaluation settings for multi-domain natural language inference , as well as on the original SNLI dataset .
It is the top single - model ( nonensemble ) result in the EMNLP RepEval 2017 Multi - NLI Shared Task , and the new state - of - the - art for encoding - based results on the SNLI dataset .
Github Code Link : https://github.com/ easonnie/multiNLI_encoder
2 Model
Our model mainly consists of two separate components , a sentence encoder and an entailment classifier .
The sentence encoder compresses each source sentence into a vector representation and the classifier makes a three - way classification based on the two vectors of the two source sentences .
The model follows the ' encoding - based rule ' , i.e. , the encoder will encode each source sentence into a fixed length vector without any information or function based on the other sentence ( e.g. , cross - attention or memory comparing the two sentences ) .
In order to fully explore the generalization of the sentence encoder , the same encoder is applied to both the premise and the hypothesis with shared parameters projecting them into the same space .
This setting follows the idea of Siamese Networks in .
shows the overview of our encoding model ( the standard classifier setup is not shown here ; see and for that ) .
Sentence Encoder
Our sentence encoder is simply composed of multiple stacked bidirectional LSTM ( biLSTM ) layers with shortcut connections followed by a max pooling layer .
Let bilstm i represent the ith biLSTM layer , which is defined as :
where hi t is the output of the ith biLSTM at time t over input sequence ( x i 1 , x i 2 , ... , xi n ) .
In a typical stacked biLSTM structure , the input of the next LSTM - RNN layer is simply the output sequence of the previous LSTM - RNN layer .
In our settings , the input sequences for the ith biLSTM layer are the concatenated outputs of all the previous layers , plus the original word embedding sequence .
This gives a shortcut connection style setup , related to the widely used idea of residual connections in CNNs for computer vision , highway networks for RNNs in speech processing , and shortcut connections in hierarchical multitasking learning ; but in our case we feed in all the previous layers ' output se-quences as well as the word embedding sequence to every layer .
Let W = ( w 1 , w 2 , ... , w n ) represent words in the source sentence .
We assume w i ?
Rd is a word embedding vector which are initialized using some pre-trained vector embeddings ( and is then fine - tuned end - to - end via the NLI supervision ) .
Then , the input of ith biLSTM layer at time t is defined as :
Then , assuming we have m layers of biLSTM , the final vector representation will be obtained by applying row - max - pool over the output of the last biLSTM layer , similar to .
The final layer is defined as :
, d m is the dimension of the hidden state of the last forward and backward LSTM layers , and v is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .
The closest encoder architecture to ours is that of , whose model consists of a single - layer biLSTM with a max - pooling layer , which we treat as our starting point .
Our experiments ( Section 4 ) demonstrate that our enhancements of the stacked - bi RNN with shortcut connections provide significant gains on top of this baseline ( for both SNLI and Multi - NLI ) .
Entailment Classifier
After we obtain the vector representation for the premise and hypothesis sentence , we apply three matching methods to the two vectors ( i ) concatenation ( ii ) element - wise distance and ( iii ) elementwise product for these two vectors and then concatenate these three match vectors ( based on the heuristic matching presented in ) .
Let v p and v h be the vector representations for premise and hypothesis , respectively .
The matching vector is then defined as :
At last , we feed this final concatenated result m into a MLP layer and use a softmax layer to make final classification . , we train on only the SNLI training set ( and we also verify that the tuning decisions hold true on the SNLI dev set ) .
Parameter Settings
We use cross - entropy loss as the training objective with Adam - based opti-Model Accuracy SNLI Multi - NLI Matched Multi - NLI Mismatched CBOW 80.6 65.2 64.6 biLSTM Encoder 81.5 67.5 67.1 300D Tree - CNN Encoder 82.1 --300D SPINN - PI Encoder 83.2 --300D NSE Encoder 84.6 --biLSTM -Max Encoder 84 . mization with 32 batch size .
The starting learning rate is 0.0002 with half decay every two epochs .
The number of hidden units for MLP in classifier is 1600 .
Dropout layer is also applied on the output of each layer of MLP , with dropout rate set to 0.1 .
We used pre-trained 300D Glove 840B vectors to initialize the word embeddings .
Tuning decisions for word embedding training strategy , the hyperparameters of dimension and number of layers for biLSTM , and the activation type and number of layers for MLP , are all explained in Section 4 .
Results and Analysis
Ablation Analysis Results
We now investigate the effectiveness of each of the enhancement components in our over all model .
These ablation results are shown in and 4 , all based on the Multi - NLI development sets .
Finally , shows results for different encoders on SNLI and Multi - NLI test sets .
First , shows the performance changes for different number of biLSTM layers and their varying dimension size .
The dimension size of a biLSTM layer is referring to the dimension of the hidden state for both the forward and backward LSTM - RNNs .
As shown , each added layer model improves the accuracy and we achieve a substantial improvement in accuracy ( around 2 % ) on both matched and mismatched settings , compared to the single - layer biLSTM in .
We only experimented with up to 3 layers with 512 , 1024 , 2048 dimensions each , so the model still has potential to improve the result further with a larger dimension and more layers .
Next , in , we show that the shortcut connections among the biLSTM layers is also an important contributor to accuracy improvement ( around 1.5 % on top of the full 3 - layered stacked - RNN model ) .
This demonstrates that simply stacking the biLSTM layers is not sufficient to handle a complex task like Multi - NLI and it is significantly better to have the higher layer connected to both the output and the original input of all the previous layers ( note that results are based on multi-layered models with shortcut connections ) .
Next , in , we show that fine - tuning the word embeddings also improves results , again for both the in - domain task and cross - domain tasks ( the ablation results are based on a smaller model with a 128 +256 2 - layer biLSTM ) .
Hence , all our models were trained with word embeddings being fine - tuned .
The last ablation in shows that a classifier with two layers of relu is preferable than other options .
Thus , we use that setting for our strongest encoder .
Multi - NLI and SNLI Test Results
Finally , in , we report the test results for MNLI and SNLI .
First for Multi - NLI , we improve substantially over the CBOW and biL - STM Encoder baselines reported in the dataset paper .
We also show that our final shortcut - based stacked encoder achieves around 3 % improvement as compared to the 1 layer biLSTM - Max Encoder in the second last row ( using the exact same classifier and optimizer settings ) .
Our shortcut - encoder was also the top singe - model ( non-ensemble ) result on the EMNLP RepEval Shared Task leaderboard .
Next , for SNLI , we compare our shortcutstacked encoder with the current state - of - the - art encoders from the SNLI leaderboard ( https :// nlp.stanford.edu/projects/snli/ ) .
We also compare to the recent biLSTM - Max Encoder of , which served as our model 's 1 - layer starting point .
The results indicate that ' Our Shortcut - Stacked Encoder ' sur-passes all the previous state - of - the - art encoders , and achieves the new best encoding - based result on SNLI , suggesting the general effectiveness of simple shortcut - connected stacked layers in sentence encoders .
Conclusion
We explored various simple combinations and connections of biLSTM - RNN layered architectures and developed a Shortcut - Stacked Sentence Encoder for natural language inference .
Our model is the top single result in the EMNLP RepEval 2017 Multi - NLI Shared Task , and it also surpasses the state - of - the - art encoders for the SNLI dataset .
In future work , we are also evaluating the effectiveness of shortcut - stacked sentence encoders on several other semantic tasks .
Addendum : Shortcut vs. Residual
In later experiments , we found that a residual connection can achieve similar accuracies with fewer number of parameters , compared to a shortcut connection .
Therefore , in order to reduce the model size and to also follow the SNLI leaderboard settings ( e.g. , 300D and 600D embeddings ) , we performed some additional SNLI experiments with the shortcut connections replaced with residual connections , where the input to each next biL - STM layer is the concatenation of the word embedding and the summation of outputs of all previous layers ( related to ResNet in computer vision ) .
shows these residual - connection SNLI test results and the parameter comparison to shortcut - connection models ( using 3 stacked - biLSTM layers , and one 800 unit MLP layer , based on SNLI dev set tuning ) .
