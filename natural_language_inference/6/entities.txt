167	19	26	achieve
167	27	43	good convergence
167	44	48	with
167	49	62	deeper models
168	10	19	seen that
168	20	29	all tasks
168	30	42	benefit from
168	43	56	deeper models
171	0	18	Multitask learning
173	18	31	NLI objective
173	32	40	leads to
173	43	61	better performance
173	62	64	on
173	69	89	English NLI test set
173	101	109	comes at
173	114	118	cost
173	119	121	of
173	124	166	worse cross - lingual transfer performance
173	9	11	in
173	170	186	XNLI and Tatoeba
9	91	134	https://github.com / facebookresearch/LASER
95	0	26	XNLI : cross - lingual NLI
106	2	21	Our proposed method
106	22	29	obtains
106	34	46	best results
106	47	49	in
106	50	86	zero - shot cross - lingual transfer
106	87	90	for
106	91	116	all languages but Spanish
107	15	31	transfer results
107	32	35	are
107	36	58	strong and homogeneous
107	59	65	across
107	66	79	all languages
110	21	45	zero - short performance
110	46	48	is
110	49	70	( at most ) 5 % lower
110	71	75	than
110	87	94	English
110	97	106	including
110	107	124	distant languages
110	125	129	like
110	130	161	Arabic , Chinese and Vietnamese
110	176	183	achieve
110	184	207	remarkable good results
110	84	86	on
110	211	235	low - resource languages
110	236	240	like
110	241	248	Swahili
113	21	31	outperform
113	32	45	all baselines
113	64	66	by
113	69	87	substantial margin
125	0	38	MLDoc : cross - lingual classification
131	14	24	our system
131	25	32	obtains
131	37	59	best published results
131	60	63	for
131	64	93	5 of the 7 transfer languages
133	0	20	BUCC : bitext mining
145	14	24	our system
145	25	36	establishes
145	39	65	new state - of - the - art
145	66	69	for
145	70	88	all language pairs
145	89	110	with the exception of
145	111	133	English - Chinese test
146	8	18	outperform
146	19	47	Artetxe and Schwenk ( 2018 )
148	0	27	Tatoeba : similarity search
156	69	91	similarity error rates
156	92	101	below 5 %
156	102	105	are
156	26	30	with
157	20	23	for
157	24	36	37 languages
157	55	67	48 languages
157	76	86	error rate
157	87	97	below 10 %
157	102	104	55
157	68	72	with
157	110	124	less than 20 %
158	15	27	15 languages
158	28	44	with error rates
158	45	55	above 50 %
21	22	35	interested in
21	36	83	universal language agnostic sentence embeddings
21	86	93	that is
21	96	118	vector representations
21	119	121	of
21	122	131	sentences
21	132	140	that are
21	141	148	general
21	149	164	with respect to
21	165	179	two dimensions
21	186	200	input language
21	209	217	NLP task
23	17	22	train
23	25	39	single encoder
23	40	49	to handle
23	50	68	multiple languages
2	0	42	Massively Multilingual Sentence Embeddings
4	38	81	joint multilingual sentence representations
