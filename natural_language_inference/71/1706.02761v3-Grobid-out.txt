title
Gated Orthogonal Recurrent Units: On Learning to Forget
abstract
We present a novel recurrent neural network (RNN) based model that combines the remembering ability of unitary RNNs with the ability of gated RNNs to effectively forget redundant/irrelevant information in its memory. We achieve this by extending unitary RNNs with a gating mechanism. Our model is able to outperform LSTMs, GRUs and Unitary RNNs on several long-term dependency benchmark tasks. We empirically both show the orthogonal/unitary RNNs lack the ability to forget and also the ability of GORU to simultaneously remember long term dependencies while forgetting irrelevant information. This plays an important role in recurrent neural networks. We provide competitive results along with an analysis of our model on many natural sequential tasks including the bAbI Question Answering, TIMIT speech spectrum prediction, Penn TreeBank, and synthetic tasks that involve long-term dependencies such as algorithmic, parenthesis, denoising and copying tasks.
Introduction
Recurrent Neural Networks with gating units -such as Long Short Term Memory (LSTMs) and Gated Recurrent Units (GRUs)) -have led to rapid progress in different are as of machine learning such as language modeling, neural machine translation, and speech recognition. These works have proven the importance of gating units for Recurrent Neural Networks.
The main advantage of using these gated units in RNNs is primarily due to the ease of optimization of the models using them and to reduce the learning degeneracies such as vanishing gradients that can cripple conventional RNNs. Most importantly, by designing special gates, it is easier to impose a particular behavior on the model, such as creating shortcut connections through time by using input and forget gates in LSTMs and resetting the memory via the reset gate of a GRU. This feature also brings modularity to the neural network design that seems to make training of those models easier. Gated RNNs are also empirically shown to achieve better results for a wide variety of real-world tasks.
Recently, using unitary and orthogonal matrices (instead of general matrices) in RNNs have attracted an increasing amount of attention in the machine learning community. This trend was following the demonstration that these matrices can be effective in solving tasks involving long-term dependencies and gradients vanishing/exploding problem. Thus a unitary/orthogonal RNN can capture long term dependencies more effectively in sequential data than a conventional RNN or LSTM. As a result, this type of model has been shown to perform well on tasks that would require rote memorization and simple reasoning, such as the copy task(Hochreiter and Schmidhuber 1997) and the sequential MNIST. Those models can just be viewed as an extension to vanilla RNNs) that replaces the transition matrices with either unitary or orthogonal matrices.
In this paper, we refer the ability of a model to omit parts of the input sequence that contain redundant information and to filter out the noise input in general as the means of a forgetting mechanism. Previously have shown the importance of the forgetting mechanism for LSTM networks and with very similar motivations, we discuss the utilization of a forgetting mechanism for RNNs with orthogonal transitions. The importance of forgetting for those networks is mainly due to that unitary/orthogonal RNNs can backpropagate the gradients without vanishing through time, and it is very easy for them to just have an output that depends on equal amounts of all the elements of the whole input sequence. From this perspective, learning to forget can be difficult with unitary/orthogonal RNNs and they can clog up the memory with useless information. However, most real-world applications and natural tasks require the model to filter out irrelevant or redundant information from the input sequence. We argue that difficulties of forgetting can cause unitary and orthogonal RNNs to perform badly on many realistic tasks, and demonstrate this empirically with a toy task.
We propose a new architecture, the Gated Orthogonal Recurrent Unit (GORU), which combines the advantages of the above two frameworks, namely (i) the ability to capture long term dependencies by using orthogonal matrices and (ii) the ability to "forget" by using a GRU structure. We demonstrate that GORU is able to learn long term dependencies effectively, even in complicated datasets which require a forgetting ability. In this work, we focus on implementation of orthogonal transition matrices which is just a subset of the unitary matrices.
GORU outperforms a recent variation of unitary RNN called EURNN) on language modeling, denoising, parenthesis and the question answering tasks. We show that the unitary RNN fails catastrophically on a denoising task which requires the model to forget. On question answering, speech spectrum prediction, algorithmic, parenthesis and the denoising tasks, GORU achieves better accuracy on the test set over all other models that we compare against. We have attempted to use gates on the unitary matrices with complex numbers, but we encountered some training challenges of training gating mechanisms, thus we have decided to just to focus on orthogonal matrices for this paper.

Background
Given an input sequence x t ? R dx , t ? {1, 2, ? ? ? , T }, a vanilla RNN defines a sequence of hidden states ht ? Rd h updated at each time step according to the rule
where W h ? Rd h ?d h , W x ? R dx?d hand b ? Rd hare model parameters and ? is a nonlinear activation function.
RNNs have proven to be effective for solving sequential tasks due to their flexibility to . However, a well-known problem called gradient vanishing and gradient explosion has prevented RNNs from efficiently learning long-term dependencies. Several approaches have been developed to solve this problem, with LSTMs and GRUs being the most successful and widely used.

Gated Recurrent Unit
A big step forward from LSTM is the Gated Recurrent Unit (GRU), proposed by Cho et al,, which removed the extra memory state in LSTM. Specifically, the hidden state ht in a GRU is updated as follows:
where demonstrated the architecture of GRU model. Although LSTMs and GRUs were proposed to solve the exploding and vanishing gradient problem they can in practice still suffer from this issue for long-term tasks. As a result, gradient clipping is usually required in the training process, although that only addresses the gradient explosion. and real-valued. Therefore, any vector x that multiplies a unitary or an orthogonal matrix satisfies:

Unitary and Orthogonal Matrix RNN
For unitary matrices, the nonlinear activation function ? needs to handle complex-valued inputs. In this paper, we use the generalizations of the popular real-valued activation function ReLU(x) = max(0, x) known as
where b is a bias vector. This variant was found to perform effectively on a suite of benchmarks in). Even though modReLU was developed for complex value models, it turns out this activation function fits unitary/orthogonal matrices best.

The Gated Orthogonal Recurrent Unit
The Problem of Forgetting in Orthogonal RNNs
First, we argue for the advantage of an RNN which can forget some of its past inputs. This is desirable because we seek a state representation which can capture the most important elements of the past sequence, and can throwaway irrelevant details or noise. This ability becomes particularly critical when the dimensionality of the RNN state is smaller than the product of the sequence length with the input dimension; i.e., when some form of compression is necessary. For this compression to be most useful for further processing, it is likely that it requires a non-linear combination of the past input values, allowing the network to forget and ignore unnecessary elements from the past. Now consider an RNN whose state is obtained as a sequence of orthogonal transformations, with each transformation being a function of the input at a given time step. Let us focus on class of orthogonal transformations thatare basically rotation for the simplicity of our analysis, which (noncommutativity aside) are analogous to addition in the space of angles. When we compose several orthogonal operators, we just add more angles together. So we forget in the mild sense that we get in the state a combination of several rotations (like adding the angles) and we lose track of exactly which individual rotations were applied. The advantage is that, in the space of angles, the derivative of the final angle to any of the individually added angle is 1, so there is no vanishing gradient. However, we cannot have complete forgetting, e.g., making the new state independent of the past inputs (or of some of them which we wish to forget): for a new rotation to cancel an old rotation, one would need the new rotation to "know" about the old rotation to cancel, i.e., it would need to be a function of the old rotation. But this is not what happens, because each new rotation is chosen before looking at the current state. Instead, in a regular RNN, the state update depends in a non-linear way on the past state, so that for example when a particular value of the state is reached, it can be reset to 0. This would not be possible with just the composition of orthogonal transformations. These considerations motivate an architecture in which we combine orthogonal or unitary transformations with non-linearities which can be trained to forget when and where it is appropriate.

GORU Architecture
This section introduces the Gated Orthogonal Recurrent Unit (GORU). In our architecture, we change the hidden state loop matrix into an orthogonal matrix and change the respective activation function to modReLU:
where ? is a suitable nonlinear activation function and W z ,
rt and z tare the reset and update gates, respectively. U ? Rd h ?d h is kept orthogonal.
In fact, we have only modified the main loop that absorbs new information to orthogonal while leaving the gates unchanged compared to the GRU. demonstrated the architecture of GORU model. We enforce matrix U to be orthogonal by using parametrization method purposed in. U is decomposed into a sequence of 2-by-2 rotation matrices as shown in. Each 2-by-2 rotation contains one trainable rotation parameter.
The update gates of the GORU help the model to filter out irrelevant or noise information coming from the input. It can bethought of as acting like a low-pass filter. The orthogonal transition matrices help the model to prevent the gradients to vanish through time. However, the ways an orthogonal transformation can interact with the hidden state of an RNN is limited to reflections and rotations. The reset gate enables the model to rescale the magnitude of the hidden state activations (h t ).

Experiments
We compare GORU with unitary RNNs (using the EURNN parameterization purposed by Jing et al.) and two other well-known gatedRNNs (LSTMs and GRUs). Previous research on unitary RNNs has mainly focused on memorization tasks; in contrast, we focus on more realistic noisy tasks, which require the model to discard parts of the input sequence to be able to use its capacity efficiently. GORU is implemented in Tensorflow, available from https://github.com/jingli9111/ GORU-tensorflow

Copying Memory Task
The first task we consider is the well known Copying Memory Task. The copying task is a synthetic task that is commonly used to test the network's ability to remember information seen T time steps earlier.
Specifically, the task is defined as follows. An alphabet consists of symbols {a i }, i ? {0, 1, ? ? ? , n ? 1, n, n + 1}, the first n of which represent data, and the remaining two representing "blank" and "marker", respectively. Here we choose n = 8. The input sequence contains 10 data steps, followed by "blank". The RNN model is supposed to output "blank" and give the original sequence once it sees the "marker". Note that each instance has a different location for these 10 elements of data.
In this experiment, we use RMSProp optimization with a learning rate of 0.001 and a decay rate of 0.9 for all models. The batch size is set to 128. All these models have roughly same number of hidden to hidden parameters, despite not having similar neuron layer sizes. Hidden state sizes are set to 128, 100, 90, 512, respectively to match total number of hidden to hidden parameters. GORU is the only gatedsystem to successfully solve this task while the GRU and LSTM both get stuck at the baseline. EURNN is seen to converges within hundreds of iterations.
This task only requires the model to efficiently overcome the gradient vanishing/explosion problem and does not require a forgetting ability. The EURNN performs perfectly and goes through to the baseline in no time -as previously seen. The GORU is the only gated-system to successfully solve this task while the GRU and LSTM get stuck at the baseline as shown in.

Denoise Task
We evaluate the forgetting ability of each RNN architecture on a synthetic "denoise" task. A list of data points are located randomly in along noisy sequence. The RNN model is supposed to filter out the useless part ("noise") and output the remaining sequential labels.
Similarly to the labels of copying memory task above, an alphabet consists of symbols {a i }, i ? {0, 1, ? ? ? , n ? 1, n, n + 1}, the first n of which represent data, and the remaining two represent "noise" and "marker", respectively. The input sequence contains 10 randomly located data steps and the rest are filled by "noise". The RNN model is supposed to output those 10 data in a sequence after it sees the "marker". Just as in the previous experiment, we use RM-SProp optimization algorithm with a learning rate of 0.01 and a decay rate of 0.9 for all models. The batch size is set to 128. All these models have roughly same number of hidden to hidden parameters. Hidden state sizes are set to 128, 100, 90, 512, respectively to match total number of hidden to hidden parameters. EURNN get stuck at the baseline because of lacking forgetting mechanism, while GORU and GRU successfully solve the task.
This task requires both the ability of learning long dependencies but also the ability to forget the noisy input. The GORU and GRU both are able to successfully outperform LSTM in terms of both learning speed and final performances as shown in. EURNN, however, gets stuck at the baseline, just as we intuitively expected.

Parenthesis Task
The parenthesis task) requires the RNN model to count the number of each type of unmatched parentheses at each time step, given that there are 10 types of parentheses. The input data contains 10 pairs of different parenthesis types -e.g., ], }, ?, ...' -mixed with random noise/characters between them. The neural network outputs how many unmatched parentheses there are. For instance, given '(((', the neural network would output '123'.Note that there are nevermore than 10 unmatched parentheses in any category.
In our experiment, the total input length is set to 200. We used batch size 128 and RMSProp Optimizer with a learning rate 0.001, decay rate 0.9 on all models. Hidden state sizes are set to match their total numbers of hidden to hidden parameters.
This task requires learning long-term dependencies and forgetting of the noisy data. The GORU is able to successfully outperform GRU, LSTM and EURNN in terms of both learning speed and final performances as shown in.
We also analyzed the activations of the update gates for GORU and GRU. According to the histogram of activations shown in, both models behave very similarly, and when the model receives noise as input, the activations of its: The activations of the update gates for GORU and GRU on the parenthesis task. In those bar-charts we visualize the number of activations thatare greater than 0.7 normalized by the total number of units in the gate,
As can be seen in the top two plots, the activations of the update gate peaks and becomes almost one when the input is noise when the magnitude of the noise input in the bottom plot is above the red bar.

Algorithmic Task
We tested the RNN models on algorithmic task as described in. The model is fed with random graph as an input sequence and required to output the shortest path at the end of the sequence. We have used the exact same setup and use the data provided as in.
We used batch size 50 and hidden size 128 for all models. The RNNs are trained with RMSProp optimizer with a learning rate of 0.001 and decay rate of 0.9.
We summarized the test set results in. We found that the GORU performs averagely better than GRU/LSTM and EURNN.

Model
Accuracy EURNN 66.3 ? 3.2 LSTM 60.9 ? 4.7 GRU 74.1 ? 4.8 GORU 75.0 ? 5.3

bAbI: Episodic Question Answering
We tested the ability of our RNN models on a word-level episodic question answering task. The bAbI dataset examines RNN's ability to understand language and perform basic logical reasoning. Each training example is a set of statements, thatare logically related in some fashion. For instance, one training example consists of these three statements and a question: Mary went to the bathroom. John moved to the Hallway. Mary traveled to the office. Where is Mary? Answer: office. There are twenty different types of questions that can be asked -some requiring deduction between lines, some requiring association. The bAbI dataset is useful because it contains a small sized vocabulary, short sentences and requires one word answers for each story. Thus, it is a good benchmarking test because the word mapping layers are not the dominant sources of parameters.
We test each task with a uni-directional RNN without any attention mechanism. In detail, we word-embed and then feed one RNN the sequence of statements. Another RNN is fed the word-embedded question. Then, we concatenate the outputs of the two RNN's into a single input for a third RNN that then outputs the correct word.
We summarized the test set results as follows in. We found that the GORU performs averagely better than GRU/LSTM and EURNN. We also show the big gains over EURNN by introducing the gates.

Language Modeling: Character-level Prediction
We test each RNN on character-level language modeling. The RNN is fed by one character each step from are al context and supposed to output the prediction for the next character. We used the Penn Treebank corpus.
We use RMSProp with minibatch size of 32 and a learning rate of 0.001. Each training sequence is unfolded into 50 time steps. Similar to most work in language modeling, at the end of each sequence, the hidden state is saved and used Task GORU GRU LSTM EURNN baseline   to initialize the hidden state for the next sequence. This allows the neural network to give consistent predictions even at the beginning of a sequence.
We show the final test performance in by comparing their performance in terms of bits-per-character. GORU is performing comparable to LSTM and GRU in our experiments and it performs significantly better than EURNN. We have also done an ablation study with dis abling reset and update gates. Since most of the relevant information for character-level prediction can be obtained by only using the recent rather than distant past, the core of the character-prediction challenge does not involve the main strength of EURNN.: Penn Treebank character-level modeling Test on GORU, GRU, LSTM and EURNN. We only use single layer models. We choose the size of the models to match the number of parameters. GORU is able to outperform EURNN. We also tested the performance of restricted GORU which shows the necessity of both reset and update gates.

Speech Spectrum Prediction
We tested the ability of our RNN models on real-world speech spectrum prediction task in short-time Fourier transform (STFT)). We used TIMIT dataset sampled in 8 kHz. The audio .wav file is initially divided into different time frames and then Fourier transformed into the frequency domain and finally normalized for training/testing. In our STFT operation we uses a Hann analysis window of 256 samples (32 milliseconds) and a window hop of 128 samples (16 milliseconds). In this task, the RNNs are required to predict th log-magnitude of the STFT frame at time t + 1, given all the log-magnitudes of STFT frames up to time t. We used a training set with 2400 utterances, a validation set of 600 utterances and 1000 utterances for test. We trained all RNNs for with the same batch size 32 using Adam optimization with a learning rate of 0.001.
We found GORU significantly outperforms all other models with same hidden size as shown in

Conclusion
We have built a novel RNN that brings the benefits of orthogonal matrices to gated architectures: the Gated Orthogonal Recurrent Units (GORU). By replacing the hidden to hidden matrix in the reseting path of the GRU to bean orthogonal matrix, and replacing the non-linear activation to a mod-ReLU, GORU gains the advantage of unitary/orthogonal RNNs since the gradient can pass through longtime steps without exploding. Our empirical results showed that GORU is the only model we found that could solve both the synthetic copying task and the denoise task. Moreover, GORU is able to outperform GRU and LSTM in several benchmark tasks.
These results suggest that the GORU is the first step in bringing an explicit forgetting mechanism to the class of unitary/orthogonal RNNs. Our method demonstrates how to incorporate orthogonal matrices into a variety of neural network architectures, and we are excited to open the gate the next level of neural networks.