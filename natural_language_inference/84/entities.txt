99	0	18	QUESTION ANSWERING
129	40	46	adding
129	47	71	any external information
129	72	82	results in
129	85	108	significant improvement
129	109	113	over
129	118	132	baseline model
129	141	158	3.7 - 10.5 points
133	25	33	spelling
133	40	50	helps more
133	51	55	than
133	14	20	adding
133	65	75	dictionary
133	84	103	3 points difference
130	9	25	dictionary alone
130	36	48	mean pooling
130	56	64	performs
130	65	74	similarly
130	75	77	to
130	78	82	LSTM
135	4	37	model with GLoVe embeddings ( G )
135	38	40	is
135	41	52	still ahead
135	53	57	with
135	60	76	1.1 point margin
134	25	29	uses
134	37	39	SD
134	48	67	1.1 point advantage
134	68	72	over
134	14	19	model
134	88	92	uses
134	93	110	just the spelling
155	0	21	ENTAILMENT PREDICTION
172	64	72	spelling
172	73	76	was
172	77	90	not as useful
172	91	93	on
172	94	111	SNLI and MultiNLI
178	27	54	dictionary - enabled models
178	55	79	significantly outperform
178	80	95	baseline models
178	96	99	for
178	100	109	sentences
178	110	120	containing
178	121	131	rare words
173	27	32	using
173	33	56	fixed random embeddings
173	57	60	for
173	61	70	OOV words
173	109	146	did not bring a significant advantage
173	147	151	over
173	156	164	baseline
179	0	18	LANGUAGE MODELLING
199	37	42	using
199	43	63	external information
199	64	74	to compute
199	75	85	embeddings
199	86	88	of
199	89	102	unknown words
199	103	108	helps
199	109	111	in
199	112	121	all cases
201	3	12	note that
201	13	30	lemma + lowercase
201	31	39	performs
201	40	45	worse
201	46	50	than
201	51	60	any model
201	61	65	with
201	70	80	dictionary
202	0	6	Adding
202	7	15	spelling
202	16	39	consistently helps more
202	40	44	than
202	45	51	adding
202	52	74	dictionary definitions
204	0	5	Using
204	11	34	dictionary and spelling
204	38	66	consistently slightly better
204	67	71	than
204	78	91	just spelling
205	6	23	Glo Ve embeddings
205	24	34	results in
205	39	54	best perplexity
20	17	24	propose
20	27	37	new method
20	38	51	for computing
20	52	77	embeddings " on the fly "
20	86	103	jointly addresses
20	108	132	large vocabulary problem
20	141	156	paucity of data
20	157	169	for learning
20	170	185	representations
20	186	188	in
20	193	230	long tail of the Zipfian distribution
21	181	186	train
21	189	196	network
21	197	207	to predict
21	108	123	representations
21	78	80	of
21	132	137	words
21	237	245	based on
21	246	260	auxiliary data
25	0	33	Several sources of auxiliary data
25	41	45	used
25	46	60	simultaneously
25	61	63	as
25	64	69	input
25	70	72	to
25	75	89	neural network
25	100	107	compute
25	110	133	combined representation
26	34	42	used for
26	43	70	out - of - vocabulary words
26	76	89	combined with
26	90	122	withinvocabulary word embeddings
26	123	142	directly trained on
26	147	163	task of interest
26	167	182	pretrained from
26	186	206	external data source
27	18	41	auxiliary data encoders
27	46	66	trained jointly with
27	71	80	objective
27	83	91	ensuring
27	96	108	preservation
27	109	111	of
27	112	130	semantic alignment
27	131	135	with
27	136	151	representations
27	152	154	of
27	155	180	within - vocabulary words
12	0	39	Learning representations for rare words
2	12	46	COMPUTE WORD EMBEDDINGS ON THE FLY
