title
The NarrativeQA Reading Comprehension Challenge
abstract
Reading comprehension ( RC ) - in contrast to information retrieval - requires integrating information and reasoning about events , entities , and their relations across a full document .
Question answering is conventionally used to assess RC ability , in both artificial agents and children learning to read .
However , existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information ( e.g. , local context similarity or global term frequency ) ; they thus fail to test for the essential integrative aspect of RC .
To encourage progress on deeper comprehension of language , we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts .
These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience .
We show that although humans solve the tasks easily , standard RC models struggle on the tasks presented here .
We provide an analysis of the dataset and the challenges it presents .
Introduction
Natural language understanding seeks to create models that read and comprehend text .
A common strategy for assessing the language understanding capabilities of comprehension models is to demonstrate that they can answer questions about documents they read , akin to how reading comprehension is tested in children when they are learning to read .
After reading a document , a reader usually can not reproduce Title : Ghostbusters II Question : How is Oscar related to Dana ?
Answer : her son Summary snippet : . . .
Peter 's former girlfriend Dana Barrett has had a son , Oscar . . .
Story snippet :
DANA ( setting the wheel brakes on the buggy )
Thank you ,
Frank .
I 'll get the hang of this eventually .
She continues digging in her purse while Frank leans over the buggy and makes funny faces at the baby , OSCAR , a very cute nine - month old boy .
FRANK ( to the baby )
Hiya , Oscar .
What do you say , slugger ?
FRANK ( to Dana )
That 's a good - looking kid you got there , Ms. Barrett .
Figure 1 :
Example question - answer pair .
The snippets here were extracted by humans from summaries and the full text of movie scripts or books , respectively , and are not provided to the model as supervision or at test time .
Instead , the model will need to read the full text and locate salient snippets based solely on the question and its reading of the document in order to generate the answer .
the entire text from memory , but often can answer questions about underlying narrative elements of the document : the salient entities , events , places , and the relations between them .
Thus , testing understanding requires creation of questions that examine high - level abstractions instead of just facts occurring in one sentence at a time .
Unfortunately , superficial questions about a document may often be answered successfully ( by both humans and machines ) using a shallow pattern match - ing strategies or guessing based on global salience .
In the following section , we survey existing QA datasets , showing that they are either too small or answerable by shallow heuristics ( Section 2 ) .
On the other hand , questions which are not about the surface form of the text , but rather about the underlying narrative , require the formation of more abstract representations about the events and relations expressed in the course of the document .
Answering such questions requires that readers integrate information which maybe distributed across several statements throughout the document , and generate a cogent answer on the basis of this integrated information .
That is , they test that the reader comprehends language , not just that it can pattern match .
We present a new task and dataset , which we call NarrativeQA , which will test and reward artificial agents approaching this level of competence ( Section 3 ) .
The dataset consists of stories , which are books and movie scripts , with human written questions and answers based solely on human - generated abstractive summaries .
For the RC tasks , questions maybe answered using just the summaries or the full story text .
We give a short example of a sample movie script from this dataset in .
Fictional stories have a number of advantages as a domain .
First , they are largely self - contained : beyond the basic fundamental vocabulary of English , all the information about salient entities and concepts required to understand the narrative is present in the document , with the expectation that a reasonably competent language user would be able to understand it .
1 Second , story summaries are abstractive and generally written by independent authors who know the work only as a reader .
We make the dataset available online .
2
Review of Reading Comprehension Datasets and Models
There are a large number of datasets and associated tasks available for the training and evaluation of read - 1 For example , new names and words maybe coined by the author ( e.g. " muggle " in Harry Potter novels ) but the reader need only appeal to the book itself to understand the meaning of these concepts , and their place in the narrative .
This ability to form new concepts based on the contexts of a text is a crucial aspect of reading comprehension , and is in part tested as part of the question answering tasks we present .
2
http://deepmind.com/publications ing comprehension models .
We summarize the key features of a collection of popular recent datasets in .
In this section , we briefly discuss the nature and limitations of these datasets and their associated tasks .
MCTest ) is a collection of short stories , each with multiple questions .
Each such question has set of possible answers , one of which is labelled as correct .
While this could be used as a QA task , the MCTest corpus is in fact intended as an answer selection corpus .
The data is human generated , and the answers can be phrases or sentences .
The main limitation of this dataset is that it serves more as a an evaluation challenge than as the basis for end - to - end training of models , due to its relatively small size .
In contrast , CNN / Daily Mail , Children 's Book Test ( CBT ) , and BookTest each provide large amounts of question - answer pairs .
Questions are Cloze - form ( predict the missing word ) and are produced from either short abstractive summaries ( CNN / Daily Mail ) or from next sentence in the document the context was taken from ( CBT and Book - Test ) .
The tasks associated with these datasets are all selecting an answer from a set of options , which is explicitly provided for CBT and BookTest , and is implicit for CNN / Daily Mail , as the answers are always entities from the document .
This significantly favors models that operate by pointing to a particular token ( or type ) .
Indeed , the most successful models on these datasets , such as the Attention Sum Reader ( AS Reader ) , exploit precisely this bias in the data .
However , these models are inappropriate for answers requiring synthesis of a new answer .
This bias towards answers thatare shallowly salient is a more serious limitation of the CNN / Daily Mail dataset , since its context documents are news stories which usually contain a small number of salient entities and focus on a single event .
SQuAD and News QA ) offer a different challenge .
A large number of a questions and answers are provided for a set of documents , where the answers are spans of the context document , i.e. contiguous sequences of words from the document .
Although the answers are not just single word / entity answers , many plausible questions for assessing RC can not be asked because no document span would contain its answer .
While they provide a large number of questions , these are from a relatively small number of documents , which are themselves fairly short , thereby limiting the lexical and topical diversity models trained on this data can cope with .
While the answers are multiword phrases , the spans are generally short and rarely cross sentence boundaries .
Simple models scoring and / or extracting candidate spans conditioned on the question and superficial signal from the rest of the document do well .
These models will not trivially generalize to problems where the answers are not spans in the document , supervision for spans is not provided , or several discontinuous spans are needed to generate a correct answer .
This restricts the scalability and applicability of models doing well on SQuAD or New s QA to more complex problems .
MS MARCO presents a bolder challenge : questions are paired with sets of snippets ( " context passages " ) that contain the information necessary to answer the question , and answers are free - form human generated text .
However , as no restriction was placed on annotators preventing them from copying answers from source documents , many answers are in fact verbatim copies of short spans from the context passages .
Models which do well on SQuAD ( e.g. , ) , extracting spans or pointing , do well here too , and the same concerns as above about the general applicability of solutions to this dataset to larger reading comprehension problems applies .
Search QA ) is a recent dataset in which the context for each question is a set of documents retrieved by a search engine using the question as the query .
However , in contrast with previous datasets neither questions nor answers were produced by annotating the context documents , but rather the context documents were retrieved after collecting pre-existing question - answer pairs .
As such , it is not open to same annotation bias as the datasets discussed above .
However , upon examining answers in the Jeopardy data used to construct this dataset , one finds that 80 % of answers are bigrams or unigrams , and 99 % are 5 tokens or fewer .
Of a sample of 100 answers , 72 % are named entities , all are short noun - phrases .
Summary of Limitations .
We see several limitations of the scope and depth of the RC problems in existing datasets .
First , several datasets are small ( MCTest ) or not overly naturalistic ( bAb I ; ) .
Second , in more naturalistic documents , a majority of questions require only a single sentence to locate supporting information for answering .
This , we suspect , is largely an artifact of the question generation methodology , in which annotators have created questions from a context document , or where context documents that explicitly answer a question are identified using a search engine .
Although the factoidlike Jeopardy questions of Search QA also appears to favor questions answerable with local context .
Finally , we see further evidence of the superficiality of the questions in the architectures that have evolved to solve them , which tend to exploit span selection based on representations derived from local context and the query .
NarrativeQA : A New Dataset
In this section , we introduce our new dataset , Nar-rativeQA , which addresses many of the limitations identified in existing datasets .
Desiderata
From the above discussed features and limitations , we define our desiderata as follows .
We wish to construct a dataset with a large number of questionanswer pairs based on either a large number of supporting documents or from a smaller collection of large documents .
This permits the training of neural network - based models over word embeddings and provide decent lexical coverage and diversity .
The questions and answers should be natural , unconstrained , and human generated , and answering questions should frequently require reference to several parts or a larger span of the context document rather than superficial representations of local context .
Furthermore , we want annotators to privilege writing answers expressed in their own words , and consider higher - level relations between entities , places , and events , rather than copy short spans of the document .
Furthermore , we want to evaluate models both on the fluency and correctness of generated free - form answers , and as an answer selection problem , which requires the provision of sensible distractors to the correct answer .
Finally , the scope and complexity of the QA problem should be such that current models struggle , while humans are capable of solving the task correctly , so as to motivate further research into the development of models seeking human reading comprehension ability .
Data Collection
Method
We will consider complex , self - contained narratives as our documents / stories .
To make the annotation tractable and lead annotators towards asking nonlocalized questions , we will only provide them human written summaries of the stories for generating the question - answer pairs .
We present both books and movie scripts as stories in our dataset .
Books were collected from Project Gutenberg 3 and movie scripts scraped from the web .
We matched our stories with plot summaries from Wikipedia using titles and verified the matching with help from human annotators .
The annotators were asked to determine if both the story and the summary refer to a movie or a book ( as some books are made into movies ) , or if they are the same part in a series produced in the same year .
In this way we obtained 1,567 stories .
This provides with a smaller set of documents , compared to the other datasets , but the documents are long which provides us with good lexical coverage and diversity .
The bottleneck for obtaining a larger number of publicly available stories was finding corresponding summaries .
Annotators on Amazon Mechanical
Turk were instructed to write 10 question - answer pairs each based solely on a given summary .
Reading and annotating summaries is tractable unlike writing questions and answers based on the full stories , and moreover , as the annotators never see the full stories we are much less likely to get questions and answers which are extracted from a localized context .
Annotators were instructed to imagine that they are writing questions to test students who have read the full stories but not the summaries .
We required questions thatare specific enough , given the length and complexity of the narratives , and to provide a diverse set of questions about characters , events , why this happened , and soon .
Annotators were encouraged to use their own words and we prevented them from copying .
We asked for answers thatare grammatical , complete sentences , and explicitly allowed short answers ( one word , or a few - word phrase , or a short sentence ) as we think that answering with a full sentence is frequently perceived as artificial when asking about factual information .
Annotators were asked to avoid extra , unnecessary information in the question or the answer , and to avoid yes / no questions or questions about the author or the actors .
About 30 question - answer pairs per summary were obtained .
The result is a collection of human written natural questions and answers .
As we have multiple questions per summary / story , this allows us to consider answer selection ( from among the 30 ) as a simpler version of the QA than answer generation from scratch .
Answer selection and multiple - choice question answering are frequently used .
We additionally collected a second reference answer for each question by asking annotators to judge whether a question is answerable , given the summary , and provide an answer if it was .
All but 2.3 % of the questions were judged as answerable .
Core Statistics
We collected 1,567 stories , evenly split between books and movie scripts .
We partitioned the dataset into non-overlapping training , validation , and test portions , along stories / summaries .
See for detailed statistics .
The dataset contains 46,765 question - answer pairs .
The questions are grammatical questions written by human annotators , average 9.8 tokens in length , and are mostly formed as ' WH'- questions ( see ) .
We categorized a sample of 300 questions in .
We observe a good variety of question types .
An interesting category are questions which ask for something related to or occurring together / before / after with an event , of which there are about 15 % .
Answers in the dataset are human written , short , averaging 4.73 tokens , but not restricted to spans from the documents .
There are 44.05 % and 29.57 % answers that appear as spans of the summaries and the stories , respectively ; as expected , lower proportion of answers are spans on stories compared to summaries on which they were constructed .
Tasks
We present tasks varying in their scope and complexity : we consider either the summary or the story as context , and for each we evaluate answer generation and answer selection .
The task of answering questions based on summaries is similar in scope to previous datasets .
However , summaries contain more complex relationships and timelines than news articles or short paragraphs from the web and thus provide a task different in nature .
We hope that Narrative QA will motivate the design of architectures capable of modeling such relationships .
This setting is similar to the previous tasks in that the questions and answers were constructed based on these supporting documents .
The full version of Narrative QA requires reading and understanding entire stories ( i.e. , books and movie scripts ) .
This task is at present intractable for existing neural models out of the box .
We further discuss the challenges and possible approaches in the following sections .
We require the use of metrics for generated text .
We evaluate using Bleu - 1 , Bleu - 4 , Meteor , and Rouge - L ( Lin , 2004 ) , using two references for each question , 6 except for the human baseline where we evaluate one reference against the other .
We also evaluate our models using a ranking metric .
This allows us to evaluate how good our model is at reading comprehension regardless of how good it is at generating answers .
We rank answers for questions associated with the same summary / story and compute the mean reciprocal rank ( MRR ) .
7
Baselines and Oracles
In this section , we show that Narrative QA presents a challenging problem for current approaches to reading comprehension by evaluating several baselines based on information retrieval ( IR ) techniques and neural models .
Since neural models use quite different processes for generating answers ( e.g. , predicting a single word or entity , selecting a span of the document context , or open generation of the answer sequence ) , we present results on each .
We also report the human performance by scoring the second reference answer against the first .
Simple IR Baselines
We consider basic IR baselines which retrieve an answer by selecting a span of tokens from the context document based on a similarity measure between the candidate span and a query .
We compare two queries : the question and ( as an oracle ) the gold standard answer .
The answer oracle provides an upper bound We lowercase both the candidates and the references and remove the end of sentence marker and the final full stop .
on the performance of span retrieval models , including the neural models discussed below .
When using the question as the query , we obtain generalization results of IR methods .
Test set results are computed by extracting either 4 - gram , 8 - gram , or full - sentence spans according to the best performance on the validation set .
We consider three similarity metrics for extracting spans : Bleu - 1 , Rouge - L , and the cosine similarity between bag - of - words embedding of the query and the candidate span using pre-trained Glo Ve word embeddings .
Neural Benchmarks
As a first benchmark we consider a simple bidirectional LSTM sequence to sequence ( Seq2Seq ) model ) predicting the answer directly from the query .
Importantly , we provide no context information from either summary or story .
Such a model might classify the question and predict an answer of similar topic or category .
Previous reading comprehension tasks such as CNN / Daily Mail motivated models constrained to predicting a single token from the input sequence .
The AS Reader considers the entire context and predicts a distribution over unique word types .
We adapt the model for sequence prediction by using an LSTM sequence decoder and choosing a token from the input at each step of the 8 Note that we do not consider the span 's context when computing the MRR for IR baselines , as the candidate spans ( i.e. all answers to questions on the story ) are given and simply ranked by their similarity to the query .
output sequence .
As a span- prediction model we consider a simplified version of the Bi- Directional Attention Flow network .
We omit the character embedding layer and learn a mapping from words to a vector space rather than making use of pre-trained embeddings ; and we use a single layer bi-directional LSTM to model interactions among context words conditioned on the query ( modelling layer ) .
As proposed , we adopt the output - layer tailored for spanprediction and leave the rest unchanged .
It was not our aim to use the state - of - the - art model for other datasets but rather to provide a strong benchmark .
Span prediction models can be trained by obtaining supervision on the training set from the oracle IR model .
We use start and end indices of the span achieving the highest Rouge - L score with respect to the reference answers as labels on the training set .
The model is then trained to predict these spans by maximizing the probability of the indices .
Neural Benchmarks on Stories
The design of the Narrative QA dataset makes the straight - forward application of the existing neural architectures computationally infeasible , as this would require running an recurrent neural network on sequences of hundreds of thousands of time steps or computing a distribution over the entire input for attention , as is common .
We split the task into two steps : first , we retrieve a small number of relevant passages from the story using an IR system , and subsequently , apply one of the neural models above on the resulting document .
The question becomes the query for retrieval .
This IR problem is much harder that traditional document retrieval , as the documents , the passages here , are very similar , and the question is short and entities mentioned likely occur many times in the story .
Our retrieval system considers chunks of 200 words from story and computes representations for all chunks and the query .
We then select a varying number of such chunks based on their similarity to the query .
We experiment with different representations and similarity measures in Section 5 .
Finally , we concatenate the selected chunks in the correct temporal order and insert delimiters between them to obtain a much shorter document .
For span prediction models , we then further select a span from the retrieved chunks as described in Section 4.2 .
Experiments
In this section , we describe the data prepraration methodology we used , and experimental results on the summary - reading task as well as the full story task .
Data Preparation
The provided narratives contain a large number of named entities ( such as names of characters or places ) .
Inspired by Hermann et al. , we replace such entities with markers , such as @entity 42 .
These markers are permuted during training and testing so that none of their embeddings learn a specific entity 's representation .
This allows us to build representations for entities from stories that were never seen in training , since they are given a specific identifier ( to differentiate them from other entities in the document ) from a set of generic identifiers re-used across documents .
Entities are replaced according to a simple heuristic based on capital first character and the respective word not appearing in lowercase .
Reading Summaries Only
Reading comprehension of summaries is similar to a number of previous reading comprehension tasks where questions were constructed based on the context document .
However , plot summaries tend to contain more intricate event time lines and a larger number of characters , and in this sense , are more complex to follow than news articles or paragraphs from Wikipedia .
See for the results .
Given that questions were constructed based on the summaries , we expected that both neural models and span - selection models would perform well .
This is indeed the case , with the neural span prediction model significantly outperforming all other proposed methods .
However , there remains a significant room for improvement when compared with the oracle and human scores .
Both the plain sequence to sequence model and the AS Reader , successfully applied to the CNN / DailyMail reading comprehension task , also perform well on this task .
We observe that the AS Reader tends to copy subsequent tokens from the context , thus behaving like a span prediction model .
An additional inductive bias results in higher performance for the span prediction model .
Similar observations between AS Reader and span models have also been made by .
Note that we have tuned each model separately on the development set twice , once selecting the best model based on Rouge - L and report the first four metrics , and a second time selecting based on MRR .
summarizes the results on the full Narra - tive QA task , where the context documents are full stories .
As expected ( and desired ) , we observe a decline in performance of the span- selection oracle IR model , compared with the results on summaries .
This is unsurprising as the questions were constructed on summaries and confirms the initial motivation for designing this task .
As previously , we considered all spans of a given length across the entire story for this model .
For short answers of one or two wordstypically main characters in a story - the candidate , i.e. the closest span to the reference answer , is easily found due to being mentioned throughout the text .
For longer answers it becomes much less likely , compared to the summaries , that a high - scoring span can be found in the story .
Note that this distinguishes Narrative QA from many of the reviewed datasets .
Reading Full Stories Only
In our IR plus neural two - step approach to the task , we first retrieve relevant chunks of the stories and then apply existing reading comprehension models .
We use the questions to guide the IR system for chunk extraction , with the results of the standalone IR baselines giving an indication of the difficulty of this aspect of the task .
The retrieval quality has a direct effect on the performance of all neural models ; a challenge which models on summaries are not presented with .
We considered several approaches to chunk selection : we retrieve chunks based on the highest Rouge - L or Bleu - 1 scoring span with respect to the question in the story ; comparing topic distributions from an LDA model between questions and chunks according to their symmetric Kullback - Leibler divergence .
Finally , we also consider the cosine similarity of TF - IDF representations .
We found that this approach lead to the best performance of the subsequently applied model on the validation set , irrespective of the number of chunks .
Note that we used the answer as the query on the training , and the question for validation and test .
Given the retrieved chunks , we experimented with several neural models using them as context .
The AS Reader , which was the better - performing model on the summaries task , underperforms the simple no -context Seq2Seq baseline ( shown in ) in terms of MRR .
While is does slightly better on the other metrics , it clearly fails to make use of the retrieved context to gain a distinctive margin over the no -context Seq2Seq model .
Increasing the number of retrieved chunks , and thereby recall of possibly relevant parts of the story , had only a minor positive effect .
The span prediction model - which here also uses selected chunks for context - does especially poorly in this setup .
While this model provided the best neural results on the summaries task , we suspect that its performance was particularly badly hurt by the fact that there is so little lexical and grammatical overlap between the source of the questions ( summaries ) and the context provided ( stories ) .
As with the AS Reader , we observed no significant differences for varying number of chunks .
These results leave us a large gap to human performance , highlighting the success of our design objective to build a task that is realistic and straightforward for humans while very difficult for current reading comprehension models .
Qualitative Analysis and Challenges
We find that the proposed dataset meets the desiderata we set out in Section 3.1 .
In particular , we constructed a dataset with a number of long documents , characterised by good lexical coverage and diversity .
The questions and answers are human generated and natural sounding .
And , based on a small manual examination ( of ' Ghostbusters II ' , ' Airplane ' , ' Jacob 's Ladder ' ) , only a small number of questions and answers are shallow paraphrases of sentences in the full document .
Most questions require reading segments at least several paragraphs long , and in some cases even multiple segments spread throughout the story .
Computational challenges identified in Section 5.3 naturally suggest a retrieval procedure as the first step .
Story snippet : I should state therefore , that I , Anthony Rogers , am , so far as I know , the only man alive whose normal span of eighty - one years of life has been spread over a period of 573 years .
To be precise , I lived the first twenty - nine years of my life between 1898 and 1927 ; the other fifty - two since 2419 .
The gap between these two , a period of nearly five hundred years , I spent in a state of suspended animation , free from the ravages of katabolic processes , and without any apparent effect on my physical or mental faculties .
When I began my long sleep , man had just begun his real conquest of the air .
. .
Figure 2 :
Example question - answer pair with snippets from the summary and the story .
We found that the retrieval is challenging even for humans not familiar with the presented narrative .
In particular , the task often requires referring to larger parts of the story , in addition to knowing at least some background about entities .
This makes the search procedure , based on only a short question , a challenging and interesting task in itself .
We show example question - answer pairs in .
These examples were chosen from a small set of manually annotated question - answer pairs to be representative of this collection .
In particular , the examples show that larger parts of the story are required to answer questions .
Consider .
While the relevant paragraph depicting the injury appears early on , it is not until the next snippet ( which appears at the end of the narrative ) that the lethal consequences of the injury are revealed .
This illustrates an iterative reasoning process as well as extremely long temporal dependencies we encountered during manual annotation .
As shown in , reading comprehension on movie scripts requires understanding of written dialogue .
This is a challenge as dialogue is typically non-descriptive , whereas the questions were asked based on descriptive summaries , requiring models to " read between the lines " .
We expect that understanding narratives as complex as those presented in Narrative QA will require transferring text understanding capability from other supervised learning tasks .
Related Work
This paper is the first large - scale question answering dataset on full - length books and movie scripts .
However , although we are the first to look at the QA task , learning to understand books through other modeling objectives has become an important subproblem in NLP .
These include high level plot understanding through clustering of novels or summarization of movie scripts , to more fine grained processing by inducing character types , understanding relationships between characters , or understanding plans , goals , and narrative structure in terms of abstract narratives ) .
In computer vision , the MovieQA dataset ) fulfills a similar role as Nar-rative QA .
It seeks to test the ability of models to comprehend movies via question answering , and part of the dataset includes full length scripts .
Conclusion
We have introduced a new dataset and a set of tasks for training and evaluating reading comprehension systems , born from an analysis of the limitations of existing datasets and tasks .
While our QA task resembles tasks provided by existing datasets , it exposes new challenges because of its domain : fiction .
Fictional stories - in contrast to news stories - are selfcontained and describe richer set of entities , events , and the relations between them .
We have a range of tasks , from simple ( which requires models to read summaries of books and movie scripts , and generate or rank fluent English answers to human - generated questions ) to more complex ( which requires models to read the full stories to answer the questions , with no access to the summaries ) .
In addition to the issue of scaling neural models to large documents , the larger tasks are significantly more difficult as questions formulated based on one or two sentences of a summary might require appealing to possibly discontiguous sentences or paragraphs
Title : Jacob 's Ladder Question :
What is the fatal injury that Jacob sustains which ultimately leads to his death ?
Answer : A bayonete stabbing to his gut .
Summary snippet :
A terrified Jacob flees into the jungle , only to be bayoneted in the gut by an unseen assailant .
[ . . . ]
In a wartime triage tent in 1971 , military doctors fruitlessly treating Jacob reluctantly declare him dead Story snippet :
Ashe spins around one of the attackers jams all eight inches of his bayonet blade into Jacob 's stomach .
Jacob screams .
It is aloud and piercing wail .
[ . . . ]
Int. Vietnam Field Hospital - Day
A doctor leans his head in front of the lamp and removes his mask .
His expression is somber .
He shakes his head .
His words are simple and final .
DOCTOR
He 's gone .
Cut to Jacob Singer . . .
The doctor steps away .
A nurse rudely pulls a green sheet up over his head .
The doctor turns to one of the aides and throws up his hands in defeat .
from the source text .
This requires potential solutions to these tasks to jointly model the process of searching for information ( possibly in several steps ) to serve as support for generating an answer , alongside the process of generating the answer entailed by said support .
End - to - end mechanisms for both searching for information , such as attention , do not scale beyond selecting words or n-grams in short contexts such as sentences and small documents .
Likewise , neural models for mapping documents to answers , or determining entailment between supporting evidence and a hypothesis , typically operate on the scale of sentences rather than sets of paragraphs .
We have provided baseline and benchmark results for both sets of tasks , demonstrating that while existing models give sensible results out of the box on summaries , they do not get any traction on the book - scale tasks .
Having given a quantitative and qualitative analysis of the difficulty of the more complex tasks , we suggest research directions that may help bridge the gap between existing models and hu-man performance .
Our hope is that this dataset will serve not only as a challenge for the machine reading community , but as a driver for the development of a new class of neural models which will take a significant step beyond the level of complexity which existing datasets and tasks permit .
