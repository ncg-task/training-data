title
Learning to Compose Task-Specific Tree Structures
abstract
For years, recursive neural networks (RvNNs) have been shown to be suitable for representing text into fixed-length vectors and achieved good performance on several natural language processing tasks. However, the main drawback of RvNNs is that they require structured input, which makes data preparation and model implementation hard. In this paper, we propose Gumbel Tree-LSTM, a novel tree-structured long short-term memory architecture that learns how to compose task-specific tree structures only from plain text data efficiently. Our model uses Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically and to calculate gradients of the discrete decision. We evaluate the proposed model on natural language inference and sentiment analysis, and show that our model outperforms or is at least comparable to previous models. We also find that our model converges significantly faster than other models.
Introduction
Techniques for mapping natural language into vector space have received a lot of attention, due to their capability of representing ambiguous semantics of natural language using dense vectors. Among them, methods of learning representations of words, e.g. word2vec or GloVe , are relatively well-studied empirically and theoretically, and some of them became typical choices to consider when initializing word representations for better performance at downstream tasks.
Meanwhile, research on sentence representation is still in active progress, and accordingly various architecturesdesigned with different intuition and tailored for different tasks-are being proposed. In the midst of them, three architectures are most frequently used in obtaining sentence representation from words. Convolutional neural networks (CNNs) utilize local distribution of words to encode sentences, similar to n-gram models. Recurrent neural networks (RNNs) encode sentences by reading words in sequential order. Recursive neural networks Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
(RvNNs 1 )), on which this paper focuses, rely on structured input (e.g. parse tree) to encode sentences, based on the intuition that there is significant semantics in the hierarchical structure of words. It is also notable that RvNNs are generalization of RNNs, as linear chain structures on which RNNs operate are equivalent to left-or right-skewed trees.
Although there is significant benefit in processing a sentence in a tree-structured recursive manner, data annotated with parse trees could be expensive to prepare and hard to be computed in batches). Furthermore, the optimal hierarchical composition of words might differ depending on the properties of a task.
In this paper, we propose Gumbel Tree-LSTM, which is a novel RvNN architecture that does not require structured data and learns to compose task-specific tree structures without explicit guidance. Our Gumbel Tree-LSTM model is based on tree-structured long short-term memory (Tree-LSTM) architecture, which is one of the most renowned variants of RvNN.
To learn how to compose task-specific tree structures without depending on structured input, our model introduces composition query vector that measures validity of a composition. Using validity scores computed by the composition query vector, our model recursively selects compositions until only a single representation remains. We use Straight-Through (ST) Gumbel-Softmax estimator to sample compositions in the training phase. ST Gumbel-Softmax estimator relaxes the discrete sampling operation to be continuous in the backward pass, thus our model can be trained via the standard backpropagation. Also, since the computation is performed layer-wise, our model is easy to implement and naturally supports batched computation.
From experiments on natural language inference and sentiment analysis tasks, we find that our proposed model outperforms or is at least comparable to previous sentence encoder models and converges significantly faster than them.
The contributions of our work are as follows:
? We designed a novel sentence encoder architecture that learns to compose task-specific trees from plain text data.
? We showed from experiments that the proposed architecture outperforms or is competitive to state-of-the-art models. We also observed that our model converges faster than others.
? Specifically, we saw that our model significantly outperforms previous RvNN works trained on parse trees in all conducted experiments, from which we hypothesize that syntactic parse tree may not be the best structure for every task and the optimal structure could differ per task.
In the next section, we briefly introduce previous works which have similar objectives to that of our work. Then we describe the proposed model in detail and present findings from experiments. Lastly we summarize the over all content and discuss future work.

Related Work
There have been several works that aim to learn hierarchical latent structure of text by recursively composing words into sentence representation. Some of them carry unsupervised learning on structures by making composition operations soft. To the best of our knowledge, gated recursive convolutional neural network (grConv)) is the first model of its kind and used as an encoder for neural machine translation. The grConv architecture uses gating mechanism to control the information flow from children to parent. grConv and its variants are also applied to sentence classification tasks. Neural tree indexer (NTI) utilizes soft hierarchical structures by using Tree-LSTM instead of grConv.
Although models that operate with soft structures are naturally capable of being trained via backpropagation, the structures predicted by them are ambiguous and thus it is hard to interpret them. CYK Tree-LSTM resolves this ambiguity while maintaining the soft property by introducing the concept of CYK parsing algorithm). Though their model reduces the ambiguity by explicitly representing anode as a weighted sum of all candidate compositions, it is memory intensive since the number of candidates linearly increases by depth.
On the other hand, there exist some previous works that maintain the discreteness of tree composition processes, instead of relying on the soft hierarchical structure. The architecture proposed by greedily selects two adjacent nodes whose reconstruction error is the smallest and merges them into the parent. In their work, rather than directly optimized on classification loss, a composition function is optimized to minimize reconstruction error. introduce reinforcement learning to achieve the desired effect of discretization. They show that REINFORCE algorithm can be used in estimating gradients to learn a tree composition function minimizing classification error. However, slow convergence due to the reinforcement learning setting is one of its drawbacks, according to the authors.
In the research are a outside the RvNN, compositionality in vector space also has been a longstanding subject Dell'Arciprete 2012, to name a few). And more recently, there exist works aiming to learn hierarchical latent structure from unstructured data).

Model Description
Our proposed architecture is built based on the treestructured long short-term memory network architecture. We introduce several additional components into the Tree-LSTM architecture to allow the model to dynamically compose tree structure in a bottom-up manner and to effectively encode a sentence into a vector. In this section, we describe the components of our model in detail.

Tree-LSTM
Tree-structured long short-term memory network (Tree-LSTM) is an elegant variant of RvNN, where it controls information flow from children to parent using similar mechanism to long short-term memory (LSTM). Tree-LSTM introduces cell state in computing parent representation, which assists each cell to capture distant vertical dependencies.
The following are formulae that our model uses to compute parent representation from its children:
where W comp ? R 5D h ?2D h b comp ? R 2D h , and is the element-wise product. Note that our formulation is akin to that of SPINN), but our version does not include the tracking LSTM. Instead, our model can apply an LSTM to leaf nodes, which we will soon describe.

Gumbel-Softmax
Gumbel-Softmax (Jang, Gu, and Poole 2017) (or Concrete distribution) is a method of utilizing discrete random variables in a network. Since it approximates one-hot vectors sampled from a categorical distribution by making them continuous, gradients of model parameters can be calculated using the reparameterization trick and the standard backpropagation. Gumbel-Softmax is known to have an advantage over score-functionbased gradient estimators such as REINFORCE which suffer from high variance and slow convergence. Gumbel-Softmax distribution is motivated by Gumbel-Max trick, an algorithm for sampling from a categorical distribution. Consider
Figure 1: Visualization of forward and backward computation path of ST Gumbel-Softmax. In the forward pass, a model can maintain sparseness due to arg max operation. In the backward pass, since there is no discrete operation, the error signal can backpropagate.
a k-dimensional categorical distribution whose class probabilities p 1 , ? ? ? , pk are defined in terms of unnormalized log probabilities ? 1 , ? ? ? , ? k :
Then a one-hot sample z = (z 1 , ? ? ? , z k ) ? R k from the distribution can be easily drawn by the following equations:
Here, g i , namely Gumbel noise, perturbs each log(? i ) term so that taking arg max becomes equivalent to drawing a sample weighted on p 1 , ? ? ? , pk . In Gumbel-Softmax, the discontinuous arg max function of Gumbel-Max trick is replaced by the differentiable softmax function. That is, given unnormalized probabilities ? 1 , ? ? ? , ? k , a sample y = (y 1 , ? ? ? , y k ) from the Gumbel-Softmax distribution is drawn by
where ? is a temperature parameter; as ? diminishes to zero, a sample from the Gumbel-Softmax distribution becomes cold and resembles the one-hot sample. Straight-Through (ST) Gumbel-Softmax estimator, whose name reminds of Straight-Through estimator (STE), is a discrete version of the continuous Gumbel-Softmax estimator. Similar to the STE, it maintains sparsity by taking different paths in the forward and backward propagation. Obviously ST estimators are biased, however they perform well in practice, according to several previous works and our own result.
In the forward pass, it discretizes a continuous probability vector y sampled from the Gumbel-Softmax distribution into the one-hot vector
And in the backward pass it simply uses the continuous y, thus the error signal is still able to backpropagate. See for the visualization of the forward and backward pass. ST Gumbel-Softmax estimator is useful when a model needs to utilize discrete values directly, for example in the case that a model alters its computation path based on samples drawn from a categorical distribution.

Gumbel Tree-LSTM
In our Gumbel Tree-LSTM model, an input sentence composed of N words is represented as a sequence of word vectors (x 1 , ? ? ? , x N ), where xi ? R Dx . Our basic model applies an affine transformation to each xi to obtain the initial hidden and cell state:
which we call leaf transformation. In Eq. 10, W leaf ? R 2D h ?Dx and b leaf ? R 2D h . Note that we denote the representation of i-th node at t-th layer as rt i = ht i ; ct i . Assume that t-th layer consists of Mt node representations: (r t 1 , ? ? ? , rt Mt ). If two adjacent nodes, say rt i and rt i+1 , are selected to be merged, then Eqs. 1-3 are applied by assuming [h l ; cl ] = rt i and [h r ; c r ] = rt i+1 to obtain the parent representation [h p ; c p ] = r t+1 i . Node representations which are not selected are copied to the corresponding positions at layer t+1. In other words, the (t+1)-th layer is composed of
This procedure is repeated until the model reaches N -th layer and only a single node is left. It is notable that the property of selecting the best node pair at each stage resembles that of easy-first parsing. For implementation-wise details, please see the supplementary material. . Then the validity score of each candidate is computed using the query vector q (denoted as v 1 , v 2 , v 3 ). In the training time, the model samples a parent node among candidates weighted on v 1 , v 2 , v 3 , using ST Gumbel-Softmax estimator, and in the testing time the model selects the candidate with the highest validity. At layer t + 1 (the top layer), the representation of the selected candidate ('the cat') is used as a parent, and the rest are copied from those of layer t ('sat', 'on'). Best viewed in color.
Parent selection. Since information about the tree structure of an input is not given to the model, a special mechanism is needed for the model to learn to compose taskspecific tree structures in an end-to-end manner. We now describe the mechanism for building up the tree structure from an unstructured sentence. First, our model introduces the trainable composition query vector q ? RD h . The composition query vector measures how valid a representation is. Specifically, the validity score of a representation r = [h; c] is defined by q ? h.
At layer t, the model computes candidates for the parent representations using Eqs. 1-3: (r t+1 1 , ? ? ? ,r t+1 Mt+1 ). Then, it calculates the validity score of each candidate and normalize it so that
In the training phase, the model samples a parent from candidates weighted on vi , using the ST Gumbel-Softmax estimator described above. Since the continuous Gumbel-Softmax function is used in the backward pass, the error backpropagation signal safely passes through the sampling operation, hence the model is able to learn to construct the task-specific tree structures that minimize the loss by backpropagation.
In the validation (or testing) phase, the model simply selects the parent which maximizes the validity score.
An example of the parent selection is depicted in.
LSTM-based leaf transformation. The basic leaf transformation using an affine transformation (Eq. 10) does not consider information about the entire sentence of an input and thus the parent selection is performed based only on local information.
SPINN) addresses this issue by using the tracking LSTM which sequentially reads input words. The tracking LSTM makes the SPINN model hybrid, where the model takes advantage of both tree-structured composition and sequential reading. However, the tracking LSTM is not applicable to our model, since our model does not use shift-reduce parsing or maintain a stack. In the tracking LSTM's stead, our model applies an LSTM on input representations to give information about previous words to each leaf node:
where h 1 0 = c 1 0 = 0. From the experimental results, we validate that the LSTM applied to leaf nodes has a substantial gain over the basic leaf transformer.

Experiments
We evaluate performance of the proposed Gumbel Tree-LSTM model on two tasks: natural language inference and sentiment analysis. The implementation is made publicly available. The detailed experimental settings are described in the supplementary material.

Natural Language Inference
Natural language inference (NLI) is a task of predicting the relationship between two sentences (hypothesis Model Accuracy (%) # Params Time (hours) 100D Latent Syntax Tree-LSTM  80.5 500k 72-96 * 100D CYK Tree-LSTM 81  and premise). In the Stanford Natural Language Inference (SNLI) dataset, which we use for NLI experiments, a relationship is either contradiction, entailment, or neutral. For a model to correctly predict the relationship between two sentences, it should encode semantics of sentences accurately, thus the task has been used as one of standard tasks for evaluating the quality of sentence representations. The SNLI dataset is composed of about 550,000 sentences, each of which is binary-parsed. However, since our model operate on plain text, we do not use the parse tree information in both training and testing. The classifier architecture used in our SNLI experiments follows. Given the premise sentence vector (h pre ) and the hypothesis sentence vector (h hyp ) which are encoded by the proposed Gumbel Tree-LSTM model, the probability of relationship r ? {entailment, contradiction, neutral} is computed by the following equations:
where W r clf ? R 1?Dc , b r clf ? R 1 , and ? is a multi-layer perceptron (MLP) with the rectified linear unit (ReLU) activation function.
For  We also apply dropout on the word vectors with probability 0.1. Similar to 100D experiments, we initialize the word embedding matrix with GloVe 300D pretrained vectors 4 , however we do not update the word representations during training.
Since our model converges relatively fast, it is possible to train a model of larger size in a reasonable time. In the 600D experiment, we set D x = 300, D h = 600, and an MLP with three hidden layers (D c = 1024) is used. The dropout probability is set to 0.2 and word embeddings are not updated during training.
The size of mini-batches is set to 128 in all experiments, and hyperparameters are tuned using the validation split. The temperature parameter ? of Gumbel-Softmax is set to 1.0, and we did not find that temperature annealing improves performance. For training models, Adam optimizer  is used.
The results of SNLI experiments are summarized in. First, we can see that LSTM-based leaf transformation has a clear advantage over the affine-transformation-based one. It improves the performance substantially and also leads to faster convergence.
Secondly, comparing ours with other models, we find that our 100D and 300D model outperform all other models of similar numbers of parameters. Our 600D model achieves the accuracy of 86.0%, which is comparable to that of the state-of-the-art model, while using far less parameters.
It is also worth noting that our models converge much faster than other models. All of our models converged within a few hours on a machine with NVIDIA Titan Xp GPU.
We also plot validation accuracies of various models during first 5 training epochs in, and validate that our Model SST-2 (%) SST-5 (%) DMN 88.6 52.1 NSE 89.7 52.8 byte-mLSTM 91.8 52.9 BCN+Char+CoVe 90.3 53.7 RNTN 85.4 45.7 Constituency Tree-LSTM 88.0 51.0 NTI-SLSTM-LSTM 89.3 53.1 Latent Syntax Tree-LSTM  86.5 -Constituency Tree-LSTM + Recurrent Dropout 89.4 52.3 Gumbel Tree-LSTM (Ours) 90.7 53.7 models converge significantly faster than others, not only in terms of total training time but also in the number of iterations. 5

Sentiment Analysis
To evaluate the performance of our model in single-sentence classification, we conducted experiments on Stanford Sentiment Treebank (SST) dataset. In the SST dataset, each sentence is represented as a binary parse tree, and each subtree of a parse tree is annotated with the corresponding sentiment score. Following the experimental setting of previous works, we use all subtrees and their labels for training, and only the root labels are used for evaluation. The classifier has a similar architecture to SNLI experiments. Specifically, for a sentence embedding h, the probability for the sentence to be predicted as label s ? {0, 1} (in the binary setting, SST-2) or s ? {1, 2, 3, 4, 5} (in the fine-grained setting, SST-5) is computed as follows:
where W s clf ? R 1?Dc , b s clf ? R 1 , and ? is a single-hidden layer MLP with the ReLU activation function. Note that subtrees labeled as neutral are ignored in the binary setting in both training and evaluation.
We trained our SST-2 model with hyperparameters D x = 300, D h = 300, D c = 300. The word vectors are initialized with GloVe 300D pretrained vectors and fine-tuned during training. We apply dropout (p = 0.5) on the output of the word embedding layer and the input and the output of the MLP layer. The size of mini-batches is set to 32 and Adadelta  optimizer is used for optimization.
For our SST-5 model, hyperparameters are set to D x = 300, D h = 300, D c = 1024. Similar to the SST-2 model, we optimize the model using Adadelta optimizer with batch size 64 and apply dropout with p = 0.5. summarizes the results of SST experiments. Our SST-2 model outperforms all other models substantially except byte-mLSTM, where a byte-level language model trained on the large product review dataset is used to obtain sentence representations.
We also see that the performance of our SST-5 model is on par with that of the current state-of-the-art model, which is pretrained on large parallel datasets and uses character n-gram embeddings alongside word embeddings, even though our model does not utilize external resources other than GloVe vectors and only uses wordlevel representations. The authors of stated that utilizing pretraining and character n-gram embeddings improves validation accuracy by 2.8% (SST-2) or 1.7% (SST-5).
In addition, from the fact that our models substantially outperform all other RvNN-based models, we conjecture that task-specific tree structures built by our model help encode sentences into vectors more efficiently than constituency-based or dependency-based parse trees do.

Qualitative Analysis
We conduct a set of experiments to observe various properties of our trained models. First, to see how well the model encodes sentences with similar meaning or syntax into close vectors, we find nearest neighbors of a query sentence. Second, to validate that the trained composition functions are non-trivial and task-specific, we visualize trees composed by SNLI and SST model given identical sentence.
Nearest neighbors We encode sentences in the test split of SNLI dataset using the trained 300D model and find nearest neighbors given a query sentence. presents five nearest neighbors for each selected query sentence. In finding nearest neighbors, cosine distance is used as metric. The result shows that our model effectively maps similar sentences into vectors close to each other; the neighboring sentences are similar to a query sentence not only in terms of word overlap, but also in semantics. For example in the second column, the nearest sentence is 'the woman is looking at a dog', whose meaning is almost same as the query sentence. We can also see that other neighbors partially share semantics with the query sentence. # sunshine is on a man 's face . a girl is staring at a dog .
the woman is wearing boots . 1 a man is walking on sunshine .
the woman is looking at a dog . the girl is wearing shoes 2 a guy is in a hot , sunny place a girl takes a photo of a dog . a person is wearing boots . 3 a man is working in the sun . a girl is petting her dog . the woman is wearing jeans . 4 it is sunny . a man is taking a picture of a dog , while a woman watches . a woman wearing sunglasses .
5 a man enjoys the sun coming through the window . a woman is playing with her dog . the woman is wearing a vest .  Tree examples show that two models (300D SNLI and SST-2) generate different tree structures given an identical sentence. In and 4b, the SNLI model groups the phrase 'i love this' first, while the SST model groups 'this very much' first. and 4d present how differently the two models process a sentence containing relative pronoun 'which'. It is intriguing that the models compose visually plausible tree structures, where the sentence is divided into two phrases by relative pronoun, even though they are trained without explicit parse trees. We hypothesize that these examples demonstrate that each model generates a distinct tree structure based on semantic properties of the task and learns non-trivial tree composition scheme.

Conclusion
In this paper, we propose Gumbel Tree-LSTM, a novel Tree-LSTM-based architecture that learns to compose taskspecific tree structures. Our model introduces the composition query vector to compute validity of the candidate parents and selects the appropriate parent according to validity scores. In training time, the model samples the parent from candidates using ST Gumbel-Softmax estimator, hence it is able to be trained by standard backpropagation while maintaining its property of discretely determining the computation path in forward propagation. From experiments, we validate that our model outperforms all other RvNN models and is competitive to state-ofthe-art models, and also observed that our model converges faster than other complex models. The result poses an important question: what is the optimal input structure for RvNN? We empirically showed that the optimal structure might differ per task, and investigating task-specific latent tree structures could bean interesting future research direction.
For future work, we plan to apply the core idea beyond sentence encoding. The performance could be further improved by applying intra-sentence or inter-sentence attention mechanisms. We also plan to design an architecture that generates sentences using recursive structures.

SST
The composition query vector is initialized by sampling from Gaussian distribution N (0, 0.01 2 ). The last linear transformation that outputs the unnormalized log probability for each class is initialized by sampling from uniform distribution U(?0.002, 0.002). All other parameters are initialized following the scheme proposed by. We used Adadelta optimizer  with default hyperparameters and halved learning rate if there is no improvement in accuracy for two epochs. In both SST-2 and SST-5 experiments, we set D x = D h = 300, used GloVe (840B, 300D) pretrained vectors with fine-tuning, and single-hidden layer MLP is used as classifier. Dropout is applied to word embeddings and the input and the output of the MLP classifier with probability 0.5. In the SST-2 experiment, we set D c to 300 and set batch size to 32. In the SST-5 experiment, D c is increased to 1024, and mini-batches of 64 sentences are fed to the model during training.