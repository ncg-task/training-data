156	26	28	of
156	33	45	single model
156	46	48	on
156	49	64	SQ u AD dev set
157	61	71	contribute
157	72	79	towards
157	84	104	model 's performance
157	11	15	both
157	16	36	syntactic embeddings
157	41	60	semantic embeddings
157	113	121	POS tags
157	127	132	to be
157	133	147	more important
160	0	12	For ablating
160	13	36	integral query matching
160	43	49	result
160	50	55	drops
160	56	61	about
160	62	65	2 %
160	89	99	shows that
160	104	124	integral information
160	125	127	of
160	128	133	query
160	134	137	for
160	138	147	each word
160	148	150	in
160	151	158	passage
160	159	161	is
160	162	169	crucial
161	4	37	query - based similarity matching
161	38	50	accounts for
161	51	85	about 10 % performance degradation
161	94	100	proves
161	105	118	effectiveness
161	119	121	of
161	122	145	alignment context words
161	146	153	against
161	154	159	query
162	0	3	For
162	4	39	context - based similarity matching
162	52	60	took out
162	65	68	M 3
162	69	73	from
162	78	93	linear function
162	104	116	proved to be
162	117	129	contributory
162	130	132	to
162	137	148	performance
162	149	151	of
162	152	179	full - orientation matching
110	4	14	tokenizers
110	18	24	use in
110	29	33	step
110	34	36	of
110	37	50	preprocessing
110	51	55	data
110	60	64	from
110	65	81	Stanford CoreNLP
111	106	118	to transform
111	123	143	passage and question
111	76	78	in
111	79	105	Stanford CoreNLP utilities
111	8	11	use
111	12	37	part - of - speech tagger
111	42	75	named - entity recognition tagger
112	0	3	For
112	8	25	skip - gram model
112	28	37	our model
112	38	47	refers to
112	52	68	word2 vec module
112	69	71	in
112	72	100	open source software library
112	103	113	Tensorflow
112	120	131	skip window
112	135	141	set as
112	142	143	2
118	8	23	memory networks
118	29	32	set
118	37	52	number of layer
118	53	55	as
118	56	57	3
114	0	10	To improve
114	15	40	reliability and stabllity
114	46	56	screen out
114	61	70	sentences
114	71	76	whose
114	77	83	length
114	84	87	are
114	88	102	shorter than 9
115	3	6	use
115	7	26	100 one dimensional
115	27	34	filters
115	35	38	for
115	39	42	CNN
115	43	45	in
115	50	75	character level embedding
115	78	82	with
115	83	88	width
115	89	91	of
115	92	93	5
115	94	97	for
115	98	106	each one
117	11	47	AdaDelta ( Zeiler , 2012 ) optimizer
117	48	52	with
117	55	76	initial learning rate
117	77	79	as
117	80	85	0.001
116	3	6	set
116	11	22	hidden size
116	23	25	as
116	26	29	100
116	30	33	for
116	34	61	all the LSTM and GRU layers
116	66	71	apply
116	72	79	dropout
116	80	87	between
116	88	94	layers
116	95	99	with
116	102	115	dropout ratio
116	116	118	as
116	119	122	0.2
27	19	28	introduce
27	33	87	Multi - layer Embedding with Memory Networks ( MEMEN )
27	93	122	end - to - end neural network
27	123	126	for
27	127	153	machine comprehension task
28	10	21	consists of
28	22	33	three parts
29	8	16	encoding
29	17	19	of
29	20	37	context and query
29	40	48	in which
29	52	55	add
29	56	97	useful syntactic and semantic information
29	98	100	in
29	105	114	embedding
29	115	117	of
29	118	128	every word
29	139	182	high - efficiency multilayer memory network
29	183	185	of
29	186	213	full - orientation matching
29	214	222	to match
29	227	247	question and context
29	258	314	pointer - network based answer boundary prediction layer
29	315	321	to get
29	326	334	location
29	335	337	of
29	342	348	answer
29	349	351	in
29	356	363	passage
2	55	76	Machine Comprehension
4	0	53	Machine comprehension ( MC ) style question answering
12	0	28	Machine comprehension ( MC )
129	10	13	see
129	19	28	our model
129	29	40	outperforms
129	41	60	all other baselines
129	65	73	achieves
129	78	107	state - of - the - art result
129	108	110	on
129	111	134	all subsets on TriviaQA
132	8	11	use
132	16	67	Stanford Question Answering Dataset ( SQuAD ) v 1.1
132	68	70	to
138	114	123	our model
138	202	204	is
138	205	216	competitive
138	220	249	state - of - the - art method
138	124	132	achieves
138	136	153	exact match score
138	12	14	of
138	157	164	75.37 %
138	172	180	F1 score
138	154	156	of
138	184	193	82 . 66 %
