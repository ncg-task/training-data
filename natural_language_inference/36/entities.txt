168	0	18	Textual Entailment
172	11	15	show
172	21	34	SRL embedding
172	39	44	boost
172	49	66	ESIM + ELMo model
172	67	69	by
172	70	89	+ 0.7 % improvement
173	0	4	With
173	9	22	semantic cues
173	29	61	simple sequential encoding model
173	62	68	yields
173	69	86	substantial gains
173	93	120	our single BERT LARGE model
173	126	134	achieves
173	137	160	new stateof - the - art
173	168	179	outperforms
173	180	203	all the ensemble models
173	204	206	in
173	211	222	leaderboard
181	0	29	Machine Reading Comprehension
186	13	21	includes
186	22	26	MQAN
186	27	30	for
186	31	57	single task and multi-task
186	58	62	with
186	63	66	SRL
186	69	81	BiDAF + ELMo
186	84	95	R.M. Reader
186	100	104	BERT
188	8	22	SRL embeddings
188	23	27	give
188	28	57	substantial performance gains
188	58	62	over
188	63	87	all the strong baselines
188	90	97	showing
188	109	124	quite effective
188	125	128	for
188	129	172	more complex document and question encoding
26	93	100	explore
26	101	119	integrative models
26	120	123	for
26	124	172	finer - grained text comprehension and inference
27	18	25	propose
27	28	59	semantics enhancement framework
27	60	63	for
27	64	72	TC tasks
28	3	12	implement
28	16	40	easy and feasible scheme
28	41	53	to integrate
28	54	70	semantic signals
28	71	73	in
28	74	98	downstream neural models
28	99	101	in
28	102	123	end - to - end manner
28	124	132	to boost
28	150	161	effectively
28	133	149	strong baselines
2	34	52	Text Comprehension
12	31	56	text comprehension ( TC )
12	65	102	machine reading comprehension ( MRC )
12	107	132	textual entailment ( TE )
