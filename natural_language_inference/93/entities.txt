132	86	107	positively contribute
132	108	110	to
132	115	128	final results
132	129	131	of
132	132	162	gated self - matching networks
132	0	45	attention - based recurrent network ( GARNN )
132	50	85	self - matching attention mechanism
134	0	25	Characterlevel embeddings
134	26	44	contribute towards
134	49	69	model 's performance
134	79	82	can
134	83	96	better handle
134	97	124	out - ofvocab or rare words
137	0	28	Character - level embeddings
137	33	36	not
137	37	45	utilized
138	18	22	gate
138	23	36	introduced in
138	37	72	question and passage matching layer
138	76	83	helpful
138	84	87	for
138	93	105	GRU and LSTM
133	0	8	Removing
133	9	24	self - matching
133	25	35	results in
133	36	53	3.5 point EM drop
108	3	6	use
108	11	20	tokenizer
108	21	25	from
108	26	42	Stanford CoreNLP
108	43	56	to preprocess
108	57	82	each passage and question
112	4	24	hidden vector length
112	28	34	set to
112	35	37	75
112	38	41	for
112	42	52	all layers
113	4	15	hidden size
113	21	31	to compute
113	32	48	attention scores
113	49	51	is
113	57	59	75
109	4	32	Gated Recurrent Unit variant
109	33	35	of
109	36	40	LSTM
109	44	59	used throughout
109	60	69	our model
115	4	9	model
115	13	27	optimized with
115	28	54	AdaDelta ( Zeiler , 2012 )
115	55	59	with
115	63	84	initial learning rate
115	85	87	of
115	88	89	1
110	0	3	For
110	4	18	word embedding
110	24	27	use
110	28	72	pretrained case - sensitive GloVe embeddings
110	104	107	for
110	113	135	questions and passages
110	148	153	fixed
110	154	160	during
110	161	169	training
110	179	191	zero vectors
110	192	204	to represent
110	205	231	all out - of - vocab words
111	3	10	utilize
111	11	40	1 layer of bi-directional GRU
111	41	51	to compute
111	52	80	character - level embeddings
111	85	115	3 layers of bi-directional GRU
111	116	125	to encode
111	126	148	questions and passages
111	155	196	gated attention - based recurrent network
111	197	200	for
111	201	230	question and passage matching
111	239	246	encoded
111	247	262	bidirectionally
114	8	13	apply
114	14	21	dropout
114	22	29	between
114	30	36	layers
114	37	41	with
114	44	56	dropout rate
114	57	59	of
114	60	63	0.2
116	10	17	used in
116	18	26	AdaDelta
116	27	30	are
116	31	46	0.95 and 1e ? 6
22	17	26	introduce
22	29	58	gated self - matching network
22	81	116	end - to - end neural network model
22	117	120	for
22	121	165	reading comprehension and question answering
23	10	21	consists of
23	22	32	four parts
24	8	33	recurrent network encoder
24	34	42	to build
24	43	57	representation
24	58	61	for
24	62	84	questions and passages
24	106	126	gated matching layer
24	127	135	to match
24	140	160	question and passage
24	171	192	self - matching layer
24	193	205	to aggregate
24	206	217	information
24	218	222	from
24	227	240	whole passage
24	255	308	pointernetwork based answer boundary prediction layer
2	35	79	Reading Comprehension and Question Answering
4	63	109	reading comprehension style question answering
