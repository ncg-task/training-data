(Contribution||has||Experiments)
(Experiments||has||Tasks)
(Tasks||has||Natural Language Inference)
(Natural Language Inference||has||Experimental setup)
(Experimental setup||used||mini- batch size)
(mini- batch size||set to||16 or 32)
(Experimental setup||used||Out - of - vocabulary ( OOV ) words)
(Out - of - vocabulary ( OOV ) words||initialized||randomly)
(randomly||with||Gaussian samples)
(Experimental setup||used||Adam ( Kingma and Ba , 2015 ))
(Adam ( Kingma and Ba , 2015 )||with||initial learning rate)
(initial learning rate||set to||1E - 3)
(Adam ( Kingma and Ba , 2015 )||with||two momentum parameters)
(two momentum parameters||set to||0.9 and 0.999)
(Adam ( Kingma and Ba , 2015 )||for||optimization)
(Experimental setup||used||pre-trained 300 - D Glove 840B vectors)
(pre-trained 300 - D Glove 840B vectors||to initialize||word embeddings)
(Experimental setup||has||dropout rate)
(dropout rate||selected from||[ 0.1 , 0.2 , 0.3 , 0.4 ])
(Experimental setup||updated||OOV vectors)
(OOV vectors||in||first epoch)
(first epoch||after||all word embeddings)
(all word embeddings||updated||normally)
(Natural Language Inference||has||Results)
(Results||has||LSTMNs)
(LSTMNs||achieve||better performance)
(Results||observe||fusion)
(fusion||is||generally beneficial)
(Results||observe||deep fusion)
(deep fusion||has||slightly improves)
(slightly improves||over||shallow fusion)
(Results||With||standard training)
(standard training||has||deep fusion)
(deep fusion||yields||state - of - the - art performance)
(Natural Language Inference||has||Baselines)
(Baselines||has||shared LSTM ( Rocktschel et al. , 2016 ))
(Baselines||has||word - by - word attention model)
(Baselines||has||matching LSTM ( m LSTM ; ))
(Baselines||compared||bag - of - words baseline)
(Tasks||has||Language Modeling)
(Language Modeling||has||Hyperparameters)
(Hyperparameters||used||stochastic gradient descent)
(stochastic gradient descent||with||initial learning rate)
(initial learning rate||of||0.65)
(stochastic gradient descent||for||optimization)
(stochastic gradient descent||has||decays)
(decays||by||factor)
(factor||of||0.85)
(factor||per||epoch)
(Hyperparameters||has||renormalize)
(renormalize||has||gradient)
(gradient||if||norm)
(norm||greater than||5)
(Hyperparameters||has||mini - batch size)
(mini - batch size||set to||40)
(Hyperparameters||has||dimensions)
(dimensions||of||word embeddings)
(dimensions||set to||150)
(150||for||all models)
(Language Modeling||has||Results)
(Results||Amongst||all deep architectures)
(all deep architectures||has||three - layer LSTMN)
(three - layer LSTMN||performs||best)
(Language Modeling||has||Baselines)
(Baselines||has||gated - feedback LSTM)
(gated - feedback LSTM||has||feedback gates)
(feedback gates||connecting||hidden states)
(hidden states||across||multiple time steps)
(hidden states||as||adaptive control)
(adaptive control||of||information flow)
(Baselines||has||Kneser - Ney 5 - gram language model ( KN5 ))
(Kneser - Ney 5 - gram language model ( KN5 )||serves as||non-neural baseline)
(non-neural baseline||for||language modeling task)
(Baselines||has||depth - gated LSTM)
(depth - gated LSTM||uses||depth gate)
(depth gate||to connect||memory cells)
(memory cells||of||vertically adjacent layers)
(Tasks||has||Sentiment Analysis)
(Sentiment Analysis||has||Hyperparameters)
(Hyperparameters||used||pretrained 300 - D Glove 840B vectors)
(pretrained 300 - D Glove 840B vectors||to initialize||word embeddings)
(Hyperparameters||used||Adam ( Kingma and Ba , 2015 ))
(Adam ( Kingma and Ba , 2015 )||for||optimization)
(optimization||with||two momentum parameters)
(two momentum parameters||set to||0.9 and 0.999)
(Hyperparameters||has||regularization constant)
(regularization constant||was||1E - 4)
(Hyperparameters||has||mini-batch size)
(mini-batch size||was||5)
(Hyperparameters||has||initial learning rate)
(initial learning rate||set to||2E - 3)
(Hyperparameters||has||gradient)
(gradient||for||words)
(words||with||Glove embeddings)
(gradient||scaled by||0.35)
(0.35||in||first epoch)
(first epoch||after||all word embeddings)
(all word embeddings||updated||normally)
(Hyperparameters||has||dropout rate)
(dropout rate||of||0.5)
(dropout rate||applied to||neural network classifier)
(Sentiment Analysis||has||Results)
(Results||show that||both 1 - and 2 - layer LSTMNs)
(both 1 - and 2 - layer LSTMNs||has||outperform)
(outperform||has||LSTM baselines)
(Results||On||fine - grained and binary classification tasks)
(fine - grained and binary classification tasks||has||our 2 - layer LSTMN)
(our 2 - layer LSTMN||performs||close)
(close||to||best system)
(Sentiment Analysis||has||Baselines)
(Baselines||are||LSTM variants)
(Baselines||report||performance)
(performance||of||paragraph vector model)
