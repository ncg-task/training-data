title
Robust Multilingual Part - of - Speech Tagging via Adversarial Training
abstract
Adversarial training ( AT ) 1 is a powerful regularization method for neural networks , aiming to achieve robustness to input perturbations .
Yet , the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing .
In this paper , we propose and analyze a neural POS tagging model that exploits AT .
In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies ( UD ) dataset ( 27 languages ) , we find that AT not only improves the over all tagging accuracy , but also 1 ) prevents over - fitting well in low resource languages and 2 ) boosts tagging accuracy for rare / unseen words .
We also demonstrate that 3 ) the improved tagging performance by AT contributes to the downstream task of dependency parsing , and that 4 ) AT helps the model to learn cleaner word representations .
5 ) The proposed AT model is generally effective in different sequence labeling tasks .
These positive results motivate further use of AT for natural language tasks .
Introduction
Recently , neural network - based approaches have become popular in many natural language processing ( NLP ) tasks including tagging , parsing , and translation .
However , it has been shown that neural networks tend to be locally unstable and even tiny perturbations to the original inputs can mislead the models .
Such maliciously perturbed inputs are called adversarial examples .
Adversarial training aims to improve the robustness of a model to input perturbations by training on both unmodified examples and adversarial examples .
Previous work
We distinguish AT from Generative Adversarial Networks ( GANs ) .
Figure 1 : Illustration of our architecture for adversarial POS tagging .
Given a sentence , we input the normalized word embeddings ( w 1 , w 2 , w 3 ) and character embeddings ( showing c 1 , c 2 , c 3 for w 1 ) .
Each word is represented by concatenating its word embedding and its character - level BiLSTM output .
They are fed into the main BiLSTM - CRF network for POS tagging .
In adversarial training , we compute and add the worst - case perturbation ?
to all the input embeddings for regularization .
on image recognition has demonstrated the enhanced robustness of their models to unseen images via adversarial training and has provided theoretical explanations of the regularization effects .
Despite its potential as a powerful regularizer , adversarial training ( AT ) has yet to be explored extensively in natural language tasks .
Recently , applied AT on text classification , achieving state - of - the - art accuracy .
Yet , the specific effects of the robustness obtained from AT are still unclear in the context of NLP .
For example , research studies have yet to answer questions such as 1 ) how can we interpret perturbations or robustness on natural language inputs ?
2 ) how are they related to linguistic factors like vocabulary statistics ?
3 ) are the effects of AT language - dependent ?
Answering such questions is crucial to understand and motivate the application of adversarial training on natural language tasks .
In this paper , spotlighting a well - studied core problem of NLP , we propose and carefully analyze a neural part - of - speech ( POS ) tagging model that exploits adversarial training .
With a BiLSTM - CRF model as our baseline POS tagger , we apply adversarial training by considering perturbations to input word / character embeddings .
In order to demystify the effects of adversarial training in the context of NLP , we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus ( Englsih ) and the Universal Dependencies dataset ( 27 languages ) , with thorough analyses of the following points :
Effects on different target languages Vocabulary statistics and tagging accuracy Influence on downstream tasks Representation learning of words
In our experiments , we find that our adversarial training model consistently outperforms the baseline POS tagger , and even achieves state - of - the - art results on 22 languages .
Furthermore , our analyses reveal the following insights into adversarial training in the context of NLP :
The regularization effects of adversarial training ( AT ) are general across different languages .
AT can prevent overfitting especially well when training examples are scarce , providing an effective tool to process low resource languages .
AT can boost the tagging performance for rare / unseen words and increase the sentence - level accuracy .
This positively affects the performance of down - stream tasks such as dependency parsing , where low sentence - level POS accuracy can be a bottleneck .
AT helps the network learn cleaner word embeddings , showing stronger correlations with their POS tags .
We argue that the effects of AT can be interpreted from the perspective of natural language .
Finally , we demonstrate that the proposed AT model is generally effective across different sequence labeling tasks .
This work therefore provides a strong motivation and basis for utilizing adversarial training in NLP tasks .
2 Related Work
POS
Tagging
Part - of - speech ( POS ) tagging is a fundamental NLP task that facilitates downstream tasks such as syntactic parsing .
While current state - of - theart POS taggers yield accuracy over 97.5 % on PTB - WSJ , there still remain issues .
The per token accuracy metric is easy since taggers can easily assign correct POS tags to highly unambiguous tokens , such as punctuation .
Sentence - level accuracy serves as a more realistic metric for POS taggers but it still remains low .
Another problem with current POS taggers is that their accuracy deteriorates drastically on low resource languages and rare words .
In this work , we demonstrate that adversarial training ( AT ) can mitigate these issues .
It is empirically shown that POS tagging performance can greatly affect downstream tasks such as dependency parsing .
In this work , we also demonstrate that the improvements obtained from our AT POS tagger actually contribute to dependency parsing .
Nonetheless , parsing with gold POS tags still yields better results , bolstering the view that POS tagging is an essential task in NLP that needs further development .
Adversarial Training
Adversarial training is a powerful regularization method , primarily explored in image recognition to improve the robustness of classifiers to input perturbations .
Given a classifier , we first generate input examples thatare very close to original inputs ( so should yield the same labels ) yet are likely to be misclassified by the current model .
Specifically , these adversarial examples are generated by adding small perturbations to the inputs in the direction that significantly increases the loss function of the classifier ( worstcase perturbations ) .
Then , the classifier is trained on the mixture of clean examples and adversarial examples to improve the stability to input perturbations .
In this work , we incorporate adversarial training into our baseline POS tagger , aiming to achieve better regularization effects and to provide their interpretations in the context of NLP .
Generating adversarial examples .
Adversarial training ( AT ) considers continuous perturbations to inputs , so we define perturbations at the level of dense word / character embeddings rather than one - hot vector representations , similarly to .
Specifically , given an input sentence , we consider the concatenation of all the word / character embeddings in the sentence :
To prepare an adversarial example , we aim to generate the worst - case perturbation of a small bounded norm that maximizes the loss function L of the current model :
where ?
is the current value of the model parameters , treated as a constant , and y denotes the target labels .
Since the exact computation of such ?
is intractable in complex neural networks , we employ the Fast Gradient Method i.e. first order approximation to obtain an approximate worst - case perturbation of norm , by a single gradient computation :
is a hyperparameter to be determined in the development dataset .
Note that the perturbation ?
is generated in the direction that significantly increases the loss L. We find such ?
against the current model parameterized by ? , at each training step , and construct an adversarial example by s adv = s + ?
However , if we do not restrict the norm of word / character embeddings , the model could trivially learn embeddings of large norms to make the perturbations insignificant .
To prevent this issue , we normalize word / character embeddings so that they have mean 0 and variance 1 for every entry , as in .
The normalization is performed every time we feed input embeddings into the LSTMs and generate adversarial examples .
To ensure a fair comparison , we also normalize input embeddings in our baseline model .
While set the norm of a perturbation ( Eq 2 ) to be a fixed value for all input sentences , to generate adversarial examples for an entire sentence of a variable length and to include character embeddings besides word embeddings , we make the perturbation size adaptive to the dimension of the concatenated input embedding s ?
RD .
We set to be ? ?
D ( i.e. , proportional to ? D ) , as the expected squared norm of s after the embedding normalization is D .
The scaling factor ?
is selected from { 0.001 , 0.005 , 0.01 , 0.05 , 0.1 } based on the development performance in each treebank .
We used 0.01 for PTB - WSJ and UD - Spanish , and 0.05 for the rest .
Note that ? = 0 would generate no noise ( identical to the baseline ) ; if ? = 1 , the generated adversarial perturbation would have a norm comparable to the original embedding , which could change the semantics of the input sentence .
Hence , the optimal perturbation scale ?
should lie in between and be small enough to preserve the semantics of the original input .
Adversarial training .
At each training step , we generate adversarial examples against the current model , and train on the mixture of clean examples and adversarial examples to achieve robustness to input perturbations .
To this end , we define the loss function for adversarial training as :
where L ( ? ; s , y ) , L (? ; s adv , y ) represent the loss from a clean example and the loss from its adversarial example , respectively , and ?
determines the weighting between them .
We used ? = 0.5 in all our experiments .
This objective function can be optimized with respect to the model parameters ? , in the same manner as the baseline model .
Method
In this section , we introduce our baseline POS tagging model and explain how we implement adversarial training on top .
Baseline POS
Tagging Model
Following the recent top - performing models for sequence labeling tasks , we employ a Bi-directional LSTM - CRF model as our baseline ( see for an illustration ) .
Character - level BiLSTM .
Prior work has shown that incorporating character - level representations of words can boost POS tagging accuracy by capturing morphological information present in each language .
Major neural character - level models include the character - level CNN and ( Bi ) LSTM . A Bi-directional LSTM ( BiLSTM ) processes each sequence both forward and backward to capture sequential information , while preventing the vanishing / exploding gradient problem .
We observed that the character - level BiLSTM outperformed the CNN by 0.1 % on the PTB - WSJ development set , and hence in all of our experiments we use the character - level BiLSTM .
Specifically , we generate a character - level representation for each word by feeding its character embeddings into the BiLSTM and obtaining the concatenated final states .
Word - level BiLSTM .
Each word in a sentence is represented by concatenating its word embedding and its character - level representation .
They are fed into another level of BiLSTM ( word - level BiLSTM ) to process the entire sentence .
CRF .
In sequence labeling tasks it is beneficial to consider the correlations between neighboring labels and jointly decode the best chain of labels for a given sentence .
With this motivation , we apply a conditional random field ( CRF ) on top of the word - level BiLSTM to perform POS tag inference with global normalization , addressing the " label bias " problem .
Specifically , given an input sentence , we pass the output sequence of the word - level BiLSTM to a firstorder chain CRF to compute the conditional probability of the target label sequence :
where ?
represents all of the model parameters ( in the BiLSTMs and CRF ) , sand y denote the input embeddings and the target POS tag sequence , respectively , for the given sentence .
For training , we minimize the negative loglikelihood ( loss function )
with respect to the model parameters .
Decoding searches for the POS tag sequence y * with the highest conditional probability using the Viterbi algorithm .
For more detail about the BiLSTM - CRF formulation , refer to .
Experiments
To fully analyze the effects of adversarial training , we train and evaluate our baseline / adversarial POS tagging models on both a standard English dataset and a multilingual dataset .
Datasets
As a standard English dataset , we use the Wall Street Journal ( WSJ ) portion of the Penn Treebank ( PTB ) , containing 45 different POS tags .
We adopt the standard split : sections 0 - 18 for training , 19 - 21 for development and 22 - 24 for testing .
For multilingual POS tagging experiments , to compare with prior work , we use treebanks from Universal Dependencies ( UD ) v 1.2 ( 17 POS ) with the given data splits .
We experiment on languages for which pre-trained Polyglot word embeddings are available , resulting in 27 languages listed in .
We regard languages with less than 60 k tokens of training data as low - resource ( Table 2 , bottom ) , as in .
Model
Accuracy 97.27 97.28 97.29 97.50 97.78 97.55 97.55 97.55 Ours - Baseline 97.54 Ours - Adversarial 97.58 .
We apply dropout to input embeddings and BiLSTM outputs for both baseline and adversarial training , with dropout rate 0.5 .
Optimization .
We train the model parameters and word / character embeddings by the mini-batch stochastic gradient descent ( SGD ) with batch size 10 , momentum 0.9 , initial learning rate 0.01 and decay rate 0.05 .
We also use a gradient clipping of 5.0 .
The models are trained with early stopping ) based on the development performance .
Evaluation .
We evaluate per token tagging accuracy on test sets .
We repeat the experiment three times and report the statistical significance .
Results
PTB - WSJ dataset .
shows the POS tagging results .
As expected , our baseline ( BiLSTM - CRF ) model ( accuracy 97.54 % ) performs on par with other state - of - the - art systems .
Built upon this baseline , our adversarial training ( AT ) model reaches accuracy 97.58 % thanks to its regularization power , outperforming recent POS taggers except .
The improvement over the baseline is statistically significant , with p-value < 0.05 on the t-test .
We provide additional analysis on this result in later sections .
training of word rarity maybe of particular help in processing morphologically complex words .
Additionally , we see that our AT model achieves notably large improvements over the baseline in resource - poor languages ( the bottom of , with average improvement 0.35 % , as compared to that for resource - rich languages , 0.20 % .
To further visualize the regularization effects , we present the learning curves for three representative languages , English ( WSJ ) , French ( UD - fr ) and Romanian ( UD - ro , low - resource ) , based on the development loss ( see ) .
For all the three languages , we can observe that the AT model ( red solid line ) prevents overfitting better than the baseline ( black dotted line ) , and this advantage is more significant in low resource languages .
For example , in Romanian , the baseline model starts to increase development loss after 1,000 iterations even with dropout , whereas the AT model keeps improving until 2,500 iterations , achieving notably lower development loss ( 0.4 down ) .
These results illustrate that AT can prevent overfitting especially well on small datasets and can augment the regularization power beyond dropout .
AT can also be viewed as an effective means of data augmenta -
Analysis
In the previous sections , we demonstrated the regularization power of adversarial training ( AT ) on different languages , based on the over all POS tagging performance and learning curves .
In this section , we conduct further analyses on the robustness of AT from NLP specific aspects such as word statistics , sequence modeling , downstream tasks , and word representation learning .
We find that AT can boost tagging accuracy on rare words and neighbors of unseen words ( 5.1 ) .
Furthermore , this robustness against rare / unseen words leads to better sentence - level accuracy and downstream dependency parsing ( 5.2 ) .
We illustrate these findings using two major languages , English ( WSJ ) and French ( UD ) , which have substantially large training and testing data to discuss vocabulary statistics and sentence - level performance .
Finally , we study the effects of AT on word representation learning ( 5.3 ) , and the applicability of AT to different sequential tasks ( 5.4 ) .
Word - level Analysis
Poor tagging accuracy on rare / unseen words is one of the bottlenecks in current POS taggers .
Aiming to reveal the effects of AT on rare / unseen words , we analyze tagging performance at the word level , considering vocabulary statistics .
Word frequency .
To define rare / unseen words , we consider each word 's frequency of occurrence in the training set .
We categorize all words in the test set based on this frequency and study the test tagging accuracy for each group ( see ) .
3
In both languages , the AT model achieves large improvements over the baseline on rare words ( e.g. , frequency 1 - 10 in training ) , as opposed to more frequent words .
This result again corroborates the data augmentation power of AT under small training examples .
On the other hand , we did not observe meaningful improvements on unseen words ( frequency 0 in training ) .
A possible explanation is that AT can facilitate the learning of words with at least a few occurrences in training ( rare words ) , but is not particularly effective in inferring the POS tags of words for which no training examples are given ( unseen words ) .
Neighboring words .
One important characteristic of natural language tasks is the sequential nature of inputs ( i.e. , sequence of words ) , where each word influences the function of its neighboring words .
Since our model uses BiLSTM - CRF for that reason , we also study the tagging performance on the neighbors of rare / unseen words , and analyze the effects of AT with the sequence model in mind .
In , we cluster all words in the test set based on their frequency in training again , and consider the tagging accuracy on the neighbors ( left and right ) of these words in the test text .
We observe that AT tends to achieve large improvements over the baseline on the neighbors of unseen words ( training frequency 0 ) , while the improvements on the neighbors of more frequent words remain moderate .
Our AT model thus exhibits strong stability to uncertain neighbors , as compared to the baseline .
We suspect that because we generate adversarial examples against entire input sentences , training with adversarial examples makes the model more robust not only to perturbations in each word but also to perturbations in its neighbor - ing words , leading to greater stability to uncertain neighbors .
Sentence - level & Downstream
Analysis
In the word - level analysis , we showed that AT can boost tagging accuracy on rare words and the neighbors of unseen words , enhancing over all robustness on rare / unseen words .
In this section , we discuss the benefit of our improved POS tagger in a major downstream task , dependency parsing .
Most of the recent state - of - the - art dependency parsers take predicted POS tags as input ( e.g. ; ; ) . empirically show that their dependency parser gains significant improvements by using POS tags predicted by a Bi - LSTM POS tagger , while POS tags predicted by the UDPipe tagger do not contribute to parsing performance as much .
This observation illustrates that POS tagging performance has a great influence on dependency parsing , motivating the hypothesis that the POS tagging improvements gained from our adversarial training help dependency parsing .
To test the hypothesis , we consider three settings in dependency parsing of English and French : using POS tags predicted by the baseline model , using POS tags predicted by the AT model , and using gold POS tags .
For English ( PTB - WSJ ) , we first convert the treebank into Stanford Dependencies ( SD ) using Stanford CoreNLP ( ver 3.8.0 ) , and then apply two wellknown dependency parsers : Stanford Parser ( ver 3.5.0 ) and Parsey Mc - Parseface ( SyntaxNet ) .
For French ( UD ) , we use Parsey Universal from Syn - taxNet .
The three parsers are all publicly available and pre-trained on corresponding treebanks .
shows the results of the experiments .
We can observe improvements in both languages by using the POS tags predicted by our AT POS tagger .
As points out , when predicted POS tags are used for downstream dependency parsing , a single bad mistake in a sentence can greatly damage the usefulness of the POS tagger .
The robustness of our AT POS tagger against rare / unseen words helps to mitigate such an issue .
This advantage can also be observed from the AT POS tagger 's notably higher sentence - level accuracy than the baseline ( see left ) .
Nonetheless , gold POS tags still yield better parsing results as compared to the baseline / AT POS taggers , supporting the claim that POS tagging needs further improvement for downstream tasks .
Effects on Representation Learning
Next , we perform an analysis on representation learning of words ( word embeddings ) for the English ( PTB - WSJ ) and French ( UD ) experiments .
We hypothesize that adversarial training ( AT ) helps to learn better word embeddings so that the POS tag prediction of a word can not be influenced by a small perturbation in the input embedding .
To verify this hypothesis , we cluster all words in the test set based on their correct POS tags 4 and evaluate the tightness of the word vector distribution within each cluster .
We compare this clustering quality among the three settings :
1 ) beginning ( initialized with GloVe or Polyglot ) , 2 ) after baseline training ( 50 epochs ) , and 3 ) after adversarial training ( 50 epochs ) , to study the effects of AT on word representation learning .
For evaluating the tightness of word vector distribution , we employ the cosine similarity metric , which is widely used as a measure of the closeness between two word vectors ( e.g. , ; ) .
To measure the tightness of each cluster , we compute the cosine similarity for every pair of words within , and then take the average .
We also report the average tightness across all the clusters .
The evaluation results are summarized in Table 6 .
We report the tightness scores for the four major clusters : noun , verb , adjective , and adverb ( from left to right ) .
As can be seen from the table , for both languages , adversarial training ( AT ) results in cleaner word embedding distributions than the baseline , with a higher cosine similarity within each POS cluster , and with a clear advantage in the average tightness across all the clusters .
In other words , the learned word vectors show stronger correlations with their POS tags .
This result confirms that training with adversarial examples can help to learn cleaner word embeddings so that the meaning / grammatical function of a word can not be altered by a small perturbation in its embedding .
This analysis provides a means to interpret the robustness to input perturbations , from the perspective of NLP .
Relation with perturbation size .
We also study how the size of added perturbations influences word representation learning in adversarial training .
Recall that we set the norm of a perturbation to be ? ?
D , where Dis the dimension of the concatenated input embeddings ( see 3.2 ) .
For instance , ? = 0 would produce no noise ; ? = 1 would generate a perturbation of a norm equivalent to the original word embeddings .
We hypothesize that AT facilitates word representation learning when ?
is small enough to preserve the semantics of input words , but can hinder the learning when ?
is too large .
To test the hypothesis , we repeat the clustering evaluation for word embeddings trained with varied perturbation scale ?: Model F1 93.81 94.32 94.66 95.15 95.56 95.77 96.37
Ours - Baseline ( BiLSTM - CRF ) 95.18 Ours - Adversarial 95.25 ) .
We observe that the quality of learned word embedding distribution keeps improving as ?
goes up from 0 to 0.1 , but starts to drop around ? = 0.5 .
We also find that this optimal ?
in word embedding learning ( i.e. , 0.1 ) is larger than the ?
which yielded the best tagging performance on development sets ( i.e. , 0.01 or 0.05 ) .
A possible explanation is that while word embeddings can adapt to relatively large ? ( e.g. , 0.1 ) during training , as adversarial perturbations are generated at the embedding level , such ?
could change the semantics of the input from the current tagging model 's perspective and hinder the training of tagging .
Other Sequence Labeling Tasks
Finally , to further confirm the applicability of AT , we experiment with our BiLSTM - CRF AT model in different sequence labeling tasks : chunking and named entity recognition ( NER ) .
Chunking can be performed as a sequence labeling task that assigns a chunking tag ( B - NP , I - VP , etc. ) to each word .
We conduct experiments on the CoNLL 2000 shared task with the standard data split : PTB - WSJ Sections 15 - 18 for training and 20 for testing .
We use Section 19 as the development set and employ the IOBES tagging scheme , following .
NER aims to assign an entity type to each word , such as person , location , organization , and misc .
We conduct experiments on the CoNLL - 2003 ( English ) shared task ( Tjong , adopting the IOBES tagging scheme as in .
The results are summarized in and 9 .
AT enhanced F 1 score from the baseline BiLSTM - CRF model 's 95.18 to 95.25 for chunking , and from 91.22 to 91.56 for NER , also significantly outperforming .
These improvements made by AT are bigger than that for English POS tagging , most likely due to the larger room for improvement in chunking and NER .
The improvements are again statistically significant , with p-value < 0.05 on the t-test .
The experimental results suggest that the proposed adversarial training scheme is generally effective across different sequence labeling tasks .
Our BiLSTM-CRF
AT model did not reach the performance by 's state - of - theart system that incorporates pretrained language models .
It would be interesting future work to combine the strengths of these joint models ( e.g. , syntactic and semantic aids ) and adversarial training ( e.g. , robustness ) .
Conclusion
We proposed and carefully analyzed a POS tagging model that exploits adversarial training ( AT ) .
In our multilingual experiments , we find that AT achieves substantial improvements on all the languages tested , especially on low resource ones .
AT also enhances the robustness to rare / unseen words and sentence - level accuracy , alleviating the major issues of current POS taggers , and contributing to the downstream task , dependency parsing .
Furthermore , our analyses on different languages , word / neighbor statistics and word representation learning reveal the effects of AT from the perspective of NLP .
The proposed AT model is applicable to general sequence labeling tasks .
This work therefore provides a strong basis and motivation for utilizing AT in natural language tasks .
