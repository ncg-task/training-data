title
Abstractive Text Summarization by Incorporating Reader Comments
abstract
In neural abstractive summarization field , conventional sequence - to - sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect .
To tackle this problem , we propose the task of reader - aware abstractive summary generation , which utilizes the reader comments to help the model produce better summary about the main aspect .
Unlike traditional abstractive summarization task , reader - aware summarization confronts two main challenges :
( 1 ) Comments are informal and noisy ;
( 2 ) jointly modeling the news document and the reader comments is challenging .
To tackle the above challenges , we design an adversarial learning model named reader - aware summary generator ( RASG ) , which consists of four components :
( 1 ) a sequence - to - sequence based summary generator ; ( 2 ) a reader attention module capturing the reader focused aspects ;
( 3 ) a supervisor modeling the semantic gap between the generated summary and reader focused aspects ; ( 4 ) a goal tracker producing the goal for each generation step .
The supervisor and the goal tacker are used to guide the training of our framework in an adversarial manner .
Extensive experiments are conducted on our large - scale real - world text summarization dataset , and the results show that RASG achieves the stateof - the - art performance in terms of both automatic metrics and human evaluations .
The experimental results also demonstrate the effectiveness of each module in our framework .
We release our large - scale dataset for further research 1 .
Introduction
Abstractive summarization can be regarded as a sequence mapping task that the source text is mapped to the target summary , and has drawn much attention since the deep neural networks are widely applied in natural language processing field .
Recently , sequence - to - sequence ( seq2seq ) framework has been proved effective for the task of abstractive summarization and other text generation tasks .
In this paper , we use " aspect " to denote the topic described in a specific paragraph or a sentence of a news document , and use " main aspect " to denote the central :
Examples of the text summarization .
The text in red denotes the focused aspect by the good summary , while the text in blue is described by the bad summary .
The text with underline is the focused aspect by reader comments .
On August 28 , according to a person familiar with the matter , Toyota Motor Corporation will invest 500 million U.S. dollars into the Uber , a taxi service company , with a valuation of up to 72 billion U.S. dollars .
The investment will focus on driverless car technology .
However , its development path is not smooth .
In March of this year , a Uber driverless car hit a woman and caused her death .
In last year , Softbank also invested into Uber with a valuation of $ 48 billion .
comments Toyota 's investment in Uber is a wise choice .
$ 500 million investment is really a lot of money !
good summary Toyota invests $ 500 million into Uber with a valuation of $ 72 billion bad summary An Uber driverless car hits a passerby to death topic which the author tends to convey to the readers .
Although a document may describe an event in many different aspects , the summary of this document should always focus on the main aspect .
As shown in , the good summary describes the main aspect and the bad summary describes another trivial aspect that is not the main point of the document .
To focus on the main aspect , some summarization methods first select several sentences about the main aspect and then generate the summary .
However , it is very challenging to discover which is the main aspect of the news document .
Nowadays , a great number of news comments are generated by readers to express their opinions about the event .
Some comments may mention the main aspect of the document for several times .
Take the casein as an example , the focused aspect of the reader is " investment of Toyota " which is also the main aspect of this document .
To be specific , we define " reader focused aspect " to denote the focused aspect by a reader through the comments .
Intuitively , these reader comments may help the summary generator capture the main aspect of document , thereby improving the quality of the generated summary .
Therefore , in this paper , we investigate a new problem setting of the task of abstractive text summarization .
We name such paradigm of extension as reader - aware abstractive text summarization .
The effect of comments or social contexts in document summarization have been explored by several previous works .
Unlike these approaches that directly extract sentences from the original document , we aim to generate a natural - sounding summary from scratch instead of extracting words from the document .
Generally , existing text summarization approaches confront two challenges when addressing reader - aware summarization task .
The first challenge is that reader comments are very noisy and informative .
Not all the information provided by the comments is useful when modeling the reader focused aspects .
Therefore , it is crucial to make the model own the ability of capturing main aspect and filtering noisy information when incorporating reader comments .
The second challenge is how to generate summaries by jointly modeling the main aspect of document and the reader focused aspect revealed by comments .
Meanwhile , the model should not be sensitive to the diverse unimportant aspects introduced by some reader comments .
Thus , simply absorbing all the reader aspect information to directly guide the model to generate summary is not feasible , as it will make the generator lose the ability of modeling the main aspect .
In this paper , we propose a summarization framework named reader - aware summary generator ( RASG ) that incorporates reader comments to improve the summarization performance .
Specifically , a seq2seq architecture with attention mechanism is employed as the basic summary generator .
We first calculate alignment between the reader comments words and document words , and this alignment information is regarded as reader attention representing the " reader focused aspect " .
Then , we treat the decoder attention weights as the focused aspect of the generated summary , a.k.a. , " decoder focused aspect " .
After each decoding step , a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect .
Given this distance , a goal tracker provides the goal to the decoder to induce it to reduce this distance .
The training of our framework RASG is conducted in an adversarial way .
To evaluate the performance of our model , we collect a large amount of document - summary pairs associated with several reader comments from social media website .
Extensive experiments conducted on this dataset show that RASG significantly outperforms the state - of - the - art baselines in terms of ROUGE metrics and human evaluations .
To sum up , our contributions can be summarized as follows :
We propose a reader - aware abstractive text summarization task .
To solve this task , we propose an end - to - end learning framework to conduct the reader attention modeling and reader - aware summary generation .
We design a supervisor as well as a goal tracker to guide the generator to focus on the main aspect of the document .
To reduce the noisy information introduced by the reader comments , we propose a denoising module to identify which comments are helpful for summary generation auto-matically .
We release a large scale abstractive text summarization dataset associated with reader comments .
Experimental results on this dataset demonstrate the effectiveness of our proposed framework .
Related Work
Text summarization can be classified into extractive and abstractive methods .
Extractive methods read the article and get the representations of the sentences and article to select sentences .
However , summaries generated by extractive methods always suffer from redundancy problem .
Recently , with the emergence of neural network models for text generation , avast majority of the literature on summarization is dedicated to abstractive summarization ) .
On the text summarization benchmark dataset CNN / DailyMail , the state - of the - art abstractive methods outperform the best extractive method in terms of ROUGE score .
Most methods for abstractive text summarization are based on the sequence - tosequence model , which encodes the source texts into the semantic representation with an encoder , and generates the summaries from the representation with a decoder .
To tackle the out - of - vocabulary problem , some researchers employ the copy mechanism to copy some words from the input document to summary .
To capture the main aspect of document , propose to select salient sentences and then rewrite these sentences to a concise summary .
This approach achieves the state - of - theart of text summarization on CNN / DailyMail benchmark dataset .
Unlike document summarization that needs to encode along text , social media summarization usually reads short and noisy text and has become a popular task these days .
After propose a short text summarization dataset on social media and many researchers follow this task .
propose a seq2seq based model which uses an CNN to refine the representation of source context .
use convolutional seq2seq model to summarize text and use the policy gradient algorithm to directly optimize the ROUGE score .
However , these summarization models do not utilize the reader 's comments in generating summaries .
To consider the reader 's comments into text summarization , the reader - aware summarization is proposed and it mainly takes the form of extractive approaches .
Graph - based method has been used for comment oriented summarization task such as , where they identify three relations ( topic , quotation , and mention ) by which comments can be linked to one another .
Recently , publish a small extractive sentence - comment dataset which can not be used to train neural models due to its small size .
propose an unsupervised compressive multi-document summarization model using sparse coding method .
Following previous work , there are some models ) using variational auto - encoder to model the latent semantic of original article and reader comments .
Different from our abstractive summarization task , these related works are all based on extractive or compressive approaches .
Problem Formulation
Before presenting our approach for the reader - aware summarization , we first introduce our notations and key concepts .
To begin with , for a document
denotes the i - th word in document X d , and x c i , j denotes the j - th word in i -th comment sentence c i .
Given the document X d , the summary generator reads the comments X c , then generates a summary ? = {? 1 ,? 2 , . . . , ?
T ? }.
Finally , we use the difference between generated summary ?
and ground truth summary Y as the training signal to optimize the model parameters .
The Proposed RASG Model Overview
In this section , we propose our reader - aware summary generator , abbreviated as RASG .
The overview of RASG is shown in which can be split into four main parts :
Summary generator is a seq2seq based architecture with attention and copy mechanisms .
Reader attention module learns a semantic alignment between each word in document and comments , thus captures the reader focused aspect .
Supervisor measures the semantic gap between decoder focused aspect and reader focused aspect .
There is also a discriminator which uses convolutional neural network to extract features and then distinguishes how similar is decoder focused aspect to reader focused aspect .
Goal tracker utilizes the semantic gap learned by supervisor and the features extracted learned by the discriminator to set a goal , which is further utilized as a more specific guidance for summary generator to produce better summary .
Summary generator
At the beginning , we use an embedding matrix e to map onehot representation of each word in the document X d and comments X c to a high - dimensional vector space .
We denote e ( x ) as the embedding representation of word x .
From these embedding representations , we employ a bi-directional recurrent neural network ( Bi - RNN ) to model the temporal interactions between words :
where h d t denotes the hidden state of t-th step in Bi - RNN for document X d .
We denote the final hidden state h d T d of Bi - RNN d as the vector representation of the document X d . Following ) , we choose the long short - term memory ( LSTM ) as the Bi - RNN cell .
Then we apply a linear transform layer on the input document vector representation h d T d and use the output of this layer as the initial state of decoder LSTM , shown in Equation 2 .
In order to reduce the burden of compressing document information into initial state s 0 , we use the attention mechanism to summarize the input document into context vector f t ?1 dynamically and we will show the detail of these in the following sections .
We then concatenate the context vector f t?1 with the embedding of previous step output e ( y t?1 ) and feed this into decoder LSTM , shown in Equation 3 .
We use the notion [ ; ] as the concatenation of two vectors .
At t- th decoding step , we use the decoder state s t?1 to attend to each the document states h d and resulting in the attention distribution ? t ?
R T d , shown in Equation 5 .
Then we use the attention distribution ?
t to weighted sum the document states as the context vector f t?1 .
Finally , an output projection layer is applied to get the final generating distribution P v over vocabulary , as shown in Equation 7 .
We concatenate goal vector gt , gap content d t , and the output of decoder LSTM st as the input of the output projection layer .
The goal vector gt represents the goal of current generation step , the gap content d t denotes the semantic gap between generated summary and reader focused document and we will show the details of these variables in the following sections .
In order to handle the out - of - vocabulary ( OOV ) problem , we equip the pointer network with our decoder , which makes our decoder capable to copy words from the source text .
The design of the pointer network is the same as the model used in , thus we omit this procedure in our paper due to the limited space .
We use the negative log-likelihood as the loss function :
Denoising module
Due to the fact that reader comments are a kind of informal text , they may consist of many noisy information , and not all the comments are helpful for generating better summaries .
Consequently , we employ a denoising module to distinguish which comments are helpful .
First , we employ an Bi - RNN c to model the comment word embeddings :
where h c i ,t denotes the hidden state of t- th word in i - th comment c i .
Next , we use average - pooling operation over these hidden states to produce a vector representation a i of i -th comment , shown in Equation 10 .
Finally , we apply a linear transform with sigmoid function to predict whether the comment is useful , and the sigmoid output ?
i ? ( 0 , 1 ) also can be seen as a salience score of i - th comment given the document representation
To train the denoising module , we use the cross entropy loss to supervise this procedure .
where ?
i ?
{ 0 , 1 } is the ground truth salience score of comments .
?
i = 1 denotes the i - th comment c i is helpful for generating summary and vice - versa .
Reader attention modeling
To model the reader focused aspect , we first calculate the word alignment of reader comments towards the document .
We use the embeddings of words in document and comments to calculate the semantic alignment score .
Precisely , ?
i , j,k is the alignment socre between the i - th document word x d i and the k - th word in the j - th comment x c j , k , as shown in :
In Equation 14 , we use a max-operation over the alignment ?
i , j , to signify whether the i - th word of document is focused by the j - th comment .
We regard the alignment score ?
i , j as the reader attention weight for the j - th reader comment to the i - th document word .
In order to reduce the interference caused by the noisy comments , we employ the comment salience score ?
j obtained from the denoising module to weighted combine the j - th reader attention ?
i , j , as shown in Equation 15 .
It means that noisy comments will contribute less in the procedure of reader attention modeling .
Finally , we get the reader attention i ?
R for i -th document word after a softmax function as shown in Equation 16 .
Supervisor
To model the semantic gap between the generated summary and the reader focused aspects , we design a supervisor module .
First , for the decoder , we need to know which aspect in document has been focused by our summary generator in the past decoding steps .
We sum up the latest k attention distributions {? t , ? t?1 , . . . , ? t?k+1 } and result in ? t ?
R T d as the focus distribution of generated summary over T d document words , shown in Equation 17 .
Then we use ?
t to weighted sum the document hidden states h d and result in mt :
where mt represents the focused aspect by the latest k decoding steps , a.k.a. , decoder focused aspect .
Next , we use the reader attention to weighted sum the document hidden states h d :
where u represents the reader focused aspect .
For encouraging the decoder focused aspect become similar to the reader focused aspect , we employ an CNN based discriminator to signify the difference between the decoder focused aspect mt and the reader focused aspect u .
Then we can use this difference to guide the decoder focus on the reader focused aspect .
Typically , the discriminator is a binary classifier which can be decomposed into a convolutional feature extractor F shown in Equation 20 and a sigmoid classification layer shown in Equation 21 and 22 .
where ?
denotes the convolutional operation , trainable parameter
W c denotes the convolutional kernel , and ? mt and ?
u are both the classification probabilities .
Note that a token generated at time twill influence not only the gradient received at that time but also the gradient at subsequent time steps .
Intuitively , the decoding attention ?
t of latter decoding step is more similar to the attention of final summary than the earlier steps .
Thus we propose to define the cumulative loss with a discount factor ? ?
( 0 , 1 ] as the loss functions .
Note that the training objective for discriminator can be interpreted as maximizing the log-likelihood for classification , whether the input x in Equation 20 comes from reader focused aspect or from decoder focused aspect .
In order to model the gap between reader focused aspect and decoder focused aspect , we subtract the reader attention ?
R T d by ? t ?
R T d resulting in attention difference ? t ?
R T d , shown in Equation 25 .
Then we use the attention difference ?
t to sum up the document hidden states h d :
( 25 ) where d t denotes the semantic of unfocused document aspects by summary generator , a.k.a. , gap content .
To encourage the summary generator focus on the unfocused document aspects , we feed the gap content d t to the generator , as shown in Equation 7 .
Goal tracker
Since the discriminator only provides a scalar guiding signal ?
mt at each decoding step , it becomes relatively less informative when the sentence length T ?
goes larger .
Inspired by LeakGAN , the proposed RASG framework allows discriminator to provide additional information , denoted as goal vector gt .
In view of there is certain relationship between the goal of current decoding step and previous steps , we need to model the temporal interactions between the goal of each step .
More specifically , we introduce a goal tracker module , an LSTM that takes the extracted feature vector F ( m t ) and gap content d t as it s input at each step t and outputs a goal vector gt :
In order to achieve higher consistency of reader focused aspect , we feed the goal vector gt into the generator to guide the generation of the next word , as shown in Equation 7 .
Model training
As our model is trained in an adversarial manner , we resplit the parameters in our model into two parts : ( 1 ) generation module including the parameters of summary generator , reader attention module and goal tracker ;
( 2 ) discriminator module including the parameters of CNN classifier .
As for training generation module , we sum up the loss function of denoising module L d , cross entropy between ground truth L g and the result of discriminator L g c , as shown in Equation 27 .
We use the L to optimize the parameters of generation module .
Next , we train the discriminator module to maximize the probability of assigning the correct label to both generated aspect mt and reader focused aspect u .
More specifically , we optimize the parameters of discriminator module according to the loss function L dc calculated in Equation 23 .
Experimental Setup
Research questions
We list four research questions that guide the experiments :
RQ1 : Does RASG outperform other baselines ?
RQ2 : What is the effect of each module in RASG ?
RQ3 : Does RASG capture useful information from noisy comments ?
RQ4 : Can goal tracker give a helpful guidance to decoder ?
Dataset
We collect the document - summary - comments pair data from Weibo which is the largest social network website in China , and users can read a document and post a comment about the document on this website .
Each sample of data contains a document , a summary and several reader comments .
Most comments are about the readers ' opinion of their focused aspect in the document .
In order to train the denoising module , we should give aground truth label ?
i for i - th comment .
When there is at least one common word in summary and comment , we regard such comment is helpful for generating summary .
Accordingly , we give the ? i = 1 to i - th comment when it contains at least one common word and give ?
i = 0 when it does not .
In total , our training dataset contains 863826 training samples .
The average length of document is 67.08 words , average length of comment is 16.61 words and average length of summary is 16.56 words .
The average comments number of a document is 9.11 .
Evaluation metrics
For evaluation metrics , we adopt ROUGE score which is widely applied for summarization evaluation .
The ROUGE metrics compare generated summary with the reference summary by computing overlapping lexical units , including ROUGE - 1 ( unigram ) , ROUGE - 2 ( bi-gram ) and ROUGE - L ( longest common subsequence ) .
Comparison methods
In order to prove the effectiveness of each module in RASG , we conduct some ablation models introduced in .
To evaluate the performance of our proposed dataset and model , we compare it with the following baselines :
( 1 ) S2S : Sequence - to - sequence framework has been proposed for language generation task .
( 2 ) S2SR : We simply add the reader attention on attention distribution ? t , in each decoding step .
( 3 ) CGU : propose to use the convolutional gated unit to refine the source representation , which achieves the state - of - the - art performance on social media text summarization dataset .
( 4 ) LEAD1 : LEAD1 is a commonly used baseline , which selects the first sentence of document as the summary .
( 5 ) TextRank : propose to build a graph , then add each sentence as a vertex and use link to represent semantic similarity .
Sentences are sorted based on final scores and a greedy algorithm is employed to select summary sentences .
Implementation details
We implement our experiments in TensorFlow ) on an NVIDIA P40 GPU .
The word embedding dimension is set to 256 and the number of hidden units is 512 .
We set the k = 5 in the Equation 17 and ? = 0.5 in Equation 23 and 24 .
We use Adagrad optimizer as our optimizing algorithm .
We employ beam search with beam size 5 to generate more fluency summary sentence .
Experimental Results
Overall performance
For research question RQ1 , we examine the performance of our model in terms of ROUGE .
lists performances of all comparisons in terms of ROUGE score .
We see that RASG achieves a 11.0 % , 9.1 % and 6.6 % increment over the state - of - the - art method CGU in terms of ROUGE - 1 , ROUGE - 2 and ROUGE - L respectively .
It is worth noticing that the baseline model S2SR achieves better performance than S2S which demonstrates the effectiveness of incorporating reader focused aspect in summary generation .
However when compared with RASG , S2SR achieves lower performance in terms of all ROUGE score .
Thus , simply adding the reader focused aspect into generation procedure is not a good reader - aware summarization method .
Ablation study
Next , we turn to research question RQ2 .
We conduct ablation tests on the usage of denoising module , supervisor as well as the goal tracker and the ROUGE score result is shown in .
The discriminator provides the scalar training signal L g c for generator training and the feature vector F ( m t ) for goal tracker .
Consequently , there is an increment of 17.51 % from RASG w / o GTD to RASG w / o GT in terms of ROUGE - L , which demonstrates the effectiveness of discriminator .
As for the effectiveness of goal tracker , compared with RASG and RASG w / o GT , RASG w/ o GTD offers a decrease of 45. 23 % and 17.88 % in terms of ROUGE - 1 , respectively .
This demonstrates that the goal tracker with the feature from discriminator plays an important role in producing better summary .
However , using the goal tracker without the feature extracted by the discriminator does not help improve the performance of the summary generator , shown by the performance of RASG w /o GTD .
Finally , RASG w/o DM offers a decrease of 10 . 22 % compared with RASG in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .
Denoising ability
Next , we turn to research question RQ3 .
Due to the fact that the denoising module is learned in a supervised way , there is aground truth label associated with each comment .
Thus when the predict salience score ?
i > 0.5 we classify it as a helpful comment and vice - versa .
As the denoising module can be regarded as a binary classifier to classify each comment to ?
i = 1 or? i = 0 , we calculate the classification recall score of comments to measure the performance of this module .
The recall curve is shown in .
As the training progresses , the recall score is on a steady upward curve which proves the improved performance of denoising module .
To conclude , the denoising module can give a meaningful salience score for the subsequent process .
Analysis of goal tracker
In this section , we turn to research question RQ4 .
The main purpose of employing goal tracker is to help the summary generator utilize the reader focused aspect .
Intuitively , we want to know whether the summary generator follows the goal set by the goal tracker .
Therefore , we calculate the cosine distance between decoder attention ?
T ? ?
R T d ( in ) and reader attention ?
R T d ( in ) .
In , we compare the cosine distance between the ablation model RASG w / o GTD and RASG .
RASG observes a decrease of cosine distance and conversely , the RASG w / o GTD observes an increment of cosine distance .
The fact that RASG can narrow the cosine distance proves that goal tracker and discriminator can lead the generator to follow the reader focused aspect .
Human evaluation
We ask three highly - educated Ph.D. students to rate 100 generated summaries of different models according to consistency and fluency .
These annotators are all native speakers .
The rating score ranges from 1 to 3 and 3 is the best .
We take the average score of all summaries as the final score of each model , as shown in .
It can be seen that RASG outperforms other baseline models in both sentence fluency and consistency by a large margin .
We calculate the kappa statistics in terms of fluency and consistency , and the score is 0.33 and 0.29 respectively .
To prove the significance of the above results , we also do the paired student t- test between our model and CGU model ( row with shaded background ) , the p-value are 0.0017 and 0.0012 for fluency and consistency respectively .
shows a document and its corresponding summaries generated by different methods .
We can observe that : Cosine distance between current content and reader content .
Case analysis
Discussion
We also ask three highly - educated annotators to rate 100 generated summaries according to two aspects : consistency and fluency .
The rating score ranges from 1 to 3 , and 3 is the best .
We finally take the average across summaries and annotators , as shown in .
Significant differences are with respect to CGU ( row with shaded background ) .
In Table 5 , we can see that RASG significantly outperforms other baseline models in both sentence fluency and consistency with the original document and comments of readers .
To prove the significance of the above results , we also do the paired student t- test between our model and baseline methods , the p-value are 0.0017 and 0.0012 for fluency and consistency respectively .
shows an example and its corresponding generated summaries by different methods .
We observe that S2S only generates fluent summary , but are contradictory to the focus of reader and ground truth summary .
However , RASG overcomes this shortcoming by using goal vector and gap content given by supervisor and goal tracker at training stage , and produce the summary not only fluent but also consistent with the focus of readers .
Conclusion
In this paper , we propose a new framework named readeraware summary generator ( RASG ) which aims to generate summaries for document from social media incorporating the reader comments .
In order to capture the reader focused aspect , we design a reader attention component with a denoising module to capture the alignment between comments and document .
We employ a supervisor to measure the semantic gap between generated summary and reader focused aspect .
A goal tracker uses the information of semantic gap and the feature extracted by the discriminator to produce a goal vector to guide the summary generator .
In our experiments , we have demonstrated the effectiveness of RASG and have found significant improvements over state - of - theart baselines in terms of ROUGE and human evaluations .
Moreover , we have verified the effectiveness of each module in RASG for improving the summarization performance .
