30	93	100	combine
30	105	156	seq2seq and template based summarization approaches
31	3	7	call
31	12	32	summarization system
31	33	41	Re 3 Sum
31	50	61	consists of
31	62	75	three modules
31	78	86	Retrieve
31	89	95	Rerank
31	100	107	Rewrite
32	3	10	utilize
32	13	64	widely - used Information Retrieval ( IR ) platform
32	65	76	to find out
32	77	101	candidate soft templates
32	102	106	from
32	111	126	training corpus
33	10	16	extend
33	21	34	seq2seq model
33	35	51	to jointly learn
33	52	92	template saliency measurement ( Rerank )
33	97	133	final summary generation ( Rewrite )
34	17	57	Recurrent Neural Network ( RNN ) encoder
34	61	71	applied to
34	72	79	convert
34	127	131	into
34	132	145	hidden states
34	84	98	input sentence
34	103	126	each candidate template
35	0	2	In
35	3	9	Rerank
35	15	22	measure
35	27	42	informativeness
35	43	45	of
35	48	66	candidate template
35	67	79	according to
35	84	106	hidden state relevance
35	107	109	to
35	114	128	input sentence
36	23	27	with
36	32	65	highest predicted informativeness
36	66	80	is regarded as
36	85	105	actual soft template
37	3	10	Rewrite
37	28	37	generated
37	17	24	summary
37	38	50	according to
37	55	68	hidden states
37	69	76	of both
37	81	102	sentence and template
151	0	7	OpenNMT
152	8	17	implement
152	22	56	standard attentional seq2seq model
156	0	5	FTSum
156	6	13	encoded
156	18	23	facts
156	24	38	extracted from
156	43	58	source sentence
156	59	69	to improve
156	115	134	generated summaries
156	70	74	both
156	79	91	faithfulness
156	96	111	informativeness
157	108	116	PIPELINE
159	13	19	trains
159	24	37	Rerank module
159	42	56	Rewrite module
43	33	71	http://www4.comp.polyu.edu.hk/cszqcao/
139	3	6	use
139	11	36	popular seq2seq framework
139	37	47	Open - NMT
140	36	42	retain
140	47	63	default settings
140	64	66	of
140	67	75	Open NMT
140	76	84	to build
140	89	109	network architecture
141	19	29	dimensions
141	33	56	word embeddings and RNN
141	57	65	are both
141	66	69	500
141	80	110	encoder and decoder structures
141	111	114	are
141	115	182	two - layer bidirectional Long Short Term Memory Networks ( LSTMs )
144	0	2	On
144	3	15	our computer
144	18	21	GPU
144	24	32	GTX 1080
144	35	41	Memory
144	44	47	16G
144	50	53	CPU
144	56	65	i7-7700 K
144	74	89	training spends
144	90	102	about 2 days
145	0	11	During test
145	21	32	beam search
145	33	40	of size
145	41	42	5
145	43	54	to generate
145	55	64	summaries
146	3	6	add
146	11	19	argument
146	22	33	replace unk
146	36	46	to replace
146	51	74	generated unknown words
146	75	79	with
146	84	95	source word
146	96	106	that holds
146	111	135	highest attention weight
147	74	83	introduce
147	87	121	additional length penalty argument
147	124	131	alpha 1
147	134	146	to encourage
147	147	164	longer generation
2	52	72	Neural Summarization
12	69	103	abstractive sentence summarization
163	8	15	examine
163	20	31	performance
163	32	53	of directly regarding
163	54	68	soft templates
163	69	71	as
163	72	88	output summaries
164	3	12	introduce
164	13	51	five types of different soft templates
176	18	32	performance of
176	33	39	Random
176	40	42	is
176	43	51	terrible
177	0	6	Rerank
177	7	26	largely outperforms
177	27	32	First
180	12	19	Optimal
180	20	35	greatly exceeds
180	36	77	all the state - of - the - art approaches
179	11	20	comparing
179	21	34	Max and First
179	40	52	observe that
179	57	75	improving capacity
179	76	78	of
179	83	98	Retrieve module
179	99	101	is
179	102	106	high
183	8	15	measure
183	20	38	linguistic quality
183	39	41	of
183	42	61	generated summaries
184	62	76	performance of
184	77	85	Re 3 Sum
184	89	107	almost the same as
184	116	130	soft templates
204	21	32	investigate
204	37	51	soft templates
204	52	58	affect
204	59	68	our model
206	58	66	provided
206	24	53	more high - quality templates
206	97	105	achieved
206	73	92	higher ROUGE scores
210	10	26	manually inspect
210	31	40	summaries
210	41	53	generated by
210	54	71	different methods
211	3	7	find
211	12	31	outputs of Re 3 Sum
211	44	74	longer and more flu - ent than
211	79	97	outputs of OpenNMT
222	17	21	with
222	22	47	different templates given
222	50	59	our model
222	63	81	likely to generate
222	82	102	dissimilar summaries
