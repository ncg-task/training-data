title
Structure - Infused Copy Mechanisms for Abstractive Summarization
abstract
Seq2seq learning has produced promising results on summarization .
However , in many cases , system summaries still struggle to keep the meaning of the original intact .
They may miss out important words or relations that play critical roles in the syntactic structure of source sentences .
In this paper , we present structure - infused copy mechanisms to facilitate copying important words and relations from the source sentence to summary sentence .
The approach naturally combines source dependency structure with the copy mechanism of an abstractive sentence summarizer .
Experimental results demonstrate the effectiveness of incorporating source - side syntactic information in the system , and our proposed approach compares favorably to state - of - the - art methods .
Introduction
: Example source sentences , reference and system summaries produced by a neural attentive seq - to - seq model .
System summaries fail to preserve summary - worthy content of the source ( e.g. , main verbs ) despite their syntactic importance .
The sequence - to - sequence learning paradigm has achieved remarkable success on abstractive summarization .
While the results are impressive , individual system summaries can appear unreliable and fail to preserve the meaning of the source texts .
presents two examples .
In these cases , the syntactic structure of source sentences is relatively rare but perfectly normal .
The first sentence contains two appositional phrases ( " suspect of murdering Jorge Microsse , " " director of Maputo central prison " ) and the second sentence has a relative clause ( " who was too drunk to drive " ) , both located between the subject and the main verb .
The system , however , fails to identify the main verb in both cases ; it instead chooses to focus on the first few words of the source sentences .
We observe that rare syntactic constructions of the source can pose problems for neural summarization systems , possibly for two reasons .
First , similar to rare words , certain syntactic constructions do not occur frequently enough in the training data to allow the system to learn the patterns .
Second , neural summarization systems are not explicitly informed of the syntactic structure of the source sentences and they tend to bias towards sequential recency .
A father who was too drunk to drive had his 11 - year - old son take the wheel .:
An example dependency parse tree created for the source sentence in .
If important dependency edges such as " father ? had " can be preserved in the summary , the system summary is likely to preserve the meaning of the original .
In this paper we seek to address this problem by incorporating source syntactic structure in neural sentence summarization to help the system identify summary - worthy content and compose summaries that preserve the important meaning of the source texts .
We present structure - infused copy mechanisms to facilitate copying source words and relations to the summary based on their semantic and structural importance in the source sentences .
For example , if important parts of the source syntactic structure , such as a dependency edge from the main verb to the subject ( " father " ?
" had , " shown in ) , can be preserved in the summary , the " missing verb " issue in can be effectively alleviated .
Our model therefore learns to recognize important source words and source dependency relations and strives to preserve them in the summaries .
Our research contributions include the following :
we introduce novel neural architectures that encourage salient source words / relations to be preserved in summaries .
The framework naturally combines the dependency parse tree structure with the copy mechanism of an abstractive summarization system .
To the best of our knowledge , this is the first attempt at comparing various neural architectures for this purpose ; we study the effectiveness of several important components , including the vocabulary size , a coveragebased regularizer , and a beam search with reference mechanism ; through extensive experiments we demonstrate that incorporating syntactic information in neural sentence summarization is effective .
Our approach surpasses state - of - the - art published systems on the benchmark dataset .
1
Related Work
Prior to the deep learning era , sentence syntactic structure has been utilized to generate summaries with an " extract - and - compress " framework .
Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents , or a pipeline approach that combines generic sentence compression with a sentence pre-selection or post-selection process .
Although syntactic information is helpful for summarization , there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems .
Existing neural summarization systems handle syntactic structure only implicitly .
Most systems adopt a " cut-andstitch " scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model .
However , there lacks a mechanism to ensure structurally salient words and relations in source sentences are preserved in the summaries .
The resulting summary sentences can contain misleading information ( e.g. , " mozambican man arrested for murder " flips the meaning of the original ) or grammatical errors ( e.g. , verbless , as in " alaska father who was too drunk to drive " ) .
Natural language generation ( NLG ) - based abstractive summarization also makes extensive use of structural information , including syntactic / semantic parse trees , discourse structures , and domainspecific templates built using a text planner or an OpenIE system .
In particular , leverage OpenIE and dependency parsing to extract fact tuples from the source text and use those to improve the faithfulness of summaries .
Different from the above approaches , this paper seeks to directly incorporate source - side syntactic structure in the copy mechanism of an abstractive sentence summarization system .
It learns to recognize important source words and relations during training , while striving to preserve them in the summaries at test time to aid reproduction of factual details .
Our intent of incorporating source syntax in summarization is different from that of neural machine translation ( NMT ) , in part because NMT does not handle the information loss from source to target .
In contrast , a summarization system must selectively preserve source content to render concise and grammatical summaries .
We specifically focus on sentence summarization , where the goal is to reduce the first sentence of an article to a title - like summary .
We believe even for this reasonably simple task there remains issues unsolved .
Our Approach
We seek to transform a source sentence x to a summary sentence y that is concise , grammatical , and preserves the meaning of the source sentence .
A source word is replaced by it s
Glove embedding before it is fed to the system ; the vector is denoted by xi ( i ? [ S ] ; ' S ' for source ) .
Similarly , a summary word is denoted by y t ( t ? [ T ] ; ' T ' for target ) .
If a word does not appear in the input vocabulary , it is replaced by a special ' unk ' token .
We begin this section by describing the basic summarization framework , followed by our new copy mechanisms used to encourage source words and dependency relations to be preserved in the summary .
The Basic Framework
We build an encoder - decoder architecture for this work .
An encoder condenses the entire source text to a continuous vector ; it also learns a vector representation for each unit of the source text ( e.g. , words as units ) .
In this work we use a two - layer stacked bi-directional Long Short - Term Memory networks as the encoder , where the input to the second layer is the concatenation of hidden states from the forward and backward passes of the first layer .
We obtain the hidden states of the second layer ; they are denoted by he i .
The source text vector is constructed by averaging over all he i and passing the vector through a feedforward layer with tanh activation to convert from the encoder hidden states to an initial decoder hidden state ( h d 0 ) .
This process is illustrated in Eq.
( 2 ) .
A decoder unrolls the summary by predicting one word at a time .
During training , the decoder takes as input the embeddings of ground truth summary words , denoted by y t , while at test time y tare embeddings of system predicted summary words ( i.e. , teacher forcing ) .
We implement an LSTM decoder with the attention mechanism .
A context vector ct is used to encode the source words that the system attends to for generating the next summary word .
It is defined in Eqs ( 3 - 5 ) , where [ || ] denotes the concatenation of two vectors .
The ?
matrix measures the strength of interaction between the decoder hidden states {h d t } and encoder hidden states {h e i }.
To predict the next word , the context vector ct and h d tare concatenated and used as input to build a new vector h d t ( Eq. ) .
h d t is a surrogate for semantic meanings carried at time step t of the decoder .
It is subsequently used to compute a probability distribution over the output vocabulary ( Eq. ) .
The copy mechanism allows words in the source sequence to be selectively copied to the target sequence .
It expands the search space for summary words to include both the output vocabulary and the source text .
The copy mechanism can effectively reduce out - ofvocabulary tokens in the generated text , potentially aiding a number of applications such as MT ) and text summarization .
Our copy mechanism employs a ' switch ' to estimate the likelihood of generating a word from the vocabulary ( p gen ) vs. copying it from the source text ( 1 ? p gen ) .
The basic model is similar to that of the pointer - generator networks .
The switch is a feedforward layer with sigmoid activation ( Eq. ) .
At time step t , it s input is a concatenation of the decoder hidden state h d t , context vector ct , and the embedding of the previously generated wordy t?1 .
For predicting the next word , we combine the generation and copy probabilities , shown in Eq. ( 9 ) .
If a word w appears once or more in the input text , it s copy probability ( i:w i =w ? t , i ) is the sum of the attention weights over all its occurrences .
If w appears in both the vocabulary and source text , P ( w ) is a weighted sum of the two probabilities .
Structure - Infused Copy Mechanisms
The aforementioned copy mechanism attends to source words based on their " semantic " importance encoded in {? t , i } , which measures the semantic relatedness of the encoder hidden state he i and the decoder hidden state h d t ( Eq. ( 4 ) ) .
However , the source syntactic structure is ignored .
This is problematic , because it hurts the system 's ability to effectively identify summary - worthy source words thatare syntactically important .
We next propose three strategies to inject source syntactic structure to the copy mechanism .
Shallow Combination
Structural info
Example ( 1 ) depth in the dependency parse tree 0 ( 2 ) label of the incoming edge ' root ' ( 3 ) number of outgoing edges 3 ( 4 ) part - of - speech tag ' VBD ' ( 5 ) absolution position in the source text 9 ( 6 ) relative position in the source text ( 0.5 , 0.6 ] , we hypothesize that structural labels , such as the incoming dependency arc and the depth in a dependency parse tree , can be helpful to predict word importance .
We consider six categories of structural labels in this work ; they are presented in .
Each structural label is mapped to a fixed - length , trainable structural embedding .
However , a critical question remains as to where the structural embeddings should be injected in the existing neural architecture .
This problem has not yet been systematically investigated .
In this work , we compare two settings :
Struct + Input concatenates structural embeddings of position i ( flattened into one vector s e i ) with the source word embedding xi and uses them as a new form of input to the encoder : .
Structural embeddings are important complements to existing neural architectures .
However , it is unclear whether they should be supplied as input to the encoder or be left out of the encoding process and directly concatenated with the encoder hidden states .
This is a critical question we seek to answer by comparing the two settings .
Note that an alternative setting is to separately encode words and structural labels using two RNN encoders , we consider this as a subproblem of the " Struct + Input " case .
The above models complement state - of - the - art by combining semantic and structural signals to determine summary - worthy content .
Intuitively , a source word is copied to the summary for two reasons : it contains salient semantic content , or it serves a critical syntactic role in the source sentence .
Without explicitly modeling the two factors , ' semantics ' can outweigh ' structure , ' resulting in summaries that fail to keep the original meaning intact .
In the following we propose a two - way mechanism to separately model the " semantic " and " structural " importance of source words .
2 - Way Combination ( + Word )
Our new architecture involves two attention matrices thatare parallel to each other , denoted by ? and ?. ?
t , i is defined as previously in Eq. ( 3 - 4 ) .
It represents the " semantic " aspect , calculated as the strength of interaction between the encoder hidden state he i and the decoder hidden state h d t .
In contrast , ?
t , i measures the " structural " importance of the i - th input word to generating the t-th output word , calculated by comparing the structure - enhanced embedding g e i with the decoder hidden state h d t ( Eq. ( 10 - 11 ) ) .
We use g e i = [ s e i | |x i ] as a primitive ( unencoded ) representation of the i - th source word .
We define ? t , i ? ? t , i + ?
t , i as a weighted sum of ?
t , i and ?
t , i , where a trainable coefficient is introduced to balance the contribution from both sides ( Eq. ) .
Merging semantic and structural salience at this stage allows us to acquire an accurate estimate of how important the i - th source word is to predicting the t- th output word .
?
t , i replaces ?
t , i to become the new attention value .
It is used to calculate the context vector ct ( Eq. ( 13 ) ) .
A reliable estimate of ct is crucial as it is used to estimate the generation probability over the vocabulary ( P vocab ( w ) , Eq. ) , the switch value ( p gen , Eq. ( 8 ) ) , and ultimately used to predict the next word ( P ( w ) , Eq. ) .
2 - Way Combination ( + Relation )
We observe that salient source relations also play a critical role in predicting the next word .
For example , if a dependency edge ( " father " nsubj ???
" had " ) is salient and " father " is selected to be included in the summary , it is likely that " had " will be selected next such that a salient source relation ( " nsubj " ) is preserved in the summary .
Because summary words tend to follow the word order of the original , we assume selecting a source word and including it in the summary has an impact on its subsequent source words , but not the reverse .
In this formulation we use ?
t , i to capture the saliency of the dependency edge pointing to the i - th source word .
Thus , an edge w j ?
w i has it s salience score saved in ?
t , j ; and conversely , an edge w j ?
w i has it s salience score in ?
t , i . ? is calculated in the same way as described in Eq. ( 10 - 11 ) .
However , we replace g e i with [ g e i ||g e p , i ] so that a dependency edge is characterized by the embeddings of it s two endpoints ( g e p ,i is the parent embedding ) .
The architectural difference between " Struct + 2 Way + Word " and " Struct + 2 Way+ Relation " is illustrated in .
To obtain the likelihood of w j being selected to the summary prior to time step t , we define ? t , j = t?1 t =0 ? t ,j that sums up the individual probabilities up to time step t - 1 .
Assume there is a dependency edge w j ? w i ( j < i ) whose salience score is denoted by ? t , i .
At time step t , we calculate ? t , j ? t , i ( or ? t , j ?
t , j for edge w j ?
w i ) as the probability of w i being selected to the summary , given that one of it s prior words w j ( j< i ) is included in the summary and there is a dependency edge connecting the two .
By summing the impact over all its previous words , we obtain the likelihood of the i - th source word being included to the summary at time step tin order to preserve salient source relations ; this is denoted by ? t , i ( Eq. ( 15 ) ) .
Next , we define ? t , i ? ? t , i + ?
t , i as a weighted combination of semantic and structural salience ( Eq. ( 16 ) ) .
?
t , i replace ?
t , i to become the new attention values used to estimate the context vector c t ) .
Finally , the calculation of generation probabilities P vocab ( w ) , switch value p gen , and probabilities for predicting the next word P ( w ) remains the same as previously ( Eq. ( 6 - 9 ) ) .
Learning Objective and Beam Search
We next describe our learning objective , including a coverage - based regularizer , and a beam search with reference mechanism .
We want to investigate the effectiveness of these techniques on sentence summarization , which has not been explored in previous work .
Learning objective .
Our training proceeds by minimizing a per-target - word cross - entropy loss function .
A regularization term is applied to the ? matrix .
Recall that ?
t , i ?
[ 0 , 1 ] measures the interaction strength between the t- th output word and the i - th input word .
Naturally , we expect a 1 - to - 1 mapping between the two words .
The coverage - based regularizer , proposed by , encourages this behavior by tracking the historical attention values attributed to the i - th input word ( up to time step t - 1 ) , denoted by ? t , i = t?1 t =0 ? t , i .
The approach then takes the minimum between ?
t , i and ?
t , i , which has the practical effect of forcing ?
t , i (?t ) to be close to either 0 or 1 , otherwise a penalty will be applied .
The regularizer ?
is defined in Eq. , where M is the size of the mini-batch , Sand T are the lengths of the source and target sequences .
For two - way copy mechanisms , ? replaces ?
to become the new attention values , we therefore apply regularization to ?
instead of ?.
When the regularizer applies , the objective becomes minimizing ( L + ? ) .
Beam search with reference .
During testing , we employ greedy search to generate system summary sequences .
For the task of summarization , the ground truth summary sequences are usually close to the source texts .
This property can be leveraged in beam search .
describe a beam search with reference mechanism that rewards system summaries that have a high degree of bigram overlap with the source texts .
We describe it in Eq. ( 18 ) , where where S ( w ) denotes the score of word w.
B ( y <t , x ) measures the number of bigrams shared by the system summary ( up to time step t- 1 ) and the source text ; {y <t , w} add s a word w to the end of the system summary .
The shorter the source text ( measured by length S ) , the more weight a shared bigram will add to the score of the current word w .
A hyperparameter ?
controls the degree of closeness between the system summary and the source text .
Experiments
We evaluate the proposed structure - infused copy mechanisms for summarization in this section .
We describe the dataset , experimental settings , baselines , and finally , evaluation results and analysis .
Data Sets
We evaluate our proposed models on the Gigaword summarization dataset .
The task is to reduce the first sentence of an article to a title - like summary .
We obtain dependency parse trees for source sentences using the Stanford neural network parser .
We also use the standard train / valid / test data splits .
Following , the train and valid splits are pruned 2 to improve the data quality .
Spurious pairs thatare repetitive , overly long / short , and pairs whose source and summary sequences have little word overlap are removed .
No pruning is performed for instances in the test set .
The processed corpus contains 4,018 K training instances .
We construct two ( non -overlapped ) validation sets : " valid - 4096 " contains 4,096 randomly sampled instances from the valid split ; it is used for hyperparameter tuning and early stopping .
" valid - 2000 " is used for evaluation ; it allows the models to be trained and evaluated on pruned instances .
Finally , we report results on the standard Gigaword test set containing 1,951 instances ( " test - 1951 " ) .
Experimental Setup
We use the Xavier scheme for parameter initialization , where weights are initialized using a Gaussian distribution W i , j ? N ( 0 , ? ) , ? = lr = 1 e - 4 Coeff. for coverage - based regularizer ? = 1 Coeff. for beam search with reference ? ?
13.5 Beam size K = 5 Minibatch size M = 64 Early stopping criterion ( max 20 epochs ) valid .
loss Gradient clipping g ?
[ - 5 , 5 ] .
Models implementing the structure - infused copy mechanisms ( " Struct +* " ) outperform the baseline .
S : the government filed another round of criminal charges in a widening stock options scandal T : options scandal widens B : government files more charges in stock options scandal I : another round of criminal charges in stock options scandal H : charges filed in stock options scandal W : another round of criminal charges in stock options scandal R : government files another round of criminal charges in options scandal :
Example system summaries .
' S : ' source ; ' T : ' target ; ' B : ' baseline ; ' I : ' Struct + Input ; ' H: ' Struct + Hidden ; ' W : ' 2 Way + Word ; " R : " 2 Way + Relation .
" 2 Way + Relation " is able to preserve important source relations in the summary , e.g. , " government nsubj ? ?? ? files , " " files dobj ???
round , " and " round nmod ? ?? ? charges . "
S : red cross negotiators from rivals north korea and south korea held talks wednesday on emergency food shipments to starving north koreans and agreed to meet again thursday T : koreas meet in beijing to discuss food aid from south eds B : north korea , south korea agree to meet again I : north korea , south korea meet again H : north korea , south korea meet on emergency food shipments W : north korea , south korea hold talks on food shipments R : north korea , south korea hold talks on emergency food shipments :
Example system summaries .
" Struct + Hidden " and " 2 Way + Relation " successfully preserve salient source words ( " emergency food shipments " ) , which are missed out by other systems .
We observe that copying " hold talks " from the source also makes the resulting summaries more informative than using the word " meet . "
Results
ROUGE results on valid set .
We first report results on the Gigaword valid - 2000 dataset in .
We present R - 1 , R - 2 , and R - L scores ) that respectively measures the overlapped unigrams , bigrams , and longest common subsequences between the system and reference summaries 3 .
Our baseline system ( " Baseline " ) implements the seq2seq architecture with the basic copy mechanism ( Eq. ) .
It is a strong baseline that resembles the pointer - generator networks described in .
The structural models ( " Struct +* " ) differ from the baseline only on the structure - infused copy mechanisms .
All models are evaluated without the coverage regularizer or beam search ( 3.3 ) to ensure fair comparison .
Overall , we observe that models equipped with the structure - infused copy mechanisms are superior to the baseline , suggesting that combining source syntactic structure with the copy mechanism is effective .
We found that the " Struct + Hidden " architecture , which directly concatenates structural embeddings with the encoder hidden states , outperforms " Struct + Input " despite that the latter requires more parameters .
" Struct + 2 Way + Word " also demonstrates strong performance , achieving 43.21 % , 21. 84 % , and 40.86 % F 1 scores , for R - 1 , R - 2 , and R - L respectively .
ROUGE results on test set .
We compare our proposed approach with a range of state - of - the - art neural summarization systems .
Results on the standard Gigaword test set are presented in .
Details about these systems are provided in .
Overall , our proposed approach with structureinfused pointer networks perform strongly , yielding ROUGE scores thatare on - par with or surpassing state - of - the - art published systems .
Notice that the scores on the valid - 2000 dataset are generally higher than those of test - 1951 .
This is because the ( source , summary ) pairs in the Gigaword test set are not pruned ( see 4.1 ) .
In some cases , none ( or very few ) of the summary words appear in the source .
This may cause difficulties to the systems equipped with the copy mechanism .
The " Struct + 2 Way + Word " architecture that respectively models the semantic and syntactic importance of source words achieves the highest scores .
It outperforms its counterpart of " Struct + 2 Way + Relation , " which seeks to preserve source dependency relations in summaries .
We conjecture that the imperfect dependency parse trees generated R - 1 R - 2 R- L ABS 29.55 11.32 26.42 ABS + 29.76 11.88 26.96 Luong - NMT 33.10 14.45 30.71 RAS - LSTM 32.55 14.70 30.03 RAS - Elman 33.78 15.97 31.15 ASC+FSC1 34.17 15.94 31.92 lvt2k-1sent 32.67 15.59 30.64 lvt5k-1sent 35.30 16.64 32.62 Multi - Task 32.75 15.35 30.82 DRGD 36.27 17.57 33.62
Baseline ABS and ABS + are the first work introducing an encoder - decoder architecture for summarization .
Luong - NMT is a re-implementation of the attentive stacked LSTM encoder - decoder of .
RAS - LSTM and RAS - Elman describe a convolutional attentive encoder that ensures the decoder focuses on appropriate words at each step of generation .
ASC+ FSC1 presents a generative auto - encoding sentence compression model jointly trained on labelled / unlabelled data .
lvt2k - 1sent and lvt5k - 1 sent address issues in the attentive encoder - decoder framework , including modeling keywords , capturing sentence - toword structure , and handling rare words .
Multi - Task w/ Entailment combines entailment with summarization in a multi - task setting .
DRGD describes a deep recurrent generative decoder learning latent structure of summary sequences via variational inference .
by the parser may affect the " Struct + 2 Way + Relation " results .
However , because the Gigaword dataset does not provide gold - standard annotations for parse trees , we could not easily verify this and will leave it for future work .
In and 6 , we present system summaries produced by various models .
System
Info .
Fluency Faithful .
Struct + Input 2.9 3.3 3.0 Struct + 2 Way + Relation 3.0 3.4 3.1 Ground - truth Summ .
3.2 3.5 3.1 : Informativeness , fluency , and faithfulness scores of summaries .
They are rated by Amazon turkers on a Likert scale of 1 ( worst ) to 5 ( best ) .
We choose to evaluate Struct + 2 Way + Relation ( as oppose to 2 Way + Word ) because it focuses on preserving source relations in the summaries .
Linguistic quality .
To further gauge the summary quality , we hire human workers from the Amazon Mechanical Turk platform to rate summaries on a Likert scale of 1 to 5 according to three criteria : fluency ( is the summary grammatical and well - formed ? ) , informativeness ( to what extent is the meaning of the original sentence preserved in the summary ? ) , and faithfulness ( is the summary accurate and faithful to the original ? ) .
We sample 100 instances from the test set and employ 5 turkers to rate each summary ; their averaged scores are presented in .
We found that " Struct + 2 Way+ Relation " outperforms " Struct + Input " on all three criteria .
It also compares favorably to ground - truth summaries on " fluency " and " faithfulness . "
On the other hand , the ground - truth summaries , corresponding to article titles , are judged as less satisfying according to human raters .
Dependency relations .
We investigate the source dependency relations preserved in the summaries in .
A source relation is considered preserved if both its words appear in the summary .
We observe that the models implementing structure - infused copy mechanisms ( e.g. , " Struct + 2 Way + Word " ) are more likely to preserve important dependency relations in the summaries , including nsubj , dobj , amod , nmod , and nmod : poss .
Dependency relations thatare less important are less likely to be preserved .
These results show that our structure - infused copy mechanisms can learn to recognize the importance of dependency relations and selectively preserve them in the summaries .
Coverage and reference beam .
In , we investigate the effect of applying the coverage regularizer ( " coverage " ) and reference - based beam search ( " ref beam " ) ( 3.3 ) to our models .
The coverage regularizer is applied in a second training stage , where the system is trained for an extra 5 epochs with coverage and the model yielding the lowest validation loss is selected .
Both coverage and ref beam can improve the system performance .
Our observation suggests that ref beam is an effective addition to shorten the gap between different systems . Output vocabulary size .
Finally , we investigate the impact of the output vocabulary size on the summarization performance in .
All our models by default use an output vocabulary of 5 K words in order to make the results comparable to state - of - the - art - systems .
However , we observe that there is a potential to further boost the system performance ( 17.25?17.62 R - 2 F1 - score , w/o coverage or ref beam ) if we had chosen to use a larger vocabulary ( 10 K ) and can endure a slightly longer training time ( 1.2 x ) .
In , we further report the percentages of reference summary words covered by the output vocabulary ( " InVcb " ) and covered by either the output vocabulary or the source text ( " InVcb + Src " ) .
The gap between the two conditions shortens as the size of the output vocabulary is increased .
Conclusion
In this paper , we investigated structure - infused copy mechanisms that combine source syntactic structure with the copy mechanism of an abstractive summarization system .
We compared various system architectures and showed that our models can effectively preserve salient source relations in summaries .
Results on benchmark datasets showed that the structural models are on - par with or surpass state - of - theart published systems .
