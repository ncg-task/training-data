{
  "has" : {
    "Approach" : {
      "to scale" : "attention models",
      "prune down" : ["length of the source sequence", {"from sentence" : "Therefore , in order to scale attention models for this problem , we aim to prune down the length of the source sequence in an intelligent way ."}],
      "to use" : {
        "two - layer hierarchical attention" : {
          "Instead of" : {
            "naively attending to all the words" : {
              "of" : "source"
            }
          },
          "from sentence" : "Instead of naively attending to all the words of the source at once , our solution is to use a two - layer hierarchical attention ."
        }
      },
      "For" : {
        "document summarization" : {
          "means" : {
            "dividing the document" : {
              "into" : "chunks of text"
            }
          },
          "sparsely attending to" : {
            "one or a few chunks at a time" : {
              "using" : "hard attention",
              "applying" : {
                "usual full attention" : {
                  "over" : "those chunks"
                }
              }
            }
          },
          "call" : ["coarse - to - fine attention", {"from sentence" : "For document summarization , this means dividing the document into chunks of text , sparsely attending to one or a few chunks at a time using hard attention , then applying the usual full attention over those chunks - we call this method coarse - to - fine attention ."}]
        }
      }
    }
  }
}