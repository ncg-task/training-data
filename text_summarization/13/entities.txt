234	0	22	Sharpness of Attention
237	3	10	compute
237	15	30	entropy numbers
237	31	43	by averaging
237	44	68	over all generated words
237	69	71	in
237	76	90	validation set
239	3	7	note
239	17	31	entropy of C2F
239	32	34	is
239	35	43	very low
240	61	66	model
240	75	93	learns to focus on
240	101	123	few top - level chunks
240	124	126	of
240	131	139	document
240	140	144	over
240	149	169	course of generation
245	0	18	Attention Heatmaps
250	0	2	In
250	3	7	HIER
250	13	20	observe
250	30	58	attention becomes washed out
250	114	123	averaging
250	124	156	all of the encoder hidden states
255	3	7	C2 F
255	25	28	get
255	29	49	very sharp attention
255	50	52	on
255	53	62	some rows
16	21	29	to scale
16	30	46	attention models
16	76	86	prune down
16	91	120	length of the source sequence
17	86	92	to use
17	95	129	two - layer hierarchical attention
17	0	10	Instead of
17	11	45	naively attending to all the words
17	46	48	of
17	53	59	source
18	0	3	For
18	4	26	document summarization
18	34	39	means
18	40	61	dividing the document
18	62	66	into
18	67	81	chunks of text
18	84	105	sparsely attending to
18	106	135	one or a few chunks at a time
18	136	141	using
18	142	156	hard attention
18	164	172	applying
18	177	197	usual full attention
18	198	202	over
18	203	215	those chunks
18	221	225	call
18	238	266	coarse - to - fine attention
185	3	13	train with
185	14	59	minibatch stochastic gradient descent ( SGD )
185	60	64	with
185	65	78	batch size 20
185	79	82	for
185	83	92	20 epochs
185	95	118	renormalizing gradients
185	119	131	below norm 5
186	3	13	initialize
186	18	31	learning rate
186	32	34	to
186	35	38	0.1
186	39	42	for
186	47	66	top - level encoder
186	71	72	1
186	73	76	for
186	81	98	rest of the model
186	105	125	begin decaying it by
186	126	141	a factor of 0.5
186	153	158	after
186	163	184	validation perplexity
186	185	190	stops
186	191	201	decreasing
188	14	34	all other parameters
188	35	48	as uniform in
188	53	77	interval [ ? 0.1 , 0.1 ]
187	63	78	word embeddings
187	21	25	with
187	84	119	300 dimensional word2vec embeddings
187	3	6	use
187	7	20	2 layer LSTMs
187	79	83	with
187	26	42	500 hidden units
191	7	14	dropout
191	15	22	between
191	23	91	stacked LSTM hidden states and before the final word generator layer
191	92	105	to regularize
191	113	136	dropout probability 0.3
189	0	3	For
189	4	24	convolutional layers
189	30	33	use
189	36	48	kernel width
189	49	51	of
189	52	69	6 and 600 filters
190	0	21	Positional embeddings
190	22	26	have
190	27	39	dimension 25
192	0	2	At
192	3	12	test time
192	18	21	run
192	22	33	beam search
192	34	44	to produce
192	49	56	summary
192	57	61	with
192	64	78	beam size of 5
193	15	32	implemented using
193	33	38	Torch
193	39	47	based on
193	50	85	past version of the Open NMT system
194	7	10	ran
194	11	26	our experiments
194	27	29	on
194	32	60	12GB Geforce GTX Titan X GPU
2	36	58	Document Summarization
204	4	13	ILP model
204	14	26	ROUGE scores
204	31	47	surprisingly low
213	0	4	C2 F
213	17	41	significantly worse than
213	42	64	soft attention results
