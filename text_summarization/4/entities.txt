189	0	3	For
189	8	24	Gigaword dataset
189	30	53	compare our models with
190	0	5	ABS +
190	6	10	is a
190	11	36	fine tuned version of ABS
190	43	47	uses
190	51	72	attentive CNN encoder
190	80	92	NNLM decoder
190	95	101	Feat2s
190	130	135	is an
190	136	170	RNN sequence - to - sequence model
190	171	175	with
190	176	208	lexical and statistical features
190	209	211	in
190	216	223	encoder
190	226	237	Luong - NMT
190	238	242	is a
190	243	283	two - layer LSTM encoder - decoder model
190	286	297	RAS - Elman
190	298	302	uses
190	306	327	attentive CNN encoder
190	335	352	Elman RNN decoder
190	359	364	SEASS
190	365	369	uses
190	370	384	BiGRU encoders
190	389	401	GRU decoders
190	402	406	with
190	407	425	selective encoding
191	8	19	CNN dataset
191	25	48	compare our models with
192	0	8	Lead - 3
192	35	43	extracts
192	48	85	first three sentences of the document
192	86	88	as
192	89	96	summary
192	99	106	LexRank
192	107	115	extracts
192	116	121	texts
192	122	127	using
192	128	135	LexRank
192	138	146	Bi - GRU
192	9	13	is a
192	152	226	non-hierarchical one - layer sequence - to - sequence abstractive baseline
192	229	245	Distraction - M3
192	246	250	uses
192	253	295	sequence - to - sequence abstractive model
192	296	300	with
192	301	329	distraction - based networks
192	336	339	GBA
192	147	151	is a
192	345	395	graph - based attentional neural abstractive model
177	31	37	reduce
177	42	94	size of the input , output , and entity vocabularies
177	95	97	to
177	98	110	at most 50 K
177	131	138	replace
177	139	158	less frequent words
177	159	161	to
177	164	171	< unk >
178	64	74	initialize
178	79	102	word and entity vectors
178	3	6	use
178	41	60	pre-trained vectors
178	7	17	300D Glove
178	24	38	1000D wiki2vec
179	0	3	For
179	4	8	GRUs
179	14	17	set
179	22	32	state size
179	33	35	to
179	36	39	500
180	4	7	CNN
180	13	16	set
180	17	30	h = 3 , 4 , 5
180	31	35	with
180	36	64	400 , 300 , 300 feature maps
181	4	18	firm attention
181	26	31	tuned
181	21	22	k
181	32	46	by calculating
181	51	61	perplexity
181	62	64	of
181	69	74	model
181	75	88	starting with
181	89	142	smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... )
181	147	160	stopping when
181	165	202	perplexity of the model becomes worse
181	203	207	than
181	212	226	previous model
183	3	6	use
183	7	14	dropout
183	15	17	on
183	18	44	all non-linear connections
183	45	49	with
183	52	71	dropout rate of 0.5
187	7	18	beam search
187	19	26	of size
187	27	29	10
187	30	41	to generate
187	46	53	summary
184	3	6	set
184	11	22	batch sizes
184	23	25	of
184	26	51	Gigaword and CNN datasets
184	52	54	to
184	55	64	80 and 10
185	0	20	Training is done via
185	21	48	stochastic gradient descent
185	49	53	over
185	54	75	shuffled mini-batches
185	76	80	with
185	85	105	Adadelta update rule
185	113	157	l 2 constraint ( Hinton et al. , 2012 ) of 3
186	3	10	perform
186	11	25	early stopping
186	26	31	using
186	34	73	subset of the given development dataset
31	27	33	method
31	37	70	effectively apply linked entities
31	71	73	in
31	74	102	sequence - tosequence models
31	105	111	called
31	112	132	Entity2Topic ( E2T )
32	9	15	module
32	0	3	E2T
33	11	18	encodes
33	23	64	entities extracted from the original text
33	65	67	by
33	71	100	entity linking system ( ELS )
33	103	113	constructs
33	116	122	vector
33	123	135	representing
33	140	145	topic
33	146	148	of
33	153	176	summary to be generated
33	183	190	informs
33	195	202	decoder
33	203	208	about
33	213	237	constructed topic vector
35	23	28	using
35	29	44	entity encoders
35	45	49	with
35	50	74	selective disambiguation
35	82	94	constructing
35	95	108	topic vectors
35	109	114	using
35	115	129	firm attention
2	45	70	Abstractive Summarization
13	0	18	Text summarization
198	0	2	In
198	3	19	Gigaword dataset
198	20	25	where
198	30	45	texts are short
198	48	62	our best model
198	63	71	achieves
198	74	96	comparable performance
198	97	101	with
198	106	136	current state - of - the - art
199	3	14	CNN dataset
199	15	20	where
199	25	41	texts are longer
199	44	58	our best model
199	59	70	outperforms
199	71	94	all the previous models
201	10	13	E2T
201	14	22	achieves
201	25	48	significant improvement
201	49	53	over
201	58	77	baseline model BASE
201	80	84	with
201	85	101	at least 2 ROUGE
201	104	121	1 points increase
201	122	124	in
201	129	145	Gigaword dataset
201	150	157	6 ROUGE
201	160	177	1 points increase
201	178	180	in
201	185	196	CNN dataset
203	0	5	Among
203	10	24	model variants
203	31	50	CNN - based encoder
203	51	55	with
203	56	80	selective disambiguation
203	85	99	firm attention
203	100	108	performs
203	113	117	best
